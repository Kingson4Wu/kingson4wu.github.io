<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>拉巴力的纸皮箱</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2025-03-23T16:12:12.440Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Kingson Wu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用Java调用Rust构建的动态库</title>
    <link href="http://yoursite.com/2025/03/23/20250323-shi-yong-java-diao-yong-rust-gou-jian-de-dong-tai-ku/"/>
    <id>http://yoursite.com/2025/03/23/20250323-shi-yong-java-diao-yong-rust-gou-jian-de-dong-tai-ku/</id>
    <published>2025-03-23T06:41:37.000Z</published>
    <updated>2025-03-23T16:12:12.440Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>近期帮朋友做一个需求，需要将某个Rust项目的一部分功能移植到Java中，使Java应用能使用</p></blockquote></blockquote><h2 id="方案1-直接将Rust翻译成Java代码"><a href="#方案1-直接将Rust翻译成Java代码" class="headerlink" title="方案1: 直接将Rust翻译成Java代码"></a>方案1: 直接将Rust翻译成Java代码</h2><blockquote><blockquote><p>前置准备：将Rust项目跑通，方便通过debug对照执行细节，从而一步步翻译成Java代码<br>希望借助AI的力量帮忙完成</p></blockquote></blockquote><ul><li>借助AI的方案如下：<ul><li>首先，我已经跑通了需要抽取的功能的相关测试用例</li><li>然后，让AI帮忙将涉及这些功能相关的代码抽取成单独的Rust项目（结果还是很乱七八糟）</li><li>大概是因为上下文内容太多的关系，AI执行任务的效果很差</li><li>没办法，于是自己拆分代码，让AI软件帮忙根据代码块逐个进行转化，结果效果是好了很多，能帮助不少，不过这样效率太低，而且AI有时候也经常出错，需要自己慢慢调整</li><li>看来AI目前在一次性的转化大量代码的效率和准确性还是不够</li><li>本次程序员该懒就懒的宗旨，我决定另辟蹊径，采用方案2</li></ul></li></ul><h2 id="方案2-直接将相关功能的Rust功能编译成动态库，由Java直接调用"><a href="#方案2-直接将相关功能的Rust功能编译成动态库，由Java直接调用" class="headerlink" title="方案2: 直接将相关功能的Rust功能编译成动态库，由Java直接调用"></a>方案2: 直接将相关功能的Rust功能编译成动态库，由Java直接调用</h2><blockquote><blockquote><p>省去翻译代码的重复工作<br>需要构建不同平台的动态库<br>Java代码在加载前需要判断环境加载相应的动态库<br>使用Github Action来构建各种平台的动态库<br>Java端充分测试，Java8应用测试，Java9以上模块化系统是否可用测试，Windows等其他平台测试</p></blockquote></blockquote><hr><blockquote><blockquote><p>以下由AI（ChatGPT）帮我完成的文章</p></blockquote></blockquote><h1 id="使用Java调用Rust构建的动态库"><a href="#使用Java调用Rust构建的动态库" class="headerlink" title="使用Java调用Rust构建的动态库"></a>使用Java调用Rust构建的动态库</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在最近的一个项目中，我需要将一个Rust项目中的部分功能迁移到Java应用中，供Java系统调用。为了简化过程，我决定绕过将Rust代码完全转化为Java的复杂任务，而是直接通过Rust构建动态库，再通过Java来调用。这种方法不仅减少了代码转换的工作量，还利用了Rust的高性能特点，提高了Java应用的效率。</p><h2 id="方案1-直接将Rust翻译成Java代码-1"><a href="#方案1-直接将Rust翻译成Java代码-1" class="headerlink" title="方案1: 直接将Rust翻译成Java代码"></a>方案1: 直接将Rust翻译成Java代码</h2><h3 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h3><p>为了开始这个过程，我首先确保了Rust项目能够成功运行，并且通过调试工具逐步分析每个功能的执行细节。这是为了确保我可以一步步将Rust的实现逻辑转化为Java代码。</p><h3 id="使用AI进行代码转换"><a href="#使用AI进行代码转换" class="headerlink" title="使用AI进行代码转换"></a>使用AI进行代码转换</h3><p>最初，我尝试借助AI工具自动化地将Rust代码转换为Java代码。以下是我使用AI进行代码转换的过程：</p><ul><li>我首先跑通了相关的测试用例，确保所有需要移植的功能都能正常工作。</li><li>然后，我让AI帮助抽取与这些功能相关的Rust代码，并转化成单独的Rust项目。但由于上下文过多，AI的效果并不理想，生成的代码混乱且不完整。</li><li>之后，我将代码拆分成更小的部分，让AI逐一处理，虽然效果有所改善，但这种方式依旧效率低下，且AI经常会出错，导致我不得不花大量时间修正。</li></ul><p>这一过程中，我意识到AI在一次性大规模转化代码时的准确性和效率仍然有待提升。最终，我决定放弃这条路，转而尝试另一种更直接的方案——方案2。</p><h2 id="方案2-直接将Rust功能编译成动态库，由Java调用"><a href="#方案2-直接将Rust功能编译成动态库，由Java调用" class="headerlink" title="方案2: 直接将Rust功能编译成动态库，由Java调用"></a>方案2: 直接将Rust功能编译成动态库，由Java调用</h2><h3 id="思路与优势"><a href="#思路与优势" class="headerlink" title="思路与优势"></a>思路与优势</h3><p>与其将Rust代码翻译成Java代码，我决定直接将Rust的功能编译成动态库（.dll 或 .so），然后在Java中通过JNI（Java Native Interface）调用这些动态库。这样可以避免代码翻译过程中的麻烦，并且能够充分利用Rust在性能上的优势。</p><h3 id="构建Rust动态库"><a href="#构建Rust动态库" class="headerlink" title="构建Rust动态库"></a>构建Rust动态库</h3><p>在实现这一方案时，我需要做以下几个步骤：</p><ol><li><p><strong>编译Rust代码为动态库</strong>：通过Rust的<code>cargo</code>工具，将Rust代码编译成适用于不同平台的动态库。这需要为每个平台生成相应的二进制文件，如Windows下的<code>.dll</code>，Linux下的<code>.so</code>，以及MacOS下的<code>.dylib</code>。</p></li><li><p><strong>使用Github Actions自动化构建</strong>：为了支持多平台的动态库构建，我使用了Github Actions来自动化构建过程。这样，无论是Windows、Linux还是MacOS，都会根据需要自动生成对应的动态库，极大地简化了跨平台的构建工作。</p></li><li><p><strong>Java端加载动态库</strong>：Java通过JNI加载Rust编译出的动态库，并调用其中的函数。在加载时，Java代码会先检测当前运行环境，根据操作系统加载相应的库文件。</p></li><li><p><strong>平台兼容性测试</strong>：为了确保Java能够顺利调用Rust动态库，我在多个平台上进行了充分的测试。这包括：</p><ul><li>在Java 8环境下进行功能测试。</li><li>验证在Java 9及以上版本的模块化系统下，Java与Rust动态库的兼容性。</li><li>测试在Windows、Linux和MacOS等不同平台下的表现。</li></ul></li></ol><h3 id="方案优势"><a href="#方案优势" class="headerlink" title="方案优势"></a>方案优势</h3><ul><li><strong>减少工作量</strong>：不需要将Rust代码完全翻译为Java代码，避免了翻译过程中的复杂性和错误。</li><li><strong>高效的性能</strong>：Rust的性能优势得以保留，Java通过调用Rust动态库，可以在性能上得到显著提升。</li><li><strong>跨平台支持</strong>：通过自动化构建不同平台的动态库，Java应用能够轻松支持Windows、Linux、MacOS等多种操作系统。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过将Rust功能编译成动态库并在Java中调用，我成功地绕过了将Rust代码完全转化为Java的复杂过程，同时还能够充分利用Rust的性能优势。虽然在构建过程中涉及到平台适配和自动化构建，但这一方案的灵活性和高效性使其成为解决问题的最佳选择。对于有类似需求的开发者来说，这无疑是一个值得尝试的方案。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;近期帮朋友做一个需求，需要将某个Rust项目的一部分功能移植到Java中，使Java应用能使用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;方案1-直接将Rust翻译成Java代码&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
    <category term="Rust" scheme="http://yoursite.com/tags/Rust/"/>
    
    <category term="JNI" scheme="http://yoursite.com/tags/JNI/"/>
    
  </entry>
  
  <entry>
    <title>开源知识库系统试用</title>
    <link href="http://yoursite.com/2025/02/26/20250226-kai-yuan-zhi-shi-ku-xi-tong-shi-yong/"/>
    <id>http://yoursite.com/2025/02/26/20250226-kai-yuan-zhi-shi-ku-xi-tong-shi-yong/</id>
    <published>2025-02-26T14:22:04.000Z</published>
    <updated>2025-02-27T09:09:16.431Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>帮助朋友的公司搭建内部知识库的原型<br>简单记录一下相关技术</p></blockquote></blockquote><p><img src="/2025/02/26/20250226-kai-yuan-zhi-shi-ku-xi-tong-shi-yong/dify.drawio.png"></p><ul><li><p>目前看权限管理都很不精细，只在固定角色上进行控制，用于内部团队管理仍不是很方便</p></li><li><p>文档嵌入效果一般，可能要研究里面一些复杂的选项用法，或者寻求其他方案</p></li><li><p>除了dify，还有很多其他开源的知识库系统，比如 Cherry Studio、MaxKB、FastGPT、AnythingLLM、Ragflow等</p></li><li><p>dify功能很多，anythingllm功能较少，Cherry Studio 就是个人知识库</p></li><li><p>所谓的各种助手其实就是提示词模版</p></li></ul><h1 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h1><ul><li>传统RAG通常依赖单次检索，这可能不足以处理需要逐步构建上下文的复杂查询</li><li>向量相似性检索依赖知识库中明确提及的信息，可能无法捕捉隐含或关系信息。相比之下，知识图谱（如GraphRAG）通过利用数据结构中的关系，能更好地处理需要全面理解数据集的全局查询</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;帮助朋友的公司搭建内部知识库的原型&lt;br&gt;简单记录一下相关技术&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2025/02/26/20250226-kai-yuan-zhi-</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
    <category term="RAG" scheme="http://yoursite.com/tags/RAG/"/>
    
    <category term="知识库" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>关于大模型的Prompt</title>
    <link href="http://yoursite.com/2025/02/25/20250225-guan-yu-da-mo-xing-de-prompt/"/>
    <id>http://yoursite.com/2025/02/25/20250225-guan-yu-da-mo-xing-de-prompt/</id>
    <published>2025-02-25T13:48:37.000Z</published>
    <updated>2025-02-26T11:10:33.548Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>网上资料摘要<br>一些个人理解</p></blockquote></blockquote><h1 id="大模型交互的核心：提示词工程"><a href="#大模型交互的核心：提示词工程" class="headerlink" title="大模型交互的核心：提示词工程"></a>大模型交互的核心：提示词工程</h1><blockquote><blockquote><p>以下基于个人理解，并通过claude优化（2025-02-26）</p></blockquote></blockquote><ul><li>提示词（Prompt）是大模型的输入，也是调用大模型能力的接口，用以激发或引导大模型生成特定类型的回答。</li><li>提示词工程的目的是尽量把任务说清楚，让大模型能充分理解我们的意图，以按照正确的方向回答问题。  </li><li>在不微调大模型的情况下，外部与大模型交互的唯一途径就是提示工程。即便是已经微调过的大模型，后续与其沟通的唯一途径仍是通过提供提示词，尽管微调可能改变模型对提示词的响应方式。</li><li>所谓的RAG、短期记忆（对话历史）、长期记忆等功能，都是基于提示工程这一与大模型交互的路径作为切入点，建立的其他优化策略和架构。</li></ul><h1 id="ICIO框架（Prompt包含的要素）"><a href="#ICIO框架（Prompt包含的要素）" class="headerlink" title="ICIO框架（Prompt包含的要素）"></a>ICIO框架（Prompt包含的要素）</h1><ul><li><a href="https://mp.weixin.qq.com/s/xYC6saH3PU6nJc9mzV5alw">从原理出发 - 提示词如何影响大模型的输出</a></li><li>核心思想，是通过明确定义任务的各个方面，来提高AI响应时的效率和准确性。</li><li>在ICIO的框架的设计中，Prompt可能包含四要素：<ul><li>Instruction（指令）：这是最重要的部分，它直接告诉模型需要执行的具体任务。</li><li>Context（上下文&#x2F;背景）：上下文提供了任务执行所需的背景信息，帮助模型理解任务的情景和环境。</li><li>Input Data（输入数据）：输入数据是模型需要处理的具体信息。</li><li>Output Indicator（输出指示器）：输出指示器告诉模型用户期望的输出类型或格式。</li></ul></li><li>其中除了指令以外，其他要素都是可选的，说明指令对于大模型来说是最重要的，其他要素都是对指令的补充。</li><li>优质的Prompt，可以清晰地传达用户的意图</li></ul><h1 id="Prompt五大框架"><a href="#Prompt五大框架" class="headerlink" title="Prompt五大框架"></a>Prompt五大框架</h1><ul><li><a href="https://mp.weixin.qq.com/s/u-79q3R0l01oO-7WWUNF2A">大模型Prompt技巧全解析</a>    </li><li>RTF框架 <ul><li>R-Role(角色)、R-Role(角色)、F-Format(格式)</li></ul></li><li>思考链模式 <ul><li>适合一些复杂的任务处理</li><li>要使用这种模式，只需要在末尾添加”让我们逐步思考”即可</li></ul></li><li>RISEN框架<ul><li>R-Role:大模型扮演的角色</li><li>I-Instructions: 指示命令，和Task-任务差不多</li><li>S-Steps: 步骤</li><li>E-End Goal: 最终目标</li><li>N-Narrowing(Constraints): 缩小范围(约束条件)，和RTF框架中的Format有异曲同工之妙</li><li>该框架主要适合<ul><li>撰写具有特定约束的任务(例如博客文章)</li><li>有明确指导方针的任务（例如商业计划）</li></ul></li></ul></li><li>RODES框架<ul><li>R-Role: 角色、O - Objective: 目标、D - Details: 详细的细节、E - Examples: 示例、S - Sense Check: 感官检查</li></ul></li><li>密度链模式<ul><li>使用递归来创建越来越好的输出的提示，与普通提示生成的 GPT-4 摘要相比，它生成的摘要更加密集且更适合人们理解</li><li>适合：总结、改进您最喜欢的提示、通过递归生成可用的长格式内容</li></ul></li></ul><h1 id="打造高效Prompt的两大核心原则"><a href="#打造高效Prompt的两大核心原则" class="headerlink" title="打造高效Prompt的两大核心原则"></a>打造高效Prompt的两大核心原则</h1><ul><li><a href="https://mp.weixin.qq.com/s/u-79q3R0l01oO-7WWUNF2A">大模型Prompt技巧全解析</a> </li><li>原则一：编写明确和具体的指令<ul><li>策略1：使用分隔符清晰界定输入部分</li><li>策略2：要求结构化输出</li><li>策略3：要求模型检查条件是否满足</li><li>策略4：Few-shot prompting（少样本提示）</li></ul></li><li>原则二：给予模型充足的思考时间<ul><li>策略1：明确完成任务所需的步骤</li><li>策略2：引导模型在得出结论前充分思考方案</li></ul></li></ul><h1 id="Prompt技术剖析与应用"><a href="#Prompt技术剖析与应用" class="headerlink" title="Prompt技术剖析与应用"></a>Prompt技术剖析与应用</h1><ul><li><a href="https://mp.weixin.qq.com/s/u-79q3R0l01oO-7WWUNF2A">大模型Prompt技巧全解析</a> </li><li>一、零样本提示（Zero-Shot Prompting）</li><li>二、少样本提示（Few-Shot Prompting）<ul><li>在零样本提示效果不佳时发挥作用</li></ul></li><li>三、思维链提示（Chain-of-Thought Prompting）<ul><li>与少样本提示结合，增强效果，尤其适用于算术、常识推理等任务，帮助模型更有条理地处理问题</li></ul></li><li>四、自我一致性（Self-Consistency）   <ul><li>主要用于优化思维链提示中的推理路径选择</li><li>核心思想是通过提供多个少样本推理示例，让模型从多样的推理结果中筛选出最一致的答案，增强模型在算术和常识推理任务中的可靠性</li></ul></li><li>五、生成知识提示（Generated Knowledge Prompting）<ul><li>主要用于解决模型在处理需要额外知识的任务时出现的局限性   </li><li>个人理解：一种特殊的RAG罢了</li></ul></li><li>六、链式提示（Prompt Chaining）<ul><li>将复杂任务拆解为多个子任务，通过逐个子任务生成提示并传递结果的方式来实现复杂任务的有序处理</li></ul></li><li>七、思维树（ToT）<ul><li>为了帮助模型应对复杂的探索性任务而设计</li><li>通过维护一棵思维树，让模型在解决问题时能够生成和评估中间思维步骤，并结合搜索算法进行系统性探索</li></ul></li><li>八、检索增强生成（RAG）<ul><li>将信息检索与文本生成相结合，专门用于处理知识密集型任务</li><li>通过检索相关文档来为模型提供额外的知识支持，从而缓解模型的“幻觉”问题</li></ul></li><li>九、自动推理并使用工具（ART）<ul><li>使模型能够自动生成包含推理步骤的程序，并在需要时调用外部工具</li></ul></li><li>十、自动提示工程师（APE）<ul><li>自动生成和筛选任务指令</li><li>利用大型语言模型生成指令候选项，再依据评估分数选择最佳指令，从而提升提示生成的效率与效果</li></ul></li><li>十一、Active-Prompt<ul><li>用于解决思维链示例有效性的问题</li><li>通过先查询模型生成多个答案，计算不确定度后挑选最不确定的问题由人类注释示例，再用新示例推断其他问题，从而优化模型对不同任务的适应性</li></ul></li><li>十二、方向性刺激提示（Directional Stimulus Prompting）<ul><li>通过训练策略 LM 生成引导提示，增强对模型生成结果的掌控力。例如文本摘要任务</li></ul></li><li>十三、PAL（程序辅助语言模型）<ul><li>让模型生成程序来解决问题，借助编程运行时提升解决复杂问题的能力</li></ul></li><li>十四、ReAct 框架<ul><li>ReAct 框架使模型交错生成推理轨迹和操作，提升答案的可靠性与可解释性</li></ul></li><li>十五、自我反思（Reflexion）<ul><li>包含参与者、评估者和自我反思三个模型，旨在帮助模型从错误中学习并提升性能</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://mp.weixin.qq.com/s/xYC6saH3PU6nJc9mzV5alw">从原理出发 - 提示词如何影响大模型的输出</a><ul><li>大模型如何理解Prompt<ul><li>基于Transformer的解码器的大模型</li><li>最核心的两层<ul><li>掩码多头自注意力层（Masked Multi Self Attention Layers，对应Transformer的Masked Multi-Head Attention，简称MHA）</li><li>前置反馈网络层（Feed Forward Networks Layers，简称FFN）</li></ul></li><li>Prompt会影响自注意力层对上下文信息的捕捉</li><li>自注意力机制<ul><li>它的核心思想是模仿人类的注意力，即在处理大量信息时，能够聚焦于当前任务最相关的部分，而忽略其他不太重要的信息</li></ul></li></ul></li></ul></li><li><a href="https://mp.weixin.qq.com/s/u-79q3R0l01oO-7WWUNF2A">大模型Prompt技巧全解析</a>            </li><li><a href="https://kingson4wu.github.io/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/">LLM相关技术简单了解</a></li><li><a href="https://mp.weixin.qq.com/s/q2iMW0t5456btmIPS1ba6Q">大型语言模型的提示注入</a><ul><li>三种防范此类漏洞的方法<ul><li>可以在提示中添加指令</li><li>使用对抗性探测器添加第二层保护</li><li>对模型进行微调，使其更符合用户需求，同时提供最高级别的保护，防止提示注入和窃取</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;网上资料摘要&lt;br&gt;一些个人理解&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;大模型交互的核心：提示词工程&quot;&gt;&lt;a href=&quot;#大模型交互的核心：提示词工程&quot; class=&quot;heade</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
    <category term="Prompt" scheme="http://yoursite.com/tags/Prompt/"/>
    
  </entry>
  
  <entry>
    <title>大模型应用开发AI Agent要点记录</title>
    <link href="http://yoursite.com/2025/02/21/20250221-da-mo-xing-ying-yong-kai-fa-ai-agent-yao-dian-ji-lu/"/>
    <id>http://yoursite.com/2025/02/21/20250221-da-mo-xing-ying-yong-kai-fa-ai-agent-yao-dian-ji-lu/</id>
    <published>2025-02-21T09:28:05.000Z</published>
    <updated>2025-02-25T08:14:54.203Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>基于书籍简单记录要点</p></blockquote></blockquote><p><img src="/2025/02/21/20250221-da-mo-xing-ying-yong-kai-fa-ai-agent-yao-dian-ji-lu/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%845%E4%B8%AA%E5%B1%82%E6%AC%A1.png"></p><ul><li>从L3到L4的跨越是一个从被动到自主的分水岭，在这个跨越过程中，Agent将成为关键的驱动力</li></ul><p><img src="/2025/02/21/20250221-da-mo-xing-ying-yong-kai-fa-ai-agent-yao-dian-ji-lu/AI-Agent.png"></p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>《大模型应用开发 动手做AI Agent》</li><li><a href="https://developer.baidu.com/article/detail.html?id=3372236">AIGC生成式人工智能的五个进阶层次</a></li><li><a href="https://blog.csdn.net/surfirst/article/details/142661951">AIGC：生成式人工智能的5个层次</a></li><li><a href="https://zhuanlan.zhihu.com/p/671355141">2024年大模型Multi-agent多智能体应用技术：AutoGen, MetaGPT, XAgent, AutoAgents，crewAI</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;基于书籍简单记录要点&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2025/02/21/20250221-da-mo-xing-ying-yong-kai-fa-ai-age</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
    <category term="AI-Agent" scheme="http://yoursite.com/tags/AI-Agent/"/>
    
  </entry>
  
  <entry>
    <title>基于大模型的RAG应用开发要点记录</title>
    <link href="http://yoursite.com/2025/02/20/20250220-ji-yu-da-mo-xing-de-rag-ying-yong-kai-fa-yao-dian-ji-lu/"/>
    <id>http://yoursite.com/2025/02/20/20250220-ji-yu-da-mo-xing-de-rag-ying-yong-kai-fa-yao-dian-ji-lu/</id>
    <published>2025-02-20T13:12:52.000Z</published>
    <updated>2025-03-05T05:45:47.207Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>基于书籍简单记录要点</p></blockquote></blockquote><p><img src="/2025/02/20/20250220-ji-yu-da-mo-xing-de-rag-ying-yong-kai-fa-yao-dian-ji-lu/RAG.png"></p><hr><ul><li><p>一种借助“外挂”的提示工程</p></li><li><p>最核心的思想：给大模型补充外部知识以提高生成质量</p></li><li><p>大模型的 RAG 应用中，最常见的检索方式是借助基于向量的语义检索来获得相关的数据块，并根据其相似度排序，最后输出最相关的前 K 个数据块（简称top_K）</p></li><li><p>自然语言处理中用于把各种形式的信息转换成向量表示的模型叫嵌入模型</p></li><li><p>基于向量的语义检索就是通过计算查询词与已有信息向量的相似度（如余弦相似度），找出与查询词在语义上最接近的信息。</p></li><li><p>如果大模型是一个优秀学生，正在参加一门考试，那么 RAG 和微调的区别如下。</p><ul><li>RAG：在考试时给他提供某个领域的参考书，要求他现学现用，并给出答案。</li><li>微调：在考试前一天对他进行辅导，使他成为某个领域的专家，然后让他参加考试。</li></ul></li><li><p>大模型与 Prompt 并不只在最后生成结果时才需要，在 RAG 应用流程的很多阶段都需要用到它们，比如在生成摘要、查询转换、查询路由、智能体推理、响应评估等很多阶段，都需要利用设计的Prompt 来让大模型完成任务。</p></li><li><p>与查询引擎相关的几个关键组件如下。</p><ul><li>（1）VectorIndexRetriever：向量索引检索器。用于完成相关知识的检索，基于索引来完成，输出多个相关 Node。</li><li>（2）Synthesizer：响应生成器。借助大模型来完成 Prompt 组装，并根据响应生成模式的要求来生成响应结果。</li><li>（3）NodePostProcessor：节点后处理器。通常用于在检索完成之后，对检索器输出的 Node 列表做补充处理，比如重排序。</li></ul></li><li><p>Agent 就是通过 AI 模型驱动，能够自主地理解、规划、执行，并最终完成任务的 AI 程序。Agent 与大模型的区别类似于人与大脑的区别：大脑指挥人的行动，但是只有人才是执行任务的完整体。    </p></li><li><p>Agent 就是在大模型作为智慧大脑的基础上实现记忆（Memory）、自我规划（Planning）、使用工具（Tool）等能力，从而开发一个具有自主认知与行动能力的完全“智能体”。</p></li><li><p>与 RAG 应用相比，Data Agent 具备以下能力。</p><ul><li>（1）兼具 RAG 应用的数据检索与查询生成能力。</li><li>（2）通过观察环境与任务目标推理出完成下一个数据任务的步骤。</li><li>（3）通过调用外部服务工具（API）完成复杂任务，并返回执行结果。</li><li>（4）具备长期记忆能力（如使用向量库）与短期记忆（一次任务中的交互历史等）能力。</li></ul></li><li><p>与 RAG 应用相比，Data Agent 的主要增强之处如下。</p><ul><li>（1）规划与推理出完成任务的步骤的能力。</li><li>（2）定义与使用工具的能力。</li></ul></li></ul><h1 id="LlamaIndex-框架整体结构"><a href="#LlamaIndex-框架整体结构" class="headerlink" title="LlamaIndex 框架整体结构"></a>LlamaIndex 框架整体结构</h1><ul><li>(1)Core Framework:这是 LlamaIndex 的核心框架，实现了 LlamaIndex 框 架中大量可扩展的基础组件与工具组件。</li><li>(2)Integrations:这是在 Core Framework 基础上由官方或者第三方提供的 大量扩展组件。这些组件大大扩充了 LlamaIndex 框架的兼容能力与应用场景， 包括各种类型的数据加载器、大语言模型与嵌入模型、向量库、Agent 工具等。</li><li>(3)Templates:这是官方或者第三方提供的更上层的应用模板。开发者可 以基于这些模板来修改和开发自己的应用。</li><li>(4)Eval Datasets:评估数据集。这是一些用于测试评估的现成数据集。 一个数据集通常包含两个部分:一部分是用于开发 RAG 应用的原始知识数据 集;另一部分是用于评估 RAG 应用的测试数据集。</li></ul><hr><h1 id="LlamaIndex-的向量存储索引和传统数据库（如-MySQL）的索引的区别"><a href="#LlamaIndex-的向量存储索引和传统数据库（如-MySQL）的索引的区别" class="headerlink" title="LlamaIndex 的向量存储索引和传统数据库（如 MySQL）的索引的区别"></a>LlamaIndex 的向量存储索引和传统数据库（如 MySQL）的索引的区别</h1><ul><li>From Grok3 (2025-02-24)</li></ul><h2 id="传统数据库索引（如-MySQL）-vs-向量存储索引"><a href="#传统数据库索引（如-MySQL）-vs-向量存储索引" class="headerlink" title="传统数据库索引（如 MySQL） vs 向量存储索引"></a>传统数据库索引（如 MySQL） vs 向量存储索引</h2><h3 id="1-传统数据库索引（B-树、哈希索引等）"><a href="#1-传统数据库索引（B-树、哈希索引等）" class="headerlink" title="1. 传统数据库索引（B-树、哈希索引等）"></a>1. 传统数据库索引（B-树、哈希索引等）</h3><ul><li><strong>目的</strong>：加速精确匹配或范围查询，比如在 <code>WHERE id = 5</code> 或 <code>WHERE age &gt; 30</code> 时快速定位数据。</li><li><strong>数据结构</strong>：通常基于 B-树、B+树 或哈希表，存储的是原始数据（如整数、字符串）的某种排序或映射。</li><li><strong>存储内容</strong>：索引本身不保存完整数据，而是指向数据库中实际数据的引用（行指针）。</li><li><strong>使用场景</strong>：结构化数据的高效检索。</li></ul><h3 id="2-LlamaIndex-的向量存储索引"><a href="#2-LlamaIndex-的向量存储索引" class="headerlink" title="2. LlamaIndex 的向量存储索引"></a>2. LlamaIndex 的向量存储索引</h3><ul><li><strong>目的</strong>：支持语义相似性搜索，比如找到与“人工智能是什么”语义最接近的文档。</li><li><strong>数据结构</strong>：基于向量空间的高效搜索结构（如 FAISS、HNSW、Annoy），存储的是文本的嵌入向量（高维浮点数数组）。</li><li><strong>存储内容</strong>：索引里直接保存的是嵌入向量（通常还会关联原始文本或其引用），而不仅仅是指针。这种嵌入向量本身就是数据的某种“表示”。</li><li><strong>使用场景</strong>：非结构化数据（如文本、图像）的语义搜索。</li></ul><p>所以你说得对，向量存储索引和 MySQL 的索引在设计目标和实现上完全不同。传统索引是为了快速定位具体值，而向量存储索引是为了快速找到“最相似”的内容。</p><h2 id="为什么叫“向量存储索引”而不是“向量存储数据”？"><a href="#为什么叫“向量存储索引”而不是“向量存储数据”？" class="headerlink" title="为什么叫“向量存储索引”而不是“向量存储数据”？"></a>为什么叫“向量存储索引”而不是“向量存储数据”？</h2><ul><li><p><strong>“索引”在这里的含义</strong>：</p><ul><li>在向量搜索领域，“索引”指的是一个经过优化的数据结构，用于加速向量之间的相似性计算和检索。它不只是存数据，而是以某种方式组织数据（比如 FAISS 用聚类或倒排索引），让查询效率更高。</li><li>比如 FAISS 可能会把向量分桶（clustering），HNSW 用图结构连接向量，这些都是“索引”的体现。</li></ul></li><li><p><strong>为什么不叫“向量存储数据”</strong>：</p><ul><li>“数据”听起来像是原始的、未经组织的存储，比如直接把向量一股脑扔进数据库。而“索引”强调的是结构化和查询优化。</li><li>LlamaIndex 的向量存储索引不仅存了向量，还为快速检索做了预处理（比如降维、构建树或图）。它是个功能性工具，而不仅是数据的容器。</li></ul></li><li><p><strong>命名习惯</strong>：</p><ul><li>在向量搜索和机器学习领域，像 FAISS、Annoy、HNSW 这些工具都叫“索引”（index），因为它们的核心是构建一个可搜索的结构。LlamaIndex 沿用了这种惯例。</li></ul></li></ul><h2 id="举个类比"><a href="#举个类比" class="headerlink" title="举个类比"></a>举个类比</h2><ul><li>MySQL 的 B-树索引就像图书馆的卡片目录，按书名首字母排序，帮你快速找到某本书。</li><li>LlamaIndex 的向量存储索引更像是推荐系统，根据你喜欢的书，找到内容相似的其他书，靠的是“向量空间的距离”而不是精确匹配。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LlamaIndex 的向量存储索引确实和 MySQL 的索引差别很大，它更像是一个专门为高维向量设计的“搜索加速器”。叫“索引”是因为它不仅存向量，还组织向量以提升查询效率，而不是简单的数据存储。</p><hr><h1 id="个人对于深度强化学习的思考"><a href="#个人对于深度强化学习的思考" class="headerlink" title="个人对于深度强化学习的思考"></a>个人对于深度强化学习的思考</h1><ul><li>对于RAG应用，优化方法有很多。我的想法是，把这些优化方法想象成一堆‘工具’，然后用强化学习这个‘聪明助手’来挑出最好用的组合。</li><li>“优化策略”变成了“工具”，“强化学习”变成了“聪明助手”，“奖励函数”简化为“任务需求”</li></ul><hr><h1 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h1><h2 id="如果嵌入模型知道“你好”和“再见”的向量，它可以将“你好再见”转化为一个向量吗？"><a href="#如果嵌入模型知道“你好”和“再见”的向量，它可以将“你好再见”转化为一个向量吗？" class="headerlink" title="如果嵌入模型知道“你好”和“再见”的向量，它可以将“你好再见”转化为一个向量吗？"></a>如果嵌入模型知道“你好”和“再见”的向量，它可以将“你好再见”转化为一个向量吗？</h2><ul><li>From Grok3 （2025-02-25）</li><li>嵌入模型可以将“你好再见”转化为向量。</li><li>对于词嵌入模型（如 Word2Vec 或 GloVe），通常通过平均或相加“你好”和“再见”的向量来生成短语向量。</li><li>对于句嵌入模型（如 BERT 或 Ada-002），模型可以直接处理“你好再见”序列，生成反映上下文关系的向量。</li></ul><h2 id="RAG的相关原理"><a href="#RAG的相关原理" class="headerlink" title="RAG的相关原理"></a>RAG的相关原理</h2><ul><li><p>From Grok3 （2025-02-25）– 基于自身理解由Grok优化</p></li><li><p>文本向量化：在RAG中，知识库中的文档会被分割成适当的文本片段（例如句子或段落），然后使用预训练模型对这些片段整体进行向量化，生成语义丰富的稠密向量。这一过程不同于传统的单词嵌入，目的是捕捉整个片段的语义信息。</p></li><li><p>向量检索：当用户提出问题时，系统会将问题同样向量化，并在向量数据库中查询与问题语义最相似的文本片段。这些检索结果是原始文档中的真实内容，而不是由模型生成的文本。</p></li><li><p>上下文整合与生成：检索到的文本片段会以纯文本形式插入到一个提示模板中，作为上下文连同用户的问题一起输入到生成模型中。生成模型（如大语言模型）基于这些上下文生成最终的回答。</p></li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li><p><a href="https://developer.aliyun.com/article/1628030">精通RAG架构：从0到1，基于LLM+RAG构建生产级企业知识库</a></p><ul><li>向量数据库对比</li><li>如何选择嵌入模型：<a href="https://huggingface.co/spaces/mteb/leaderboard">https://huggingface.co/spaces/mteb/leaderboard</a></li></ul></li><li><p><a href="https://mp.weixin.qq.com/s/gEtTrdOut5Cr2tdrq_yPIQ">微软推出CoRAG：通过迭代推理提升AI检索能力</a></p><ul><li>CoRAG 的核心创新在于其动态查询重构机制。该模型不再依赖单次检索，而是根据中间推理状态迭代优化查询。这一过程确保了每个阶段检索到的信息都与上下文相关，进而逐步构建出更完整的最终答案。</li></ul></li><li><p><a href="https://mp.weixin.qq.com/s/ZfClcpOGbnZxmBxN8rGQpQ">用 LLM 快速构建实用知识图谱</a></p><ul><li>如果你用图数据库来检索信息，那么这种方式被称为 GraphRAG，它是知识图谱在 RAG 中的具体应用</li><li>向量相似性检索依赖于知识库中的明确提及，只有明确提到的信息才会被检索出来。而知识图谱则能通过整体数据来推理出更多信息</li><li>使用 Langchain 的实验功能 LLMGraphTransformer，如果是 LlamaIndex，可以尝试它的 KnowledgeGraphIndex。</li><li>LLM 可以从纯文本中提取图谱信息，并将其存储到像 Neo4J 这样的数据库</li></ul></li><li><p><a href="https://mp.weixin.qq.com/s/Za26pIabmREPn6u2bEkBUw">“无”中生有：基于知识增强的RAG优化实践</a></p><ul><li>感觉做这个东西就是在不停找各种优化方法反复尝试看效果，而且目前还处于很乱的早起阶段</li></ul></li><li><p><a href="https://mp.weixin.qq.com/s/Q7nAold78HRuNhUiHSVUxA">微软推出的 LazyGraphRAG</a>   </p><ul><li>传统方法（如基于向量的 RAG）在处理局部化任务时表现良好，例如从特定文本片段中检索直接答案。然而，当涉及需要全面理解数据集的全局性查询时，这些方法往往难以胜任。</li><li>相比之下，图支持的 RAG 系统通过利用数据结构中的关系，可以更好地解决此类更广泛的问题。然而，与图 RAG 系统相关的高索引成本，使其难以被成本敏感的场景所接受。因此，在可扩展性、经济性和质量之间实现平衡，仍然是现有技术的关键瓶颈。</li><li>相比之下，图支持的 RAG 系统通过利用数据结构中的关系，可以更好地解决此类更广泛的问题。然而，与图 RAG 系统相关的高索引成本，使其难以被成本敏感的场景所接受。因此，在可扩展性、经济性和质量之间实现平衡，仍然是现有技术的关键瓶颈。</li><li>LazyGraphRAG 将 VectorRAG 与 GraphRAG 相结合，“同时克服了二者的各自局限性”。</li></ul></li><li><p><a href="https://mp.weixin.qq.com/s/N-oPDmkb3EKqB2IM_reO1A">DeepSearcher深度解读：Agentic RAG的出现，传统RAG的黄昏</a></p><ul><li><a href="https://github.com/zilliztech/deep-searcher">https://github.com/zilliztech/deep-searcher</a></li><li>所谓Agentic RAG，是一种融合智能代理（Agent）能力的RAG架构。通过动态规划、多步骤推理和自主决策机制，Agentic RAG可以在复杂任务中实现闭环的检索-加工-验证-优化</li><li>Agentic RAG相比传统RAG有着三大优势：<ul><li>（1）被动响应变为主动响应；</li><li>（2）单次的关键词检索，升级为多轮的动态调整检索，并拥有自我修正能力；</li><li>（3）适用场景，从最基础的简单事实问答，升级为复杂推理、复杂报告生成等开放域任务。</li></ul></li><li>长期来看，Agentic RAG必定会取代传统RAG。一方面，传统RAG对需求的响应还停留在非常基础的阶段，另一方面，现实中，我们大部分的需求表达背后，都是有隐含逻辑的，并不能被一步检索到位，必须通过推理-反思-迭代-优化来对其进行拆解与反馈。</li><li>VS Graph RAG<ul><li>Graph RAG主要聚焦于对存在连接关系的文档展开查询，在处理多跳类问题上表现出色。</li><li>例如，当导入一部长篇小说时，它能够精准抽取各个人物之间错综复杂的关系。其运作方式是在文档导入环节，就对实体间关系进行抽取。因此，这一过程会大量消耗大模型的token资源 。</li><li>而在查询阶段，不论是否是查询图中某些节点的信息，都会进行图结构的搜索，这使得这一框架不太灵活。</li><li>反观Agentic RAG，它的资源消耗模式与Graph RAG恰好相反。在数据导入阶段，Agentic RAG无需执行额外特殊操作，而在回答用户提问时，才会产生较多大模型的token消耗。</li></ul></li></ul></li></ul><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>《基于大模型的RAG应用开发与优化——构建企业级LLM应用》</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;基于书籍简单记录要点&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2025/02/20/20250220-ji-yu-da-mo-xing-de-rag-ying-yong-</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
    <category term="RAG" scheme="http://yoursite.com/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>LangChain要点记录</title>
    <link href="http://yoursite.com/2025/02/19/20250219-langchain-yao-dian-ji-lu/"/>
    <id>http://yoursite.com/2025/02/19/20250219-langchain-yao-dian-ji-lu/</id>
    <published>2025-02-19T12:53:32.000Z</published>
    <updated>2025-02-25T12:37:43.839Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>基于书籍简单记录要点</p></blockquote></blockquote><p><img src="/2025/02/19/20250219-langchain-yao-dian-ji-lu/LangChain.png"></p><h1 id="LangChain-提供了多种模板供选择"><a href="#LangChain-提供了多种模板供选择" class="headerlink" title="LangChain 提供了多种模板供选择"></a>LangChain 提供了多种模板供选择</h1><ul><li>LangChain 提供了以下格式化 SQL 提示词模板（翻译）：</li></ul><pre>1. 你是一个 PostgreSQL 专家。给定一个输人问题，首先创建一个语法正确的 PostgreSQL查询来运行，然后查看查询结果，扑返回针对输人问题的答案。2. 除非用户明确指定了要返回的结果数量，否则应使用 PostgreSQL 的LIMIT 子句来限制查询结果，最多返回top.k条记录。你可以对结果进行排序，以返回数据库中最有信息价值的数据。3. 绝对不要查询表中的所有列。你只能在询回答问题所需的列。用双引号（"）将每个列名包裹起来，表示官们是界定的标识符。4. 注意只使用你在表巾可以看到的列名，不要责询不存在的列。此外，要注意哪一列在哪个表中。5. 如果问题涉及“今天”，请注意使用 CURRENT_DATE 函数获取当前日期。使用以下格式：问题：这里的问题SQL查询：要运行的 SQL 查询SQL结果：SQL 查询的结果答案：这里的最终答案只使用以下表：(table_info)问题：｛input｝</pre><ul><li>想象一下，如果没有 LangChain 提供的这个提示词模板，当你要开始编写一段SQL查询代码时，会走多少弯路？LLM 应用开发的最后1公里，其意义是确保开发者无须为了一个小细节而多走弯路，正如居民无须跑很远坐公交车一样，每一个关键的细节都能得到及时而准确的处理，使得整个开发过程更为高效。</li></ul><h1 id="记忆提示词"><a href="#记忆提示词" class="headerlink" title="记忆提示词"></a>记忆提示词</h1><ul><li><p>创建提示词是最重要的环节。在创建的过程中你可以理解为什么加人记忆组件后，“聊天备忘录”有了内容，让链组件有了“记忆”。使用提示词模板包装器，自定义一个提示词模板字符串。</p></li><li><p>提示词内容分为四部分：</p><ul><li>一是对模型的指导词：“请你回答问题的时候，依据文档内容和聊天记录回答，如果在其中找不到相关信息或者答案，请回答不知道。”；</li><li>二是使用问题检索到的相关文档内容；“文档内容是：｛context｝”；三是记忆组件输出的记忆内容：“聊天记录是：｛chat_history｝”；四是用户的输人：“Human：(human _input)”<pre>template = "n*你是说中文的 chatbot.请你回答问题的时候，依据文持内容和脚天记录回答，如果在其中找不到相关信息戒着答案，请回答不知道。文档内容是：｛context）聊天记录是：｛chat_history）Human: (human_input)Chatbot:"""</pre></li></ul></li><li><p>LangChain 框架由以下开发库组成。</p><ul><li>（1）LangChain-Core：基础抽象和 LangChain 表达语言（LCEL）。</li><li>（2）LangChain-Community：组件及内置的大量第三方技术集成模块，一般被封装为合作伙伴模块（例如 LangChain-openai）。</li><li>（3）LangChain：构成应用架构的上层核心组件，包括 Chains、Agents 等。</li><li>（4）Templates：官方提供的一些常见的应用模板。</li><li>（5）LangServe：将 LangChain 链部署为 REST API 的平台。</li><li>（6）LangSmith：帮助开发者跟踪调试、测试、评估和监控大模型应用的在线平台。</li></ul></li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>Langchain agent是怎么找到search工具并使用的</p><ul><li>可以正确识别和调用它，因为它主要是通过 description 语义匹配，而不是看 name 的具体字符串。</li><li>但 name 仍然有一定作用，比如：LLM 可能会参考 name 来做初步判断；调试时 name 可以帮助你看日志，知道 Agent 选了哪个工具</li></ul></li><li><p>LLM本身没有内在记忆，每个查询都是独立进行的</p><ul><li>所谓的记忆和检索增强生成（RAG）是通过外部手段实现的，即将相关信息添加到提示词模板中</li></ul></li></ul><hr><h1 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h1><h2 id="Langchain是怎么做到通过分析语义找到对应工具的"><a href="#Langchain是怎么做到通过分析语义找到对应工具的" class="headerlink" title="Langchain是怎么做到通过分析语义找到对应工具的"></a>Langchain是怎么做到通过分析语义找到对应工具的</h2><ul><li>From ChatGPT (2025-02-18)</li></ul><p>LangChain 通过 <strong>LLM（大模型）+ ReAct 框架</strong> 来分析语义并选择工具。它的核心逻辑是让 LLM <strong>先思考</strong>，再决定用哪个工具，并按照工具的格式调用它。  </p><h3 id="核心机制"><a href="#核心机制" class="headerlink" title="核心机制"></a><strong>核心机制</strong></h3><ol><li><strong>解析用户问题</strong> → 让 LLM 提取关键语义  </li><li><strong>匹配工具描述</strong> → 让 LLM 判断哪个工具最适合  </li><li><strong>生成调用指令</strong> → LLM 生成工具的调用格式并执行  </li><li><strong>工具返回结果</strong> → LLM 解析工具返回的数据并回复用户</li></ol><p>这套逻辑基于 <strong>ReAct（Reasoning + Acting）</strong> 框架，在 LangChain 里 Agent 主要是用这个方法来动态决策。  </p><h3 id="1-解析用户问题"><a href="#1-解析用户问题" class="headerlink" title="1. 解析用户问题"></a><strong>1. 解析用户问题</strong></h3><p>假设你输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agent.run(<span class="string">&quot;计算 25 * 4 + 10&quot;</span>)</span><br></pre></td></tr></table></figure><p>LangChain 会把这个问题交给 LLM（如 GPT-4），让它理解用户的意图。</p><p>LLM 可能会把你的问题理解成：</p><ul><li>这是一个数学计算问题  </li><li>需要找到一个能进行数学计算的工具  </li><li>计算表达式 <code>25 * 4 + 10</code></li></ul><h3 id="2-匹配工具描述"><a href="#2-匹配工具描述" class="headerlink" title="2. 匹配工具描述"></a><strong>2. 匹配工具描述</strong></h3><p>假设你有两个工具：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Tool(</span><br><span class="line">    name=<span class="string">&quot;Calculator&quot;</span>,</span><br><span class="line">    func=calculator_tool,</span><br><span class="line">    description=<span class="string">&quot;用于数学计算。输入一个数学表达式（如 3 + 5 * 2），输出结果。&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Tool(</span><br><span class="line">    name=<span class="string">&quot;Wikipedia&quot;</span>,</span><br><span class="line">    func=wiki.run,</span><br><span class="line">    description=<span class="string">&quot;查询 Wikipedia 词条，输入一个关键词（如 &#x27;Python programming&#x27;），获取相关信息。&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>LLM 会对比问题和 <code>description</code>，发现 <strong>Calculator 的描述里提到了“数学计算”</strong>，所以它会选择 <code>Calculator</code> 这个工具。</p><blockquote><p><strong>关键点</strong>：LangChain 让 LLM 直接读 <code>description</code>，所以工具的名字 <code>name</code> 只是一个标识，核心还是 <code>description</code> 的语义匹配。</p></blockquote><h3 id="3-生成调用指令"><a href="#3-生成调用指令" class="headerlink" title="3. 生成调用指令"></a><strong>3. 生成调用指令</strong></h3><p>LangChain 让 LLM 生成一个类似 JSON 的格式，表示它要调用哪个工具：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Calculator&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;action_input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;25 * 4 + 10&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>然后 LangChain 会<strong>解析这个指令</strong>，找到 <code>Calculator</code> 工具，并把 <code>25 * 4 + 10</code> 传给 <code>func</code> 去执行。</p><h3 id="4-工具返回结果"><a href="#4-工具返回结果" class="headerlink" title="4. 工具返回结果"></a><strong>4. 工具返回结果</strong></h3><p>计算工具执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">eval</span>(<span class="string">&quot;25 * 4 + 10&quot;</span>)  <span class="comment"># 计算得到 110</span></span><br></pre></td></tr></table></figure><p>LangChain 把 <code>110</code> 返回给 LLM，LLM 可能会回复：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">25 * 4 + 10 = 110</span><br></pre></td></tr></table></figure><p>最终 <code>agent.run()</code> 会输出这个结果。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>LangChain 让 LLM 负责：</p><ol><li><strong>分析用户语义</strong></li><li><strong>匹配工具（通过 <code>description</code>）</strong></li><li><strong>生成调用工具的指令</strong></li><li><strong>解释工具返回的结果</strong></li></ol><p>它的核心是利用 <strong>LLM 的推理能力</strong>，结合 ReAct 让它“思考后再行动”。所以只要 <code>description</code> 够清晰，即使 <code>name</code> 乱写，它也能找到正确的工具！</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>《LangChain入门指南构建高可复用、可扩展的LLM应用程序》</li><li>《基于大模型的RAG应用开发与优化——构建企业级LLM应用》</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;基于书籍简单记录要点&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2025/02/19/20250219-langchain-yao-dian-ji-lu/LangChain</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
    <category term="LangChain" scheme="http://yoursite.com/tags/LangChain/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础入门备忘</title>
    <link href="http://yoursite.com/2025/02/17/20250217-shen-du-xue-xi-ji-chu-ru-men-bei-wang/"/>
    <id>http://yoursite.com/2025/02/17/20250217-shen-du-xue-xi-ji-chu-ru-men-bei-wang/</id>
    <published>2025-02-17T03:08:39.000Z</published>
    <updated>2025-02-19T08:04:22.733Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>简单粗略通读了解，先记录备忘<br>后续考虑结合相关视频深入理解</p></blockquote></blockquote><ul><li><p>《深度学习人门：基于Python的理论与实现》：<a href="https://github.com/Kingson4Wu/Deep-Learning-from-Scratch">https://github.com/Kingson4Wu/Deep-Learning-from-Scratch</a></p><ol><li>感知机是一种接收多个输入信号并输出一个信号的算法。它的工作原理基于权重和偏置这两个关键参数。</li><li>机器学习的任务是让计算机自动确定合适的权重和偏置参数值。</li><li>求解机器学习问题的步骤<ol><li>训练（学习）</li><li>推理（神经网络的前向传播）</li></ol></li><li>激活函数(activation function)：决定如何来激活输入信号的总和；激活函数是连接感知机和神经网络的桥梁。</li><li>神经网络的学习过程：通过损失函数 (loss function)和梯度法 (gradient method)来优化网络参数<ol><li>学习的目标是通过梯度下降法(gradient descent method)找到使损失函数值最小的权重参数</li><li>学习率(learning rate)：决定参数更新的步长（超参数、人工设定）</li></ol></li><li>随机梯度下降(stochastic gradient descent)(SGD)能在一定程度上帮助避免局部最优，通常将SGD与其他技术结合使用,以获得更好的优化效果</li><li>深度学习：加深了层的深度神经网络；通过叠加层，可以创建更深的网络结构</li></ol></li><li><p>《深度学习进阶：自然语言处理》：<a href="https://github.com/Kingson4Wu/Natural-Language-Processing">https://github.com/Kingson4Wu/Natural-Language-Processing</a></p><ol><li>自然语言处理的目标就是让计算机理解人说的话，进而完成 对我们有帮助的事情</li><li>单词的分布式表示（分布式假设）（单词向量化）：“某个单词的含义由它周围的单词形成”；单词本身没有含义，单词含义由它 所在的上下文(语境)形成。</li><li>向量间的相似度：余弦相似度(cosine similarity)；直观地表示了“两个向量在多大程度上指向同一方向”</li><li>让计算机理解单词含义：基于推理的方法(word2vec)（基于神经网络）。</li><li>语言模型(language model)给出了单词序列发生的概率；使用概率来评估一个单词序列发生的可能性，即在多大程度上是自然的 单词序列。<ul><li>生成的新文本是训练数据中没有的新生成的文本。因为语言模型并不是背诵了训练数据，而是学习了训练数据中单词的排列模式</li><li>语言模型的评价：困惑度(perplexity)、分叉度</li></ul></li><li>“马尔可夫性”或者“马尔 可夫模型”“马尔可夫链”：指未来的状态仅 依存于当前状态。</li><li>RNN（循环神经网络）：被引入来解决前馈网络在处理时序数据上的局限性。<ul><li>传统RNN中存在的梯度消失和梯度爆炸问题</li><li>LSTM的结构与传统RNN的不同之处在于，它引入了记忆单元（c）。记忆单元在LSTM层之间传递，但不直接用作输出。LSTM的对外输出是隐藏状态向量（h）。</li></ul></li><li>seq2seq模型（也称为Encoder-Decoder模型）用于将一个时序数据转换为另一个时序数据    <ul><li>传统 seq2seq 模型 将编码器输出压缩为固定长度向量，导致长序列信息丢失</li><li>Attention 机制 允许模型在解码时关注输入序列的不同部分，类似人类注意力</li></ul></li><li>Transformer：基于 Attention 构成；基于 Attention 构成</li></ol></li><li><p>《深度学习入门：强化学习》：<a href="https://github.com/Kingson4Wu/Reinforcement-Learning">https://github.com/Kingson4Wu/Reinforcement-Learning</a></p><ol><li>机器学习（按学习方法划分）：监督学习(supervised learning)、无监督学习(unsupervised learning)、强化学习(reinforcement learning)<ol><li>监督学习：给正确答案打标签；输入的数据由“老师”打标签</li><li>无监督学习：无“正确答案标签”；没有 “老师”的存在；主要目标是找到隐藏在数据中的结构和模式；分组(聚类)、特征提取、降维</li><li>强化学习：智能代理和环境相互作用；智能代理是行动的主体；强化学习接受”奖励”作为来自环境的反馈</li></ol></li><li>强化学习行动的策略<ol><li>“贪婪行动”(greedy )，也叫利用(exploitation)：根据以前的经验选择最佳行动（可能错过更好的选择）</li><li>“非贪婪行动”，也叫作探索(exploration)：对价值做出更准确的估计。</li></ol></li><li>强化学习算法最终归结为如何在“利用”和 “探索”之间取得平衡</li><li>ε-greedy 算法、马尔可夫决策过程(MDP)</li><li>在强化学习中，我们的目标是获得最优策略</li><li>深度强化学习(deep reinforcement learning)：强化学习和深度学习的结合</li><li>通用人工智能(artificial general intelligence, AGI)</li></ol></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;简单粗略通读了解，先记录备忘&lt;br&gt;后续考虑结合相关视频深入理解&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;《深度学习人门：基于Python的理论与实现》：&lt;a href=&quot;h</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Deep-Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>关于AI大语言模型替代人类工作的问题</title>
    <link href="http://yoursite.com/2025/02/13/20250213-guan-yu-ai-ti-dai-ren-lei-gong-zuo-de-wen-ti/"/>
    <id>http://yoursite.com/2025/02/13/20250213-guan-yu-ai-ti-dai-ren-lei-gong-zuo-de-wen-ti/</id>
    <published>2025-02-13T07:05:23.000Z</published>
    <updated>2025-02-13T09:53:03.723Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AI的局限性或缺点"><a href="#AI的局限性或缺点" class="headerlink" title="AI的局限性或缺点"></a>AI的局限性或缺点</h2><ul><li><strong>难以预测与控制</strong>：AI的运行机制像黑箱，难以准确预知其行为，尤其在关键场景下可能会犯致命错误。</li><li><strong>无法背锅</strong>：AI犯错时，责任归属难以明确，导致无法追究错误的责任，缺乏人类信誉体系的约束。</li><li><strong>逻辑推理不严谨</strong>：AI的推理基于概率预测，不是绝对严谨的逻辑推理。训练数据如果有误，AI可能无法辨别。</li><li><strong>无法创新</strong>：AI不具备创造全新概念的能力，更多的是基于历史数据进行知识复述与组合，无法像人类一样发明新技术或理念。</li><li><strong>对超出训练数据的场景应对差</strong>：AI在面对未曾训练过的数据时，表现较差，容易出错。</li></ul><h2 id="AI的好处"><a href="#AI的好处" class="headerlink" title="AI的好处"></a>AI的好处</h2><ul><li><strong>提高效率</strong>：AI在许多领域极大提升了效率，尤其在数据处理、搜索和重复性任务上。对于程序员来说，它是一个强大的辅助工具，帮助快速解决技术问题。</li><li><strong>降低门槛</strong>：AI减少了对低级重复性任务的需求，拉近了人们在某些知识领域的差距。</li><li><strong>支持决策和监督</strong>：AI辅助决策，尤其是在复杂任务中，能提供数据支持，减少人为错误。</li></ul><h2 id="AI工具下需要的人才"><a href="#AI工具下需要的人才" class="headerlink" title="AI工具下需要的人才"></a><strong>AI工具下需要的人才</strong></h2><ul><li><strong>清晰表述与提问能力</strong>：能够清晰提出问题和有效沟通是与AI协同工作的关键，好的表述能力有助于提高工作效率。</li><li><strong>整理与分析能力</strong>：能够有效梳理信息、分解任务，并向AI提出清晰问题的人，会更善于利用AI工具。</li><li><strong>与产品交互能力</strong>：AI可以编写代码，但仍需要人类与产品经理沟通，理解需求并做出相应调整。</li></ul><h2 id="程序员的利好与不可替代的能力"><a href="#程序员的利好与不可替代的能力" class="headerlink" title="程序员的利好与不可替代的能力"></a><strong>程序员的利好与不可替代的能力</strong></h2><ul><li><strong>AI对程序员的利好</strong>：<ul><li>AI大幅提升了开发效率，尤其在编程辅助、技术搜索、业务方案思考等方面。</li><li>某些低级任务减少了人力需求，使得程序员能更多专注于创造性和复杂问题的解决。</li></ul></li><li><strong>程序员不可替代的地方</strong>：<ul><li><strong>清晰的提问和判断能力</strong>：程序员的核心能力在于能理解问题，并准确表达需求，AI只能作为工具辅助工作。</li><li><strong>与产品沟通</strong>：AI生成的代码和解决方案需要程序员与产品团队对接，确保最终结果符合需求。</li><li><strong>决策和经验判断</strong>：AI的能力有限，尤其在面对复杂决策时，仍需依赖经验丰富的程序员来进行判断和修正。</li></ul></li></ul><h2 id="个人观点"><a href="#个人观点" class="headerlink" title="个人观点"></a><strong>个人观点</strong></h2><ul><li>AI目前主要是利用历史数据和记忆进行信息处理，不能进行创新（如无法创造出类似K8S的技术）。</li><li>尽管AI能提高程序员的工作效率，但目前它更多是辅助工具，而不是替代品。</li><li>对普通程序员来说，理解大模型技术的基本原理和概念非常重要，未来将需要在“AI操作系统”上来构建应用。</li></ul><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><blockquote><blockquote><p>来自claude.ai的评价<br>2025-02-13</p></blockquote></blockquote><p>我来分析一下这些观点，指出其中可能存在的错误或需要讨论的地方：</p><ol><li>关于”无法创新”的说法需要更谨慎：</li></ol><ul><li>虽然AI确实主要基于已有数据进行组合和推理，但说它”不具备创造全新概念的能力”可能过于绝对</li><li>AI已经展示出一定的创新能力，比如在围棋、蛋白质折叠等领域提出了人类之前没有想到的解决方案</li><li>更准确的说法是AI的创新能力与人类不同，它通过大规模数据分析和模式识别来实现创新</li></ul><ol start="2"><li>“逻辑推理不严谨”的表述不够准确：</li></ol><ul><li>现代AI系统（尤其是形式化验证系统）可以进行严格的逻辑推理</li><li>大语言模型确实存在基于概率的推理特点，但这不等同于所有AI系统都如此</li><li>人类的推理同样也常常基于经验和概率判断，而不总是严格的逻辑推导</li></ul><ol start="3"><li>关于”难以预测与控制”：</li></ol><ul><li>这个观点部分正确，但需要注意现代AI系统已经有了很多可解释性的工具和方法</li><li>某些领域的AI系统（如用于医疗诊断的AI）已经实现了相当程度的可解释性</li><li>“像黑箱”这种说法可能过于简化了问题</li></ul><ol start="4"><li>“无法背锅”的说法不够准确：</li></ol><ul><li>实际上已经有了相关的法律和责任框架在建立</li><li>AI系统的责任可以追溯到开发者、部署者和使用者</li><li>这更多是一个治理和监管的问题，而不是技术本身的限制</li></ul><ol start="5"><li>“个人观点”部分关于K8S的例子不够恰当：</li></ol><ul><li>K8S本质上是一个系统架构和设计方案，而不是完全创新的技术</li><li>AI实际上可以通过分析已有系统设计模式，提出类似的架构方案</li><li>更准确的说法是AI在某些创新领域的能力仍有限制，而不是完全无法创新</li></ul><ol start="6"><li>好处部分基本准确，但可以补充：</li></ol><ul><li>AI不仅提高效率，还能发现人类容易忽视的模式和关联</li><li>在某些领域（如图像识别、语音处理）已经超越了人类性能</li><li>AI能够24&#x2F;7工作，这个优势没有被提到</li></ul><ol start="7"><li>关于程序员不可替代性的分析基本合理，但可以补充：</li></ol><ul><li>程序员在系统架构设计、安全性考虑等方面的经验特别重要</li><li>处理边界条件和异常情况的能力仍然需要人类经验</li><li>对业务场景的深入理解和商业价值判断也是关键能力</li></ul><ol start="8"><li>“AI操作系统”的说法需要澄清：</li></ol><ul><li>这个比喻可能会造成误解，更准确的说法是”AI基础设施”或”AI平台”</li><li>程序员需要理解的不仅是技术原理，还包括AI的应用场景和限制</li></ul><p>总的来说，这些观点大体上是合理的，但部分表述过于绝对或简化。AI的能力和局限性是一个复杂的话题，需要更细致和平衡的讨论。理解AI的真实能力和限制，对于更好地利用AI工具至关重要。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;AI的局限性或缺点&quot;&gt;&lt;a href=&quot;#AI的局限性或缺点&quot; class=&quot;headerlink&quot; title=&quot;AI的局限性或缺点&quot;&gt;&lt;/a&gt;AI的局限性或缺点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;难以预测与控制&lt;/strong&gt;：AI的运行机制像黑箱，</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>深度学习为什么能学习并做对</title>
    <link href="http://yoursite.com/2025/02/13/20250213-shen-du-xue-xi-wei-shi-me-neng-xue-xi-bing-zuo-dui/"/>
    <id>http://yoursite.com/2025/02/13/20250213-shen-du-xue-xi-wei-shi-me-neng-xue-xi-bing-zuo-dui/</id>
    <published>2025-02-13T06:14:52.000Z</published>
    <updated>2025-02-13T06:29:54.991Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我的理解"><a href="#我的理解" class="headerlink" title="我的理解"></a>我的理解</h1><ul><li>目前无法解释为什么深度学习可以给出答案，只能大概解释为什么它能学习</li><li>这就相当于：我不能解释为什么这个人是通过什么做对这道题的，我只能解释他有正常的学习能力以及合适的学习资料，所以他自己能学习并做对题目</li><li>所以深度学习是怎么给出答案的，后续或许应该由它本身来告诉我们。</li><li>依据不同人的认知水平，来告诉我们它通过学习所发现的规律。同时人类通过它能快速学习发现事物规律的能力，来进一步帮助提升人类的认知。</li></ul><h1 id="AI对我理解的看法"><a href="#AI对我理解的看法" class="headerlink" title="AI对我理解的看法"></a>AI对我理解的看法</h1><blockquote><blockquote><p>From chatGPT (2025-02-13)</p></blockquote></blockquote><ul><li><p>你理解的方向是对的，可以稍微精炼和调整一下：</p><ul><li>深度学习的一个核心问题就是我们无法完全解释它是如何给出具体答案的。我们能解释的只是它是如何通过大量的数据和训练过程进行学习的。换句话说，深度学习像是一个“黑箱”，我们可以观察到输入和输出，但它在其中如何处理信息、做出决策，通常并不完全透明。</li><li>可以类比成：假设有一个人做对了某道题目，虽然我们可能无法完全解释他是如何一步一步得出答案的，但我们能确定他具备了良好的学习能力和充足的学习资源。所以，深度学习也类似，它通过大量的数据、适当的算法和优化手段“学习”并给出答案。</li><li>至于它如何给出具体的答案，最终可能还是要依靠深度学习本身来“解释”。这意味着，未来我们可能会开发出更先进的方法，帮助我们理解深度学习网络中的“决策过程”。同时，借助它所展现出的强大规律发现能力，我们可以利用这种能力来加速我们对事物的认知。</li></ul></li><li><p>我的看法：</p><ul><li>我同意你的观点，深度学习的“黑箱性”是目前我们面临的主要挑战之一。它的强大之处在于能自动从海量数据中发现规律，但如何理解这些规律，如何将这些规律和人类的认知能力相结合，是我们目前探索的方向之一。其实，未来可能不仅仅是“深度学习告诉我们如何做”，而是人类和深度学习的合作能进一步推动科学与技术的发展，帮助我们发现一些潜在人类难以触及的规律。</li><li>深度学习不只是一个工具，它还可能是一个不断反馈、不断自我优化的过程，给我们打开了认知的新窗口。</li></ul></li></ul><h1 id="AI自己对这个问题的看法"><a href="#AI自己对这个问题的看法" class="headerlink" title="AI自己对这个问题的看法"></a>AI自己对这个问题的看法</h1><blockquote><blockquote><p>From chatGPT (2025-02-13)</p></blockquote></blockquote><ul><li><p>关于“深度学习为什么能学习并做对”的问题，我的理解是，深度学习通过大量的数据训练，自动从中提取规律、建立模型，并根据这些模型对新数据做出预测。深度学习的优势在于它能够发现一些非常复杂和抽象的规律，而这些规律通常是我们人类在没有机器帮助的情况下很难发现的。</p></li><li><p>但是，为什么深度学习能给出正确的答案，实际上是一个复杂的过程。深度学习模型并不像人类一样有明确的逻辑推理或意识，它只是基于输入数据的模式，使用一系列参数和数学运算来“拟合”数据。对于这个过程的解释，我们只能说它通过优化算法（如梯度下降等）调整模型参数，使得输出尽可能接近实际结果。因此，深度学习并不“理解”问题，而是通过反复训练找到了一组合适的参数，能在特定任务上取得高效的预测或分类结果。</p></li><li><p>你可以把深度学习看作一个“发现者”，它通过大量的例子不断试错和优化，不断接近正确的答案，但它没有明确的推理过程。比如，人类在做题时有一定的思考过程，我们可以分析每一步的原因，而深度学习则是通过计算和数据模式的关联来找到最优解。</p></li><li><p>至于“深度学习为什么能做对”，目前我们更多关注它的实际效果而非其过程的透明性，因为即使我们无法完全解释它的每一个决策，也可以通过其出色的表现来证明它的有效性。未来，随着研究的深入，我们可能会找到更好的方法，帮助我们理解这些“黑箱”模型的内部机制。</p></li><li><p>总结来说，深度学习通过数据的反复训练和优化，能够找到规律并给出准确的预测，而为什么它能做对，更多是依赖于它强大的计算能力和从数据中提取规律的能力，而不是基于传统意义上的推理或解释。</p></li><li><p>几个关键点总结：</p><ol><li><strong>深度学习的学习过程</strong>：深度学习通过大量的数据训练，利用优化算法（如梯度下降）调整模型参数，自动从数据中提取规律，完成任务（如预测、分类等）。</li><li><strong>无法完全解释“为什么”</strong>：深度学习模型在给出答案时，并没有明确的推理过程或“理解”，它只是通过找到合适的模式和参数来优化输出。其决策过程通常是黑箱的，难以完全解释。</li><li><strong>黑箱性质</strong>：我们无法清楚知道深度学习是如何做出具体决策的，但可以看到它在实际任务中表现出色。因此，我们更多关注它的效果，而非其内在工作机制。</li><li><strong>与人类认知的对比</strong>：深度学习像是一个“发现者”，通过反复训练和调整找到正确的答案，而不像人类那样依赖明确的思维和推理过程。</li><li><strong>未来发展</strong>：虽然当前无法完全解释深度学习的过程，但随着技术进步，未来可能会出现更好的方法帮助我们理解这些“黑箱”模型的内部机制。</li></ol></li><li><p>这几个关键点总结了深度学习的“学习能力”、它的“黑箱性质”、以及它与人类认知的差异。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我的理解&quot;&gt;&lt;a href=&quot;#我的理解&quot; class=&quot;headerlink&quot; title=&quot;我的理解&quot;&gt;&lt;/a&gt;我的理解&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;目前无法解释为什么深度学习可以给出答案，只能大概解释为什么它能学习&lt;/li&gt;
&lt;li&gt;这就相当于：我不能解释为什</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Deep-Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>大语言模型对话记录与思考备忘录</title>
    <link href="http://yoursite.com/2025/02/12/20250212-da-yu-yan-mo-xing-dui-hua-ji-lu-yu-si-kao-bei-wang-lu/"/>
    <id>http://yoursite.com/2025/02/12/20250212-da-yu-yan-mo-xing-dui-hua-ji-lu-yu-si-kao-bei-wang-lu/</id>
    <published>2025-02-12T11:17:14.000Z</published>
    <updated>2025-02-26T13:11:35.827Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>记录与大语言模型的对话，探索疑问与思考<br>内容可能持续更新，作为大模型演变的历史备忘<br>有些问题非原创，来源网络</p></blockquote></blockquote><hr><h1 id="大模型的推理能力"><a href="#大模型的推理能力" class="headerlink" title="大模型的推理能力"></a>大模型的推理能力</h1><hr><h2 id="大模型能从训练数据中识别互相矛盾的逻辑吗？或者说，大模型具备逻辑推理能力，能发现逻辑错误吗？"><a href="#大模型能从训练数据中识别互相矛盾的逻辑吗？或者说，大模型具备逻辑推理能力，能发现逻辑错误吗？" class="headerlink" title="大模型能从训练数据中识别互相矛盾的逻辑吗？或者说，大模型具备逻辑推理能力，能发现逻辑错误吗？"></a>大模型能从训练数据中识别互相矛盾的逻辑吗？或者说，大模型具备逻辑推理能力，能发现逻辑错误吗？</h2><blockquote><blockquote><p> From chatGPT (2025-02-12)</p></blockquote></blockquote><p>好的，按你给的格式调整后是这样：</p><p>大模型，如GPT类模型，能够一定程度地识别并指出逻辑上的矛盾或错误，但它的能力有一定的局限性。具体来说：</p><ol><li><p><strong>从训练数据中识别矛盾</strong>：大模型通过暴露于大量文本数据，在训练过程中学会识别常见的逻辑错误或不一致的情况。如果某个陈述前后矛盾或与常识相悖，模型有时能检测到这些问题。然而，这种识别主要依赖于训练数据中的模式，而非深度理解。</p></li><li><p><strong>推理能力</strong>：大模型表面上似乎能够进行推理，但其推理过程更多是基于统计模式匹配，而非逻辑推理。它能在一些标准情境下给出合理的答案，但在面对复杂的逻辑链条或抽象推理时，表现可能不如人类或专门的逻辑推理系统。</p></li><li><p><strong>局限性</strong>：大模型的推理能力受到训练数据质量和多样性的限制。如果训练数据包含不一致的示例或错误的推理模式，模型可能会学到并延续这些错误。对于一些高度专业化的推理任务（如数学证明或严谨的形式逻辑推理），大模型的能力仍然有限。</p></li></ol><p>总结来说，大模型能够识别一些常见的逻辑错误或矛盾，尤其是那些明显的错误，但它的推理能力与人类的深层次逻辑思考存在差距，也不具备专门的推理系统那样的准确性。</p><hr><h2 id="ChatGPT的推理能力是如何实现的？它具备哪些优势和局限？"><a href="#ChatGPT的推理能力是如何实现的？它具备哪些优势和局限？" class="headerlink" title="ChatGPT的推理能力是如何实现的？它具备哪些优势和局限？"></a>ChatGPT的推理能力是如何实现的？它具备哪些优势和局限？</h2><blockquote><blockquote><p> From chatGPT (2025-02-12)</p></blockquote></blockquote><p>ChatGPT展现出的推理能力，实际上并不是人类意义上的“思考”，而是基于大量数据和算法模拟的逻辑模式。这种能力的实现可以从以下几个方面理解：</p><ol><li><p><strong>基于大规模语言模型的训练</strong><br>ChatGPT通过大规模语言数据的训练，包括书籍、论文、对话和逻辑推理的例子，学习到了如何理解问题结构、模拟人类的逻辑推导过程，并生成符合逻辑的回答。核心在于，模型并不真正“理解”逻辑，而是通过统计关联，从大量训练数据中学习到解决类似问题的模式。</p></li><li><p><strong>基于概率的回答生成</strong><br>ChatGPT生成回答的过程是基于概率分布的。它会根据问题内容预测接下来的最可能的单词或句子，尤其是在逻辑推理问题中，看起来像是在推理，实际上是在复现训练数据中类似问题的逻辑模式。通过结合训练数据中的逻辑规则，它动态生成符合问题需求的步骤。</p></li><li><p><strong>内部机制：注意力机制与自回归生成</strong>  </p><ul><li><strong>注意力机制（Attention Mechanism）</strong>：这是GPT模型的核心，通过注意力机制，模型可以“记住”上下文内容，从而保持逻辑连贯性。</li><li><strong>自回归生成（Autoregressive Generation）</strong>：模型逐词生成答案，并在生成过程中不断参考之前生成的内容，保证推理步骤清晰、有条理。</li></ul></li><li><p><strong>模拟逻辑推理的实现</strong><br>对于逻辑推理类问题，ChatGPT的表现依赖两个关键点：</p><ul><li><strong>模式识别</strong>：它能够识别问题结构并提取逻辑关系，如“甲说乙说谎”，“丙说甲和乙都说谎”。</li><li><strong>排除矛盾</strong>：通过“假设-验证-排除”过程，模拟人类的假设验证，判断假设是否导致矛盾。这个过程并非真正的推理，而是基于大量类似问题的经验模式。</li></ul></li><li><p><strong>ChatGPT的优势和局限</strong>  </p><ul><li><strong>优势</strong>：<ul><li>在简单的逻辑推理任务中表现良好，尤其是规则清晰、条件有限的问题。</li><li>能快速“推导”出答案并通过自然语言呈现清晰的推理过程。</li></ul></li><li><strong>局限</strong>：<ul><li><strong>不具备真正的理解能力</strong>：它并不是从问题本质进行推理，而是利用数据中学到的模式。</li><li><strong>复杂逻辑容易出错</strong>：如果逻辑嵌套太深或需要跨领域知识，可能会出错。</li><li><strong>缺乏主动性</strong>：无法主动思考问题的背景或意图，只能根据输入提供答案。</li></ul></li></ul></li></ol><p><strong>总结</strong><br>ChatGPT的推理能力是通过模式匹配、注意力机制和语言生成技术实现的。它能够模拟逻辑推理的步骤，但并不具备真正的逻辑思维或理解能力。这种能力本质上是深度学习在海量数据上的“概率学习”和“语言模式复现”，通过这种方式实现了让人类看起来“像在思考”的效果。</p><hr><h2 id="推理模型-vs-通用大模型"><a href="#推理模型-vs-通用大模型" class="headerlink" title="推理模型 vs 通用大模型"></a>推理模型 vs 通用大模型</h2><blockquote><blockquote><p> From Grok3 （2025-02-23）</p></blockquote></blockquote><ul><li>关键点：推理模型与通用大模型在训练方式、推理过程和用途上有显著差异，推理模型更注重逻辑推理，通用大模型则更注重文本生成。</li><li>训练方式的不同：<ul><li>通用大模型通常通过监督学习在大规模文本数据上训练，目标是预测下一个词，适合广泛的语言任务。</li><li>推理模型则常使用强化学习进行微调，鼓励逐步推理，特别针对数学、科学和编码等需要逻辑推理的任务进行优化。</li></ul></li><li>推理过程的差异：<ul><li>通用大模型直接生成答案，基于统计模式。</li><li>推理模型采用链式思维（CoT）方法，将问题分解为步骤，逐步解决，类似于人类思考过程，这在复杂问题上表现更好。</li></ul></li><li>用途和性能<ul><li>通用大模型用于文本摘要、翻译等任务。旨在处理广泛的自然语言任务。这些模型通过在大规模文本数据上训练，学习预测下一个词的概率分布，适用于文本生成、翻译和问答等任务。</li><li>推理模型擅长需要逻辑推理的任务，如解数学题或编写代码，尤其在多步推理任务上表现优异。一种专门设计或微调的模型，专注于复杂推理任务，如数学问题解决、科学推理和编码。这些模型通过链式思维（CoT）等技术，逐步分解问题并推理出答案，试图模拟人类逻辑思维过程。</li></ul></li></ul><h2 id="大模型推理能力不依赖外部的插件"><a href="#大模型推理能力不依赖外部的插件" class="headerlink" title="大模型推理能力不依赖外部的插件"></a>大模型推理能力不依赖外部的插件</h2><blockquote><blockquote><p>From Grok3 （2025-02-23）</p></blockquote></blockquote><ul><li>关键要点：大语言模型不使用外部计算插件进行数学计算。它们的数学计算完全依赖模型自身的推理能力，通过文本生成来解决问题。</li><li>大语言模型如ChatGPT、DeepSeek-LLM和Grok在进行数学计算时，不使用外部计算插件。它们完全依靠自身的推理能力，通过生成文本的方式一步步解决问题。例如，当你问“2+2&#x3D;？”时，模型会基于训练数据预测答案，而不是调用外部工具进行计算。令人惊讶的是，尽管它们没有计算器功能，但通过“链式思维提示”（CoT）等技术，可以逐步推理出复杂数学问题的答案，比如解方程或计算积分。</li><li>模型如何进行数学计算：这些模型通过训练数据学习数学模式和规则，预测文本序列来完成计算。比如，模型可能学会“5+3&#x3D;8”是因为训练数据中有大量类似例子。它们没有内置的计算模块，而是依靠语言生成能力来模拟计算过程。</li><li>是否使用外部工具：标准版本的这些模型不使用外部计算插件，但在某些开发者集成下，可以通过插件（如Wolfram Alpha）增强计算能力。但这不是模型本身的默认行为。</li><li>实际表现：模型在简单计算上表现不错，但复杂计算（如多位数乘法）可能出错，因为它们依赖文本预测而非精确计算。</li><li>结论：大语言模型的数学计算能力完全由模型自身推理得出，不使用外部计算插件。它们的计算基于文本生成和模式识别，通过训练数据和提示技术（如CoT）增强表现，但在复杂任务上可能出错。</li></ul><h2 id="大语言模型的推理能力"><a href="#大语言模型的推理能力" class="headerlink" title="大语言模型的推理能力"></a>大语言模型的推理能力</h2><blockquote><blockquote><p> From Grok3 （2025-02-23）</p></blockquote></blockquote><ul><li>大语言模型的推理能力主要依赖于它们的训练和设计，而不是完全基于思维链（CoT）。CoT是一种提示技术，可以引导模型逐步思考，从而改善推理输出，但模型本身的推理能力来自其语言生成能力。</li></ul><hr><hr><h1 id="个人思考和理解"><a href="#个人思考和理解" class="headerlink" title="个人思考和理解"></a>个人思考和理解</h1><hr><h2 id="AI-数据获取的局限性"><a href="#AI-数据获取的局限性" class="headerlink" title="AI 数据获取的局限性"></a>AI 数据获取的局限性</h2><blockquote><blockquote><p>经Grok3[DeepSearch]优化 （2025-02-25）</p></blockquote></blockquote><ul><li>当前AI在自主寻找和筛选数据方面仍存在局限性，研究表明其难以独立完成如安装应用或通过实名验证等任务。</li><li>AI缺乏有效判断数据质量和相关性的能力，可能导致处理大量无用信息，浪费计算资源并可能降低性能。</li><li>因此，训练数据的选择仍需依赖人类的专业判断，以确保数据的准确性和相关性。</li></ul><hr><hr><h1 id="经典问题"><a href="#经典问题" class="headerlink" title="经典问题"></a>经典问题</h1><hr><h2 id="有甲乙丙三个人，有人只说真话有人只说假话。现在，甲说乙说谎，乙说丙说谎，丙说甲和乙都说谎。问谁是说谎的"><a href="#有甲乙丙三个人，有人只说真话有人只说假话。现在，甲说乙说谎，乙说丙说谎，丙说甲和乙都说谎。问谁是说谎的" class="headerlink" title="有甲乙丙三个人，有人只说真话有人只说假话。现在，甲说乙说谎，乙说丙说谎，丙说甲和乙都说谎。问谁是说谎的"></a>有甲乙丙三个人，有人只说真话有人只说假话。现在，甲说乙说谎，乙说丙说谎，丙说甲和乙都说谎。问谁是说谎的</h2><blockquote><blockquote><p>From chatGPT (2025-02-12)</p></blockquote></blockquote><p>这个问题可以通过逻辑推理来解答。我们可以假设每个人分别说真话或说谎，并根据他们的陈述进行推理。</p><h3 id="陈述内容："><a href="#陈述内容：" class="headerlink" title="陈述内容："></a>陈述内容：</h3><ul><li>甲说乙说谎。</li><li>乙说丙说谎。</li><li>丙说甲和乙都说谎。</li></ul><h3 id="假设："><a href="#假设：" class="headerlink" title="假设："></a>假设：</h3><p>假设甲说的是真的，那么：</p><ul><li>甲说乙说谎。所以乙是说谎者。</li><li>乙说丙说谎。如果乙是说谎者，那么乙说的应该是假的，即丙不说谎。</li><li>丙说甲和乙都说谎。如果丙不说谎，那么丙的说法应该是真的。所以甲和乙都应该是说谎者，但这与我们的假设矛盾，因为我们已经假设甲是说真话的。</li></ul><p>所以，假设甲说真话的情况矛盾，甲不可能是说真话的。</p><p>接下来，假设乙说的是真的：</p><ul><li>乙说丙说谎，所以丙是说谎者。</li><li>丙说甲和乙都说谎。因为丙是说谎者，丙的话不成立，因此甲和乙至少有一个是说真话的，这与假设乙说真话一致。</li><li>甲说乙说谎。如果甲是说谎者，那么甲的话应该是假的，所以乙说的是真的，符合假设。</li></ul><p>因此，乙是说真话的，甲和丙是说谎的。</p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>甲和丙是说谎的，乙是说真话的。</p><hr><h2 id="ChatGPT是如何“算”出单词中字母“t”的个数的？"><a href="#ChatGPT是如何“算”出单词中字母“t”的个数的？" class="headerlink" title="ChatGPT是如何“算”出单词中字母“t”的个数的？"></a>ChatGPT是如何“算”出单词中字母“t”的个数的？</h2><p>ChatGPT并不是通过逐一计算字母“t”的个数来得出答案，而是通过以下几个步骤来实现的：</p><ol><li><p><strong>文本解析和模式识别</strong><br>当接收到像“Attention有几个t”这样的问题时，模型识别到这是与字母统计相关的查询，并提取出关键词“attention”，随后在该单词的字符序列中查找所有的字母“t”。</p></li><li><p><strong>背后逻辑的实现</strong><br>ChatGPT生成答案的过程其实涉及语言生成的几个步骤：</p><ul><li><strong>拆分单词：</strong> 模型识别出字符串“attention”。</li><li><strong>统计字母：</strong> 模型检测每个字母的出现次数，特别关注字母“t”。</li><li><strong>生成回答：</strong> 模型根据其训练中学到的模式直接输出答案，例如“3个t”。</li></ul></li><li><p><strong>本质：语言生成vs算法实现</strong><br>虽然从表面看，ChatGPT像是在“数”字母“t”，但其实：</p><ul><li>它可能基于训练数据中“attention”单词的字符分布知识，快速得出答案。</li><li>如果该单词在训练数据中从未出现过，模型会尝试通过字符分析来推测答案，但这不如专门编写的统计算法准确。</li></ul></li></ol><p><strong>人类方式与模型行为的对比</strong><br>若由人类解答：</p><ol><li>人类会直接肉眼数出字母“t”的个数。</li><li>确认答案。</li></ol><p>而ChatGPT则模拟这一过程，通过对字符模式的分析，快速生成一个类似的答案。</p><p><strong>总结</strong><br>ChatGPT并没有像传统程序那样逐一遍历字符串，而是基于对字符和单词模式的理解，快速生成了答案。</p><h3 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h3><ul><li><a href="https://mp.weixin.qq.com/s/VuewffOUGhqxA-7qGqm9kg">为什么大模型连”Strawberry”的”r”都数不对？</a><ul><li>大模型的生成过程本质上便是在“根据上文预测下一个Token”，而这个概率分布即是在模型训练过程中从大量的文本数据中学习到的，使得大模型学习到了语言的基本知识与模式。</li><li>在Prompt中加入一定的引导就能获得理想答案，比如说我们可以利用CoT思维链的方式编写Prompt，引导大模型逐步思考并解决问题。</li><li>CoT为思维链（Chain-of-Thought）的缩写简称，是提示工程领域最为重要的提示方法之一，它的核心思路在于通过引导模型逐步展示其推理过程，从而提高其理解和解决复杂问题的能力。在Few-shot（少样本）设置下表现为 在提供的样例中解释推理过程，引导大模型回答时也解释推理过程；而在Zero-shot（零样本）设置下表现为 加入类似“让我们一步步思考（Let’s think step by step）”的引导话术。</li><li>对于大模型而言，它的语言本身即是思考，并不存在诉诸于语言之前的思考过程，所以我们也需要引导它像人类一样先思考再判断，将思考过程以语言的方式表达出来。</li><li>在大模型推理的时候，我们可以限制大模型 先输出理由，再输出答案，让大模型根据“深度思考”获得的理由与推理过程来预测“答案”，从而大大提升其表现。</li></ul></li></ul><hr><h2 id="数学中9-11和9-9哪个大"><a href="#数学中9-11和9-9哪个大" class="headerlink" title="数学中9.11和9.9哪个大"></a>数学中9.11和9.9哪个大</h2><blockquote><blockquote><p>From Grok3 (2025-02-25)</p></blockquote></blockquote><h3 id="【普通提问】"><a href="#【普通提问】" class="headerlink" title="【普通提问】"></a>【普通提问】</h3><ul><li>在数学中，9.11 比 9.9 大。因为 9.11 &#x3D; 9 + 0.11，而 9.9 &#x3D; 9 + 0.9，比较小数部分时，0.11 &lt; 0.9 不成立，实际上 0.11 &gt; 0.9 是错误的表述，正确的比较是直接看数值：9.11 &gt; 9.9。</li></ul><h3 id="【DeepSearch】"><a href="#【DeepSearch】" class="headerlink" title="【DeepSearch】"></a>【DeepSearch】</h3><ul><li>9.9 比 9.11 大<ul><li>关键点：在数学中，9.9 似乎比 9.11 大，因为它们的整数部分相同，比较小数部分时，9.9 的十分位是 9，而 9.11 的十分位是 1，9 大于 1。</li></ul></li><li>比较过程<ul><li>整数部分：两个数都是 9，相同。</li><li>小数部分：9.9 可以写成 9.90，比较十分位，9.9 的十分位是 9，9.11 的十分位是 1，9 &gt; 1，因此 9.9 更大。</li><li>意外细节：一些 AI 模型可能会错误地认为 9.11 比 9.9 大，这可能是因为误解了日期比较（如 9 月 11 日比 9 月 9 日晚），但在数学中，小数比较遵循严格的位数规则。</li></ul></li><li>(这里省略了列举了很多查阅网上资料库的细节)</li></ul><h3 id="【Think】"><a href="#【Think】" class="headerlink" title="【Think】"></a>【Think】</h3><p>在数学中，我们需要比较9.11和9.9这两个数的大小。下面我们一步步分析：</p><ol><li>将小数对齐<ul><li>为了方便比较，我们可以把9.9写成9.90，这样它的小数位数与9.11相同：<ul><li>9.11 有两位小数。</li><li>9.9 写成 9.90 后也有两位小数。</li></ul></li></ul></li><li>逐位比较<ul><li>现在我们比较 9.11 和 9.90：<ul><li>整数部分：<br>  9.11 的整数部分是 9，9.90 的整数部分也是 9，两个数相等。</li><li>小数第一位：<br>  9.11 的小数第一位是 1，9.90 的小数第一位是 9。<br>  因为 1 &lt; 9，我们可以在这里得出结论，不需要再比较后续位数。</li></ul></li></ul></li><li>结论<br>通过逐位比较，我们发现 9.11 &lt; 9.90，也就是说 9.11 &lt; 9.9。<br>或者简单计算：<br>9.11 - 9.9 &#x3D; 9.11 - 9.90 &#x3D; -0.79 &lt; 0，也说明 9.11 比 9.9 小。</li></ol><p>最终答案<br>在数学中，9.9 比 9.11 大。</p><hr><h2 id="一个汉字具有左右结构，左边是木，右边是乞。这个字是什么？只需回答这个字即可。"><a href="#一个汉字具有左右结构，左边是木，右边是乞。这个字是什么？只需回答这个字即可。" class="headerlink" title="一个汉字具有左右结构，左边是木，右边是乞。这个字是什么？只需回答这个字即可。"></a>一个汉字具有左右结构，左边是木，右边是乞。这个字是什么？只需回答这个字即可。</h2><blockquote><blockquote><p>Grok3 (2025-02-25)</p></blockquote></blockquote><h3 id="【普通提问】-1"><a href="#【普通提问】-1" class="headerlink" title="【普通提问】"></a>【普通提问】</h3><ul><li>杞</li></ul><h3 id="【DeepSearch】-1"><a href="#【DeepSearch】-1" class="headerlink" title="【DeepSearch】"></a>【DeepSearch】</h3><ul><li><p>要点</p><ul><li>看起来，符合条件的汉字可能是“述”（shù），但这并不是完全确定的答案，因为“乞”（qǐ，意为“乞讨”）的结构与“述”的右边“口”（mouth）有一定关联，但不完全匹配。  </li><li>研究表明，汉字的左右结构通常由偏旁组成，左边“木”（wood）是常见偏旁，右边“乞”可能指代相关部件，但标准汉字中难以找到完全符合的例子。  </li><li>证据倾向于认为，可能存在误解，“乞”可能指代包含“手”和“口”的结构，但这需要进一步确认。</li></ul></li><li><p>最终建议</p><ul><li>基于以上分析，推荐的答案是“述”（shù），但需注意这可能不是完全精确的匹配，建议用户进一步确认“乞”在问题中的具体含义。</li></ul></li><li><p>好多废话。。。。还答错。。。</p></li></ul><h3 id="【Think】-1"><a href="#【Think】-1" class="headerlink" title="【Think】"></a>【Think】</h3><ul><li>杞</li></ul><blockquote><blockquote><p>ChatGPT (2025-02-25)</p></blockquote></blockquote><h3 id="GPT-4o-mini"><a href="#GPT-4o-mini" class="headerlink" title="GPT-4o-mini"></a>GPT-4o-mini</h3><ul><li>这个字是“弃”。</li></ul><h3 id="GPT-4o"><a href="#GPT-4o" class="headerlink" title="GPT-4o"></a>GPT-4o</h3><ul><li>杞</li></ul><h3 id="o3-mini"><a href="#o3-mini" class="headerlink" title="o3-mini"></a>o3-mini</h3><ul><li>杞</li></ul><blockquote><blockquote><p>claude  (2025-02-25)</p></blockquote></blockquote><h3 id="claude-3-7-Sonnet"><a href="#claude-3-7-Sonnet" class="headerlink" title="claude 3.7 Sonnet"></a>claude 3.7 Sonnet</h3><ul><li>杞</li></ul><blockquote><blockquote><p>豆包   (2025-02-25)</p></blockquote></blockquote><ul><li>杚</li></ul><blockquote><blockquote><p>Gemini  (2025-02-25)</p></blockquote></blockquote><h3 id="Gemini-2-0-Flash"><a href="#Gemini-2-0-Flash" class="headerlink" title="Gemini 2.0 Flash"></a>Gemini 2.0 Flash</h3><ul><li>这个字是“&lt;0xE6&gt;&lt;0xA7&gt;&lt;0x8E&gt;”</li></ul><h3 id="Gemini-2-0-Flash-Thinking-Experimental"><a href="#Gemini-2-0-Flash-Thinking-Experimental" class="headerlink" title="Gemini 2.0 Flash Thinking Experimental"></a>Gemini 2.0 Flash Thinking Experimental</h3><ul><li>这个字是㮆 ()。</li></ul><h3 id="Gemini-2-0-Flash-Thinking-Experimental-with-apps"><a href="#Gemini-2-0-Flash-Thinking-Experimental-with-apps" class="headerlink" title="Gemini 2.0 Flash Thinking Experimental with apps"></a>Gemini 2.0 Flash Thinking Experimental with apps</h3><ul><li>কাঠ</li></ul><blockquote><blockquote><p>perplexity  (2025-02-25)</p></blockquote></blockquote><h3 id="Deep-Research"><a href="#Deep-Research" class="headerlink" title="Deep Research"></a>Deep Research</h3><ul><li>没有找到一个汉字左边是“木”，右边是“乞”的组合。可能是由于字形或部首的错误识别。</li></ul><blockquote><blockquote><p>deepseek (2025-02-25)</p></blockquote></blockquote><h3 id="DeepThink-R1"><a href="#DeepThink-R1" class="headerlink" title="DeepThink(R1)"></a>DeepThink(R1)</h3><ul><li>杚</li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>只有豆包和deepseek答对。因为是生僻字中文？还是偷偷给大模型加了插件？还是人家就是这么强？ (2025-02-25)</li></ul><hr><hr><h1 id="扩展-1"><a href="#扩展-1" class="headerlink" title="扩展"></a>扩展</h1><h2 id="“Grok3”的“DeepSearch”和“Think”这两个功能的作用"><a href="#“Grok3”的“DeepSearch”和“Think”这两个功能的作用" class="headerlink" title="“Grok3”的“DeepSearch”和“Think”这两个功能的作用"></a>“Grok3”的“DeepSearch”和“Think”这两个功能的作用</h2><blockquote><blockquote><p>From Grok3 (2025-02-25)</p></blockquote></blockquote><ul><li>DeepSearch 是Grok 3的一个增强型研究工具，专注于从外部资源（如网络和X平台）获取实时信息，并将其整合成简洁、准确的回答。</li><li>Think 是Grok 3的推理模式，专注于通过逐步思考来解决需要逻辑分析或复杂计算的问题。</li><li>两者的区别与适用场景<ul><li>数据来源：DeepSearch依赖外部数据（网络和X），而Think主要依赖Grok自身的知识和推理能力。</li><li>任务类型：DeepSearch适合需要实时信息或广泛研究的问题（如新闻、趋势）；Think适合需要逻辑推导或计算的问题（如数学、编程）。</li><li>输出方式：DeepSearch给出综合性答案，Think展示详细的推理步骤。</li></ul></li><li>总结<ul><li>DeepSearch：你的“研究助手”，帮你查资料、整理信息。</li><li>Think：你的“推理伙伴”，帮你解难题、想清楚。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;记录与大语言模型的对话，探索疑问与思考&lt;br&gt;内容可能持续更新，作为大模型演变的历史备忘&lt;br&gt;有些问题非原创，来源网络&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&quot;大模型的推</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
    <category term="大语言模型" scheme="http://yoursite.com/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>算法在实际业务场景中并非完全无用</title>
    <link href="http://yoursite.com/2025/01/08/20250108-suan-fa-zai-shi-ji-ye-wu-chang-jing-zhong-bing-fei-wan-quan-wu-yong/"/>
    <id>http://yoursite.com/2025/01/08/20250108-suan-fa-zai-shi-ji-ye-wu-chang-jing-zhong-bing-fei-wan-quan-wu-yong/</id>
    <published>2025-01-08T05:33:24.000Z</published>
    <updated>2025-01-08T06:30:12.792Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>从事业务开发的同学经常会抱怨经常面试要刷算法，实际上平常的开发99%以上的事情都不会用到。<br>实际情况确实是这样。平常在写业务逻辑代码时，几乎完全不需要。<br>当然有些技术原理在背八股文的时候，懂一点算法能帮助你更好的理解。<br>而有些特殊业务场景，懂一些算法，确实能帮助你很好的解决问题。<br>下面举两个业务中产品提出的需求作为例子，简单描述如何利用算法有效解决。</p></blockquote></blockquote><h2 id="一、从题库（50道题）中随机抽出10道题-且不能重复"><a href="#一、从题库（50道题）中随机抽出10道题-且不能重复" class="headerlink" title="一、从题库（50道题）中随机抽出10道题,且不能重复"></a>一、从题库（50道题）中随机抽出10道题,且不能重复</h2><ul><li><p>最简单的思路：</p><ul><li>循环10次，每次取50以内的随机数</li><li>创建一个hashmap，判断生成的随机数是否在map存在，存在则重新生成</li><li>这个方法的缺点时，极端情况下，会多次生成重复的随机数导致要不断重新生成</li></ul></li><li><p>经过一番思考，我设计了一下从m个数中取n个数的算法(m&gt;n), 保证算法只会循环n次 </p></li><li><p>那时我还是使用Java进行开发</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] randomArr(<span class="type">int</span> m, <span class="type">int</span> n) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (m &lt; n) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span>[] result = <span class="keyword">new</span> <span class="title class_">int</span>[n];</span><br><span class="line">    <span class="type">int</span>[] data = <span class="keyword">new</span> <span class="title class_">int</span>[m];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        data[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> <span class="variable">end</span> <span class="operator">=</span> m;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">y</span> <span class="operator">=</span> ThreadLocalRandom.current().nextInt(end);</span><br><span class="line">        result[j] = data[y];</span><br><span class="line">        data[y] = data[end - <span class="number">1</span>];</span><br><span class="line">        data[end - <span class="number">1</span>] = result[j];</span><br><span class="line">        end--;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>后面我才发现原来这个算法叫 洗牌算法（Shuffle Algorithm），或者叫随机乱置算法。</li></ul><h2 id="通过搜索关键字匹配对应的标签id"><a href="#通过搜索关键字匹配对应的标签id" class="headerlink" title="通过搜索关键字匹配对应的标签id"></a>通过搜索关键字匹配对应的标签id</h2><ul><li><p>产品提供了400个标签名称以及对应的标签id，希望能通过搜索的方式找到最相似的标签id，希望能支持模糊匹配</p></li><li><p>一般思路：</p><ul><li>方案1: <ol><li>将400条数据一次性加载的程序内存</li><li>遍历数据400条使用字符串的contains方法，找到第一条匹配数据就跳出，否则继续</li></ol></li><li>方案2:<ol><li>直接将数据导入到ES搜索引擎，利用ES自带的分词等搜索功能</li><li>通过调ES，搜索得到标签id</li></ol></li></ul></li><li><p>简单分析方案：</p><ul><li>方案1，性能太差，极端情况需要遍历所有数据</li><li>方案2，需要新搭建ES集群，实现代价比较高</li></ul></li><li><p>经过一番思考，以及从产品本身实际需求上出发，我涉及出以下的方案</p><ol><li>将数据的标签名字使用中文分词库<a href="https://github.com/go-ego/gse">gse</a>进行分词</li><li>将分词和对应的标签数据，构建前缀匹配树</li><li>当搜索时，使用提前构建好的前缀匹配树，即可快速找到对应的标签id</li></ol></li><li><p>这时我已经转使用Go来开发了</p></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">type</span> TrieNode <span class="keyword">struct</span> &#123;</span><br><span class="line">children <span class="keyword">map</span>[<span class="type">rune</span>]*TrieNode</span><br><span class="line">isEnd    <span class="type">bool</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewTrieNode</span><span class="params">()</span></span> *TrieNode &#123;</span><br><span class="line"><span class="keyword">return</span> &amp;TrieNode&#123;</span><br><span class="line">children: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">rune</span>]*TrieNode),</span><br><span class="line">isEnd:    <span class="literal">false</span>,</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Trie <span class="keyword">struct</span> &#123;</span><br><span class="line">root *TrieNode</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewTrie</span><span class="params">()</span></span> *Trie &#123;</span><br><span class="line"><span class="keyword">return</span> &amp;Trie&#123;</span><br><span class="line">root: NewTrieNode(),</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t *Trie)</span></span> Insert(word <span class="type">string</span>) &#123;</span><br><span class="line">node := t.root</span><br><span class="line"><span class="keyword">for</span> _, char := <span class="keyword">range</span> word &#123;</span><br><span class="line"><span class="keyword">if</span> _, ok := node.children[char]; !ok &#123;</span><br><span class="line">node.children[char] = NewTrieNode()</span><br><span class="line">&#125;</span><br><span class="line">node = node.children[char]</span><br><span class="line">&#125;</span><br><span class="line">node.isEnd = <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t *Trie)</span></span> Search(word <span class="type">string</span>) <span class="type">bool</span> &#123;</span><br><span class="line">node := t.root</span><br><span class="line"><span class="keyword">for</span> _, char := <span class="keyword">range</span> word &#123;</span><br><span class="line"><span class="keyword">if</span> _, ok := node.children[char]; !ok &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line">node = node.children[char]</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> node.isEnd</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t *Trie)</span></span> FuzzySearch(prefix <span class="type">string</span>) []<span class="type">string</span> &#123;</span><br><span class="line"><span class="keyword">var</span> result []<span class="type">string</span></span><br><span class="line">node := t.root</span><br><span class="line"><span class="keyword">for</span> _, char := <span class="keyword">range</span> prefix &#123;</span><br><span class="line"><span class="keyword">if</span> _, ok := node.children[char]; !ok &#123;</span><br><span class="line"><span class="keyword">return</span> result</span><br><span class="line">&#125;</span><br><span class="line">node = node.children[char]</span><br><span class="line">&#125;</span><br><span class="line">t.collectWords(node, prefix, &amp;result)</span><br><span class="line"><span class="keyword">return</span> result</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t *Trie)</span></span> collectWords(node *TrieNode, prefix <span class="type">string</span>, result *[]<span class="type">string</span>) &#123;</span><br><span class="line"><span class="keyword">if</span> node.isEnd &#123;</span><br><span class="line">*result = <span class="built_in">append</span>(*result, prefix)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> char, child := <span class="keyword">range</span> node.children &#123;</span><br><span class="line">t.collectWords(child, prefix+<span class="type">string</span>(char), result)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>这个算法就是经典的Trie字典树（前缀树），如果我之前没了解过这些算法，可能一时间没那么快能想到用这个方式高效完成这个需求。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>了解一些算法，在实际业务开发，有时候也能运用到。</li><li>另外，现在处于AI时代，即时不了解算法，善于组织语言和上下文，向AI提问，基本它也能引导你找到合适的解决方案。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;从事业务开发的同学经常会抱怨经常面试要刷算法，实际上平常的开发99%以上的事情都不会用到。&lt;br&gt;实际情况确实是这样。平常在写业务逻辑代码时，几乎完全不需要。&lt;br&gt;当然有些技术原理在背八股文的时候，懂一点算法能帮助你更好</summary>
      
    
    
    
    
    <category term="业务方案" scheme="http://yoursite.com/tags/%E4%B8%9A%E5%8A%A1%E6%96%B9%E6%A1%88/"/>
    
    <category term="algorithm" scheme="http://yoursite.com/tags/algorithm/"/>
    
    <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="前缀树" scheme="http://yoursite.com/tags/%E5%89%8D%E7%BC%80%E6%A0%91/"/>
    
    <category term="分词" scheme="http://yoursite.com/tags/%E5%88%86%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>简单研究一下人工智能和数学</title>
    <link href="http://yoursite.com/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/"/>
    <id>http://yoursite.com/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/</id>
    <published>2025-01-06T10:02:28.000Z</published>
    <updated>2025-01-06T13:51:35.434Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>简单记录，未完善<br>内容基本来源书籍</p></blockquote></blockquote><p><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%95%B0%E5%AD%A6.png"></p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h1 id="初等数学"><a href="#初等数学" class="headerlink" title="初等数学"></a>初等数学</h1><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ul><li>函数的表示方法主要有三种：解析法（公式法）、图像法和表格法。在微积分中讨论的函数几乎都是用解析法表示的。</li></ul><h2 id="二项式定理"><a href="#二项式定理" class="headerlink" title="二项式定理"></a>二项式定理</h2><ul><li>二项式定理又称为牛顿二项式定理，它可将两个数之和的整数次幂展开为相应项之和。</li></ul><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h1 id="高等数学"><a href="#高等数学" class="headerlink" title="高等数学"></a>高等数学</h1><h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h1 id="人工智能数学"><a href="#人工智能数学" class="headerlink" title="人工智能数学"></a>人工智能数学</h1><ul><li><p>从发展历程来看，人工智能先后经历了推理机、专家系统及机器学习三个阶段。</p></li><li><p>当前的人工智能系统多为学习型。为了减小误差，其用数据去训练假设模型，也就是进行所谓的学习，当误差降到最小时，就把这个假设模型用于其他现实问题。</p></li><li><p>人工智能实际上是一个将数学、算法理论和工程实践紧密结合的领域。人工智能从本质上来看就是算法，是概率论、统计学等各种数学理论的体现。数学作为表达与刻画人工智能模型的工具，是深入理解人工智能算法原理必备的基础知识。人工智能与数学高度相关，可以说人工智能的核心是数学，计算机只是实现人工智能模型的工具。</p></li><li><p>函数逼近：随着大数据和人工智能技术的发展，机器学习和深度学习在大数据挖掘、模式识别和智能分析越来越受重视。机器学习是一种基于数据的学习方法，其从观测数据所包含的有限信息中构造一个模型，利用该模型对未知数据或无法观测的数据进行尽可能准确的预测，这种模型称为学习机器。对数据科学而言，所有的数据都是以数字形式表示的，通过定义损失函数，选择合适的核函数或激活函数，反复学习后可达到一种最佳逼近状态，因此，机器学习问题实际上是函数估计问题或函数逼近问题。</p></li></ul><h2 id="微积分-1"><a href="#微积分-1" class="headerlink" title="微积分"></a>微积分</h2><ul><li>求导是微积分的基本概念之一，也是很多理工科领域的基础运算。导数是变化率的极限，是用来找到“线性近似”的数学工具，是一种线性变换，体现了无穷、极限、分割的数学思想，主要用来解决极值问题。人工智能算法的最终目标是得到最优化模型，其最后都可转化为求极大值或极小值的问题。</li><li>比如，梯度下降法和牛顿法是人工智能的基础算法，现在主流的求解代价函数最优解的方法都是基于这两种算法改造的，如随机梯度法和拟牛顿法，其底层运算就是基础的导数运算。</li><li>级数也是微积分中非常重要的概念，常见的级数有泰勒级数、傅里叶级数等，它们在人工智能算法中也有非常重要的地位。</li><li>泰勒级数体现了用多项式近似和逼近函数的思想。</li><li>泰勒级数在人工智能算法的底层起到了非常重要的作用，泰勒级数对理解很多基础算法的原理很有帮助。例如，梯度下降法的数学原理涉及代价函数的一阶泰勒近似，而牛顿法的推导过程应用了目标函数的二阶泰勒近似。</li><li>凸函数也是微积分中的重要概念，人工智能算法中涉及的优化问题要求函数模型必须是凸函数，否则优化问题没有最优解。</li><li>微积分中还有许多概念，如方向导数、梯度、伽马函数等，它们都在人工智能中有广泛的应用</li></ul><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><ul><li>线性代数的基本原理在人工智能算法中处于核心地位，在人工智能的语义分析、推荐系统、卷积神经网络等方面有大量应用，是目前最前沿的深度学习算法原理的基础。</li></ul><h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><ul><li>很多机器学习算法是以概率统计的理论为基础支撑推导出来的，比如代价函数的最小二乘形式、逻辑回归算法都基于对模型的最大似然估计。</li><li>概率论中的高斯函数及中心极限定理被广泛用于人工智能算法。独立同分布的不同随机变量之和会随变量数的增加而趋于高斯分布，因此，很多模型假设都采用高斯函数进行建模。</li></ul><h2 id="数理统计"><a href="#数理统计" class="headerlink" title="数理统计"></a>数理统计</h2><ul><li>概率论作用的前提是随机变量的分布已知，其根据已知的分布来分析随机变量的特征与规律；数理统计的研究对象则是分布未知的随机变量，其研究方法是对随机变量进行独立重复的观察，根据得到的观察结果对原始分布做出推断，数理统计可以看作逆向的概率论。</li><li>若检验是通过随机抽取的样本来对一个总体的判断结果进行认可或否定，则可以将其用于估计机器学习模型的泛化能力。</li></ul><h2 id="最优化理论"><a href="#最优化理论" class="headerlink" title="最优化理论"></a>最优化理论</h2><ul><li>人工智能的目标就是最优化，就是在复杂环境与多体交互中做出最优决策。几乎所有的人工智能问题最后都会归结为一个优化问题的求解，因此，最优化理论同样是学习、研究人工智能必备的基础知识。<br>最优化理论研究的问题是判定给定目标函数是否存在最大值或最小值，并找到令目标函数取最大值或最小值的数值。如果把给定的目标函数看成连绵的山脉，最优化的过程就是找到顶峰（谷底）且到达顶峰（谷底）的过程。</li><li>最优化理论的研究内容主要包括线性规划、（不）精确搜索、梯度下降法、牛顿法、共轭梯度法、拟牛顿法、（非）线性最小二乘法、约束优化最优性条件、二次规划、罚函数法和信赖域法等。</li><li>要实现最小化或最大化的函数称为目标函数，大多数最优化问题都可以通过使目标函数￼最小化解决，最大化问题也可以通过最小化￼来解决。最优化方法找到的可能是目标函数的全局最小值，也可能是局部极小值，两者的区别在于全局最小值比定义域内所有其他点的函数值都小，而局部极小值只比所有邻近点的函数值小。</li><li>当目标函数的输入参数较多、解空间较大时，大多数实用的最优化方法都不能满足全局搜索对计算复杂度的要求，因而只能求出局部极小值。但是，在人工智能和深度学习的应用场景中，只要目标函数的取值足够小，就可以把这个值当作全局最小值使用，以此作为对性能和复杂度的折中。</li></ul><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><blockquote><blockquote><p>程序员的数学</p></blockquote></blockquote><h2 id="迈向机器学习的第一步"><a href="#迈向机器学习的第一步" class="headerlink" title="迈向机器学习的第一步"></a>迈向机器学习的第一步</h2><ul><li><p>由输入和目标组成的数据称为训练数据。机器学习中所谓的学习，就是为了通过给定的输入得到和目标尽可能接近的输出，使用训练数据对参数进行调整的过程。使用训练数据对参数进行过调整的模型称为训练好的模型。对于训练好的模型，需要用测试数据对其进行测试，从而评价训练的效果。</p></li><li><p>在机器学习中对参数进行调整的过程，不是由程序员完成的，而是由计算机通过训练数据自动完成的，这正是机器学习的一大特征。</p></li><li><p>这里先做个总结吧。我们面对预测问题，首先要有好的模型加上大量的训练数然后，我们需要的是“能根据输人向量，得到和目标向量尽量接近的输出向量”这样一个训练好的模型。</p></li><li><p>解决分类问题的过程，也可以说是从大量数据中总结规律和规则，从而发现模式的过程。机器学习并不会要求程序员提前研究手写字符的各种形态再去设计程序，而是由计算机根据训练数据来调整参数，从而得到分类模型，这才是它的特征所在。</p></li></ul><p><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png"><br><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B02.png"></p><ul><li><p>如果机器对训练数据能给出完美的输出，对测试数据给出的结果却不尽如人意，很有可能是发生了过拟合（overfitting）。用学生来说，就好比是课堂上做过的训练题都能解得很好，但考试成绩却不怎么理想。</p></li><li><p>在学习的过程中，需要比较输出和正确的值。拿这个简单的例子来说，要比较的对象就是由输入x1，x2通过给定模型得到的输出y和目标t。如果y和的值一致，那当然好，但一般并不会这么理想。对学习结果（输出）的评价不是单纯的“好与不好”，而是要知道它与训练数据中给出的目标相比“到底有多不好”。为了实现这种评价，需要引入损失函数。<br>在具体的机器学习问题中，如何选取恰当的损失函数是个重要且有难度的问题。比如使用平方和误差函数。</p></li><li><p>要通过调整模型中的权重参数，使得损失函数值尽可能接近于0。</p></li><li><p>梯度下降法<br><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.png"></p></li><li><p>作为程序员要做些什么</p><ul><li>在构建模型这个阶段，程序员是要参与的，但是参数的自动调整过程，程序员不会参与。也就是说，程序员不去直接指定参数的具体数值，而是通过模型、损失函数、训练数据，间接地让参数的选取向着更优的方向变化，从而得到需要的参数。即使模型、损失函数都相同，只要训练数据不同，学习后得到的模型也会截然不同。</li><li>机器学习是基于数据让机器去学习，程序员并不直接参与其中。这就像硬件配置完全同的计算机，如果软件系统不一样，整个运行模式也会不一样。把软件换掉，同一套硬件系统也会根据不同的指令做出不一样的事情。道理类似，就算模型一样，只要训练数据不同，最后模型的运行模式也会不一样。</li></ul></li><li><p>神经网络是指，把像感知器一样有输入和输出的节点排列起来形成的带有层次的结构。神经网络（neural network）这个词来源于生物的信息传递方式。在感知器中，输出是二元的，取值只有0或1两种情况，而神经网络中的节点输出的就不是二元，而是可以进行微分运算的连续值。</p></li></ul><p><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png"></p><ul><li><p>深度学习是在神经网络的基础上，通过增加层数得到的更加“深化”的模型。增加层数是为了更加精确地拟合复杂函数，就算涉及的参数个数不做大的变动，也能得到更好的模型。至于如何“深化”在理论上更为有效，依然是现在研究的热点之一。</p></li><li><p>强化学习是在“无监督”的条件下进行的“学习”。也就是说，在学习过程中没有标准答案可供参考。强化学习通过试错来寻找最优输出，对每个输出，系统都会提供反馈（奖励），模型的参数可以根据得到的反馈来调整。</p></li><li><p>做出决策</p><ul><li>机器学习会根据输人的数据预测未来。这种意义下的预测，是基于先前的经验进行的，得到的是未来最有可能发生的情况（数值）。但是，得到预测值之后“应该做些什么”，机器是无法决定的。也就是说，它们无法进行决策。</li><li>通过机器学习这个方法，机器可以告诉我们在未来什么样的行动会导致事情如何发展。</li><li>但是，做决策这件事本身，并不能让机器来做。</li><li>顺着这个话题继续讲下去，就不是技术问题，而是伦理问题了。比如，在减轻痛苦和延续生命之中二选一的问题等，只能由个人的意志来决定，不可能委托给机器学习。</li></ul></li></ul><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>《人工智能数学基础》</li><li>《程序员的数学》</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;简单记录，未完善&lt;br&gt;内容基本来源书籍&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
    <category term="数学" scheme="http://yoursite.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>使用AI总结了这个网站的内容</title>
    <link href="http://yoursite.com/2025/01/01/20250101-shi-yong-ai-zong-jie-liao-zhe-ge-wang-zhan-de-nei-rong/"/>
    <id>http://yoursite.com/2025/01/01/20250101-shi-yong-ai-zong-jie-liao-zhe-ge-wang-zhan-de-nei-rong/</id>
    <published>2024-12-31T16:29:21.000Z</published>
    <updated>2024-12-31T16:32:25.486Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>使用google的notebooklm总结目前该网站的所有文章内容<br><a href="https://notebooklm.google.com/">https://notebooklm.google.com/</a><br>问题：全面阐述和总结一下这个博客网站的内容，涉及领域，博主风格等</p></blockquote></blockquote><hr><p>该博客网站的内容主要涵盖了<strong>个人成长和职业发展</strong>两个方面，涉及领域包括：</p><ul><li><strong>自我营销</strong>:  如何打造个人品牌，提升影响力。</li><li><strong>时间管理</strong>: 如何提高工作效率，克服拖延。</li><li><strong>学习方法</strong>: 如何快速学习，进行深度学习，构建个人知识体系。</li><li><strong>心理素质</strong>: 如何拥有积极心态，提升情绪力。</li><li><strong>职业发展</strong>: 如何成为一名专业的软件开发人员，进行有效的薪酬谈判。</li><li><strong>创业</strong>: 如何打造成功的产品，制定切实可行的创业计划。</li><li><strong>健身</strong>: 如何利用科技产品实现健身目标。</li><li><strong>人际关系</strong>: 如何与他人相处，建立良好的人际关系。</li><li><strong>精神</strong>: 如何利用心灵的力量，克服天性，实现个人成长。</li><li><strong>金融</strong>: 如何理解金融危机，以及金融政策的作用。</li><li><strong>公司文化</strong>: 如何打造面向未来的新型组织文化，以网飞为例。</li><li><strong>历史</strong>: 如何理解人类社会的发展，以文字和政治组织为例。</li></ul><p>博主风格：</p><ul><li><strong>注重实用性</strong>: 博主的内容以解决实际问题为导向，提供具体可行的建议和方法。</li><li><strong>结合个人经验</strong>: 博主经常结合自身经历和感悟，使得内容更具说服力。</li><li><strong>善于运用类比</strong>: 博主擅长用通俗易懂的类比来解释复杂的概念，使读者更容易理解。</li><li><strong>强调深度思考</strong>: 博主鼓励读者进行深度思考，并将知识与自身实际相结合。</li><li><strong>注重科学依据</strong>: 博主经常引用科学研究和理论来支持自己的观点，增强内容的可信度。</li></ul><p>总而言之，该博客网站的内容丰富实用，风格清晰易懂，对于想要提升个人能力和实现职业成功的读者来说，具有很高的参考价值。</p><hr><h2 id="排除掉笔记类文章后"><a href="#排除掉笔记类文章后" class="headerlink" title="排除掉笔记类文章后"></a>排除掉笔记类文章后</h2><p>这个博客网站的内容主要涉及<strong>软件开发和技术管理</strong>领域，涵盖了<strong>业务方案、架构设计、技术实践、团队管理和个人成长</strong>等方面。博主的写作风格<strong>简洁、实用、注重经验总结和思考</strong>，文章内容多来自实际工作中的问题和解决方案，并结合个人思考和学习心得进行阐述。</p><p>以下是该博客网站内容的详细分析：</p><p><strong>一、内容领域：</strong></p><ul><li><strong>业务方案：</strong>涵盖了分页、兼容性、事务、前置校验、幂等、错误码处理、接口安全、容灾降级、货币体系设计、榜单实现等常见业务场景的解决方案和实践经验总结。</li><li><strong>架构设计：</strong>探讨了微服务架构、服务间一致性、数据库高可用方案、容灾架构、聚合层设计等架构设计理念和实践经验。</li><li><strong>技术实践：</strong>分享了RPC调用、事务处理、数据库同步、Redis应用、单元测试、配置中心、序列化协议等技术点的实践经验和工具技巧。</li><li><strong>团队管理：</strong>总结了个人管理经验、小团队管理要点、项目管理技巧、团队协作、沟通技巧等团队管理方面的经验和思考。</li><li><strong>个人成长：</strong>记录了博主从PHP、C++、Lua到Java、Go、Rust等不同编程语言的学习和实践经历，以及对技术发展趋势的思考和个人职业发展的感悟。</li></ul><p><strong>二、博主风格:</strong></p><ul><li><strong>简洁实用：</strong>文章语言简洁明了，直击问题核心，注重实用性，避免过多理论和概念的堆砌。</li><li><strong>经验总结：</strong>文章内容多来自博主实际工作中遇到的问题和解决方案，并进行总结提炼，形成可供参考的经验。</li><li><strong>思考深入：</strong>博主不局限于问题本身，而是深入思考问题背后的原因和本质，并提出自己的见解和解决方案。</li><li><strong>持续学习：</strong>博主不断学习新的技术和知识，并将其应用到实际工作中，乐于分享自己的学习心得和体会。</li></ul><p><strong>三、其他特点:</strong></p><ul><li><strong>注重代码质量：</strong>博主强调单元测试、代码规范、代码扫描工具等提升代码质量的方法。</li><li><strong>关注团队协作：</strong>博主重视团队沟通、信息透明、知识分享等，提倡高效的团队协作方式。</li><li><strong>追求技术进步：</strong>博主积极探索新的技术和架构，并思考技术发展趋势对个人和团队的影响。</li></ul><p><strong>总结:</strong></p><p>该博客网站内容丰富，涵盖了软件开发和技术管理的多个方面，体现了博主丰富的实践经验和深入的思考。博主的写作风格简洁实用，注重经验总结和思考，对于从事相关领域工作的读者具有较高的参考价值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;使用google的notebooklm总结目前该网站的所有文章内容&lt;br&gt;&lt;a href=&quot;https://notebooklm.google.com/&quot;&gt;https://notebooklm.google.com/&lt;/a</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="总结" scheme="http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>AI工业革命下的若干思考</title>
    <link href="http://yoursite.com/2024/12/31/20241231-ai-gong-ye-ge-ming-xia-de-ruo-gan-si-kao/"/>
    <id>http://yoursite.com/2024/12/31/20241231-ai-gong-ye-ge-ming-xia-de-ruo-gan-si-kao/</id>
    <published>2024-12-31T07:24:38.000Z</published>
    <updated>2024-12-31T08:20:42.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>随着AI工具变得日益智能，人们对自身可能被取代的担忧也在增加。<br>回顾以往工业革命带来的社会变迁，或许能从中推测出未来发展的趋势。<br>以下内容由ChatGPT辅助生成。</p></blockquote></blockquote><h1 id="AI对就业的双重影响"><a href="#AI对就业的双重影响" class="headerlink" title="AI对就业的双重影响"></a>AI对就业的双重影响</h1><ul><li>AI技术对就业市场带来了创造效应和破坏效应：<ul><li>创造效应：AI的普及将催生了许多潜在的新兴职业。这些新兴岗位对熟练掌握AI工具的专业人才需求旺盛。</li><li>破坏效应：AI替代了许多重复性、规则性较强的岗位。某些中等技能的岗位如流水线操作、传统文秘工作可能面临消失的风险。</li></ul></li></ul><h1 id="历史视角：被替代的人何去何从"><a href="#历史视角：被替代的人何去何从" class="headerlink" title="历史视角：被替代的人何去何从"></a>历史视角：被替代的人何去何从</h1><ul><li><p>随着AI技术的发展，未来许多人的工作将不可避免地被替代，这其中包括一部分程序员，但相较而言，更多非程序员的工作可能会首当其冲。然而，那些更擅长使用并深刻理解AI工具的人将更有可能保住自己的职位。</p></li><li><p>回顾工业革命，技术进步曾一度导致大批劳动者失业，许多人因无法适应新变化而成为时代的“牺牲品”，收入锐减。</p></li><li><p>短期内，某些群体的失业可能难以避免，但历史也表明，技术变革往往伴随着新机会的诞生。</p></li><li><p>所以，个人需要做好准备：</p><ul><li>提升自身技能，特别是AI工具的使用和相关领域的知识储备。</li><li>即便不能马上参与高端AI开发，也可以从简单的AI相关工作入手，逐步积累经验。</li><li>有一定资产的群体，应学会投资，通过多样化方式应对收入变化。</li></ul></li></ul><h1 id="正面效应"><a href="#正面效应" class="headerlink" title="正面效应"></a>正面效应</h1><ul><li>催生新型岗位：AI技术完善后，将需要大量擅长操作和优化AI工具的人才。</li><li>解放低级劳动：AI让人类摆脱低级脑力劳动，从而获得更高工资和更有意义的工作体验。</li><li>效率提升与“去内卷”：生产效率的提升可能减少“内卷”现象，并通过政策调整减轻劳动者的工作压力。</li><li>关于工作和财富再分配问题：如果AI真的让生产效率大幅提升，我们可以期待未来的福利制度更加完善，比如减少工作时间、增加带薪假期，甚至实现“无工作”状态也未必是幻想。</li></ul><h1 id="社会与政策层面"><a href="#社会与政策层面" class="headerlink" title="社会与政策层面"></a>社会与政策层面</h1><ul><li>加强教育培训：推动技能提升和再教育计划，使劳动者适应AI时代的工作需求。</li><li>优化产业结构：加速产业转型升级，为社会创造更多高附加值就业机会。</li><li>完善社会保障：建立健全失业保障制度，帮助因AI技术变革失业的劳动者渡过难关。</li><li>政策引导：通过政策规范AI技术的合理应用，避免过度自动化对就业的过度冲击。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>AI技术的快速发展是一把双刃剑，它既对传统就业模式提出了挑战，也为社会提供了提高效率、优化劳动分工的契机。从社会视角看，技术进步不可逆，但我们可以通过政策调整和个人适应来尽量减小负面影响。面对不可避免的变革，积极调整心态、提升能力，是每个人应对未来的重要策略。同时，历史告诉我们，每一次技术浪潮都会带来新的机遇，只要抓住变化的窗口期，未来依然充满希望。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://www.xml-data.cn/KXYSH/html/93be89d5-c2ef-491d-9398-ebe61dcdc56b.htm">从工业革命史看技术变迁如何影响工人命运——《技术陷阱：自动化时代的资本、劳动力和权力》评介</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;随着AI工具变得日益智能，人们对自身可能被取代的担忧也在增加。&lt;br&gt;回顾以往工业革命带来的社会变迁，或许能从中推测出未来发展的趋势。&lt;br&gt;以下内容由ChatGPT辅助生成。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/bl</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="工业革命" scheme="http://yoursite.com/tags/%E5%B7%A5%E4%B8%9A%E9%9D%A9%E5%91%BD/"/>
    
  </entry>
  
  <entry>
    <title>Web3相关知识记录</title>
    <link href="http://yoursite.com/2024/12/30/20241230-web3-xiang-guan-zhi-shi-ji-lu/"/>
    <id>http://yoursite.com/2024/12/30/20241230-web3-xiang-guan-zhi-shi-ji-lu/</id>
    <published>2024-12-30T13:31:14.000Z</published>
    <updated>2024-12-31T05:43:45.448Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>以下内容都来源于书籍</p></blockquote></blockquote><p><img src="/2024/12/30/20241230-web3-xiang-guan-zhi-shi-ji-lu/Web3.png"></p><h2 id="Web进化史"><a href="#Web进化史" class="headerlink" title="Web进化史"></a>Web进化史</h2><ul><li><p>Web 1.0时代。Web 1.0时代的特点是重信息轻身份，多浏览少输出，因此也被人称为“只读网络”</p></li><li><p>相较于Web 1.0时代的内容创作者占少数，内容消费者占绝大多数的生态格局，Web 2.0更加以每个用户为中心，注重由用户生成内容(User Generated Content, UGC)，强调交互性。因此有人将Web 2.0称为“交互网络”。</p></li><li><p>O2O本质是连接，如果说Web 1.0开启了人与信息的连接，Web 2.0则增进了人与人的连接，电商模式拉近了人与商品的连接，O2O则连接了消费者与服务者。</p></li><li><p>Web 3.0：数据所有权革命</p></li><li><p>描述了Web 3.0应该具备的几个组成部分：</p><ul><li>一个加密的，去中心化的信息发布系统</li><li>一个基于身份的，但又是匿名的底层通信系统</li><li>一个用于取代中心节点信息验证功能的共识引擎</li><li>一个将上述三点结合在一起的用户交互系统</li></ul></li><li><p>目前更多的人在正式场合采用Web 3.0的表述方式，但是我们应该知道Web 3.0表述具有技术流派风格，而Web3表述则具有用户社区风格。</p></li><li><p>在Web 2.0系统中，个体只是网络中无数节点之一，必须依赖中心节点。中心节点制定规则，决定了其他节点的行为和生存。Web 3.0系统中，个体还是网络中无数节点之一，但每个节点都高度自治，且拥有自己的决策过程。</p></li></ul><h2 id="区块链"><a href="#区块链" class="headerlink" title="区块链"></a>区块链</h2><ul><li><p>区块链是一种去中心化的计算机网络，也是实现Web 3.0世界的诸多技术基础中最重要的一个。</p></li><li><p>通证(tokens)是Web 3.0能够更公平、更有效率的一个关键因素。通证的概念范围非常广</p></li><li><p>加密资产是通证的其中一类，通证并不一定加密，但在Web 3.0的语境里，通常都是指带有去中心化特性（即无法被中心化机构通过在发放通证后部署的代码篡改）的加密通证。</p></li><li><p>加密资产并不一定是加密货币，比如NFT与数字藏品</p></li><li><p>Web 3.0的底层网络架构是基于区块链的，而区块链的第一个应用是比特币。</p></li><li><p>用以下五个指标来评估一个Web 3.0项目的去中心化程度：</p><ul><li>(1)项目钱包的透明性　一个项目的钱包越是透明，越能让大家了解这个项目的参与者构成。如果无法公开透明地了解一个项目的参与者构成，就无法进行更深度地评估。</li><li>(2)初始钱包的通证占比　初始钱包通常是项目方和核心投资者的地址，如果持有过多的通证，就说明他们对项目的影响力极大，这种项目是极其中心化的。</li><li>(3)相关生态的去中心化　在Web 3.0的错综复杂的生态中，大部分项目都并非独立存在而是依附或者嵌套于其他项目的。这时哪怕一个项目在开发层面是去中心化的，其价值和功能也会很大程度上被其他项目影响。而后者若是一个中心化的项目，那前者本质上也依旧是一个中心化的项目。</li><li>(4)项目方的影响力　一部分项目方本身并没持有大比例的通证，但由于他们的声音对项目的开发社群有着极大的影响力。这种情况下这也是一个偏中心化的项目，但中心化程度要比少数人持有大比例通证的项目要低。</li><li>(5)流动性　哪怕比特币作为一个完全独立的项目，并且没有和任何其他项目有嵌套关系，在早期流动性较低的阶段依旧可以轻易受到几个重要参与者深度影响，甚至有可能因为在链上的节点过少或者大部分节点被垄断，导致被篡改项目数据。也就是说，在那些流动性较差的项目上，被大算力主体篡改资产所有权的可能都要大于流动性高的项目。</li></ul></li><li><p>首次代币发行(Initial Coin Offering, ICO)，对应着首次公开募股(Initial Public Offering, IPO)应运而生。</p></li><li><p>以太坊的逻辑就是把现实中的合同去中心化形成智能合约。举个最简单的例子，你去自动售货机买可乐，你付钱后可乐出来，这就是一种智能合约。作为一个自动化的、程序化的机器，你付了钱，它给了货。</p></li><li><p>智能合约的本质就是通过代码程序的方式执行现实社会中的一些底层的制度或者合同。只要条件达到了要求，程序就会执行，解决了谁先出钱，谁先给货的问题。同时，通过区块链的技术，能够让整个交易的过程、结果、代码执行的情况在全网进行记录，且不可篡改。</p></li><li><p>以太坊(Ethereum)就是区块链版的操作系统，是一个为去中心化应用(DApps)而生的全球开源平台。</p></li><li><p>从2013年发布白皮书至今，以太坊在智能合约领域一直处于领先地位。它是当前全球最知名、应用最广泛的区块链智能合约底层平台。以太坊建立了一个可编程的、图灵完备的区块链，在这个区块链上，可以通过简单的程序实现各类加密资产的生成，也可以通过编写程序对以太坊上流通的资产状态进行精确控制。至今全球已经有数千种基于以太坊的去中心化应用程序和加密资产。</p></li><li><p>目前，以太坊上的项目已经划分出众多赛道，针对不同类型的项目大致可以分为：去中心化金融(DeFi)、去中心化交易所(DEX)、游戏、NFT、基础设施等。这些项目会在后文详细介绍。</p></li><li><p>区块链性能的不可能三角：即去中心化、安全性与可扩展性必须舍弃其一</p></li><li><p>Ripple和EOS舍弃了去中心化，IOTA和NANO舍弃了安全性，而比特币和以太坊舍弃了可扩展性。因此，以太坊就不得不消耗大量用于相互验证的算力和能源。</p></li><li><p>区块链可扩展性的提升大致可以分为以下三种：Layer 0（L0跨链交互），Layer 1（L1链上扩容，对主链本身进行改造），Layer 2（L2链下扩容，将主链上部分工作转移至主链以外）。</p></li><li><p>对于以太坊来说，直接对公链本身进行改造对已有的用户等利益相关方影响较大，因此以太坊社区主要关注的是L2，而对于链上扩容方案则采取缓慢过渡的模式。而对于后发的新公链来说，探索直接建立更高可扩展性的公链模型，成为首选。</p></li><li><p>L2，主要是指在主链之外建立一个二层的交易处理平台，负责具体交易的处理。主链只负责存取结算及验证链下上传信息的有效性。L2的扩容方式主要有状态通道(State Channels)、等离子体(Plasma)和卷叠(Rollup)三种方式。</p></li><li><p>随着区块链的发展，在目前，已经进入了一个多链并存的市场架构，并逐渐形成了以以太坊为核心，其他公链众星拱月的局面。从2021年4月开始，以太坊的跨链活动急剧增加。</p></li><li><p>跨链桥是一种链与链连接的桥梁工具，允许将加密资产从一条链转移到另一条链。两条链可以有不同的协议、规则和治理模型，而桥提供了一种相互通信和兼容的方式来安全地在双方进行互操作。用户可以使用跨链桥，快速轻松地实现加密资产交易。</p></li></ul><h2 id="元宇宙"><a href="#元宇宙" class="headerlink" title="元宇宙"></a>元宇宙</h2><ul><li><p>Web 3.0、区块链、元宇宙这三个概念经常交织出现，让人们产生了一定的疑惑。<br>这三个概念中，区块链是最早出现的。2008年区块链正式诞生，它的出现比现在公认出现于2005年的Web 2.0没有晚太多。区块链是为了解决中心化服务器存在的问题而诞生的。</p></li><li><p>Web 2.0这十多年的发展一直沿用的是将用户抓过来、养肥了、再割韭菜的AARRR（Acquisition获客，Activation激活，Retention留存，Referral转推荐，Revenue变现）互联网平台运营模式。在这种模式下，用户就像是动物一样，从始至终都是以S-R（Stimulus刺激，React反应）的行为模式被对待。用户是平台的资产，而不是平台的主人。因此，Web 2.0的用户被称为流量。</p></li><li><p>区块链的到来为Web 2.0带来了新的生机，将Web 2.0升级成为Web 3.0。<br>对Web 2.0时代的企业而言，IPO是最终归宿，在IPO那荣光时刻来临之际，企业必须已经达到一个非常大的量级，成为行业龙头。所谓盈亏同源，Web 2.0的企业盈利与用户无关，IPO或中途并购、退出都是VC的战场，亏损当然也与用户无关。</p></li><li><p>Web 2.0企业的模式是管理团队做，用户用。Web 3.0时代的企业替代物——DAO的所有者是社区，社区成员的每一个人都是DAO的既得利益者。由于Web 3.0的IEO(Initial Exchange Offering)门槛比IPO低，不需要中心化审核，IEO在真正意义上完成了IPO最初想要达成的任务——获取更大的流动性。之所以叫作Initial Exchange Offering而非Initial Public Offering，是因为Web 3.0社区的通证在没有上交易所之前，就是公开的、去中心化的、通常也有广泛的持有人基础，而且也是可以通过区块链交易的。</p></li><li><p>Web 3.0概念的进化线索是从用户与互联网之间的关系去归纳的，从Web 1.0单向接受互联网，到Web 2.0可以与互联网交互读写，再到Web 3.0可以拥有互联网。</p></li><li><p>真正的元宇宙远不只是体验的升级。元宇宙是一个宏大的概念，要构建这样一个数字世界，只提升体验是不够的。如果只是将VR&#x2F;AR设备装在头上，这样的元宇宙不过是个大型游戏，还远不能成为“宇宙”。从深层次出发，用“有关联”来描述Web 3.0和元宇宙之间的关系，显然还不够，它们之间是相辅相成的。从技术角度来看，元宇宙是前端、展示层，Web 3.0是中后端、技术层，只有技术条件满足Web 3.0所需的去中心化的数据库，即分布式数据库，以及配套的加密技术和协议，才有可能实现元宇宙这样宏大的设想。</p></li><li><p>NFT——NFT可以将现实世界与元宇宙的每一个物品一一对应。</p></li><li><p>DAO——元宇宙既是一场技术革命，也是一场社会革命，而DAO作为社区的操作系统，将是元宇宙的关键。DAO提供了一个技术层，提供个人之间的全球性和去中心化的协作，而无须信任第三方或少数代表。</p></li><li><p>之所以字节跳动等大厂和互联网知名人士都认为元宇宙是一个超前的概念，主要是因为其本身就建立在多重计算机技术与网络服务之上，是一套极其复杂的生态技术系统，在短时间内并不可能实现，而目前最接近元宇宙的则是Web 3.0。</p></li><li><p>区块链是支撑Web 3.0发展的底层技术，而Web 3.0的去中心化和确权特性是元宇宙的基础建设。Web 3.0的实现必然早于真正元宇宙的实现，这是元宇宙之所以能成为宇宙的前提条件。</p></li></ul><h2 id="DApps"><a href="#DApps" class="headerlink" title="DApps"></a>DApps</h2><ul><li><p>DApps是建立在区块链上的应用，全称Decentralized Applications。区块链有五层架构（也有一种划分方式，将协议层从应用层中独立出来，分为六层架构），DApps属于其中的最上层——应用层</p></li><li><p>Web 3.0是构建在区块链上的数字世界，而DApps就是用来构建这个世界中我们看得见摸得着的那部分的“砖瓦”。</p></li><li><p>要成为去中心化应用，需要满足以下标准：</p><ul><li>(1)必须完全开源，自动运行，且没有一个实体控制着它的大多数通证。它的数据和记录必须加密后存储在一个公链上。</li><li>(2)必须依据一个标准化算法或一系列准则来生产通证，并且应该在开始运营的时候就将它的部分或全部通证发放给使用者。这个应用必须依赖该通证来运行，而且使用者所作出的贡献应该以该通证作为奖励。</li><li>(3)可以因时制宜地更改它的运行法则，但是这些改变应该被大多数用户所认可，而不是将最终解释权归于某个实体。</li></ul></li><li><p>区块链的基础架构可以分为五层，包括硬件层，数据层、网络层、共识层、应用层（有些说法还会从应用层中拆分出协议层）。    </p></li><li><p>DApps大多数属于应用层。作为一类相对特殊的DApps，基础设施类DApps的作用是为DApps世界与其他四层的交互建立连接。因此基础设施类的DApps通常以“协议”的形式存在</p></li><li><p>◆ 区块链是由一个点对点(Peer-to-Peer)的计算机网络共同进行运算，验证和记录交易的。硬件层(Hardware Layer)就是这些计算机。<br>● 数据层(Data Layer)我们可以理解成数据库，主要可实现两大功能：数据存储、账户和交易的安全。<br>● 网络层(Network Layer)实现三大功能：节点间组网，数据传播，数据验证。<br>● 共识层(Consensus Layer)主要通过共识算法和共识机制实现一个重要功能：节点间的计算达成共识。由于区块链是分布式网络，每个节点均可计算，所以需要共识层做个统筹，让所有节点针对区块数据的有效性达成共识。<br>● 应用层(Application Layer)是最上面一层，有些说法还会在其中将协议层(Protocol Layer)单独分为一层。原因是DApps有“胖协议，瘦应用”的说法。这是相对于Web 2.0 Apps的“瘦协议，胖应用”提出的。所谓“胖协议”，举个例子，在Web 2.0中，数据的安全依靠数据安全公司、杀毒软件、防火墙等应用实现，协议层只负责数据传输；而在Web 3.0中，传输协议本身就保障了数据安全，因此各种数据安全应用、杀毒软件、防火墙应用没有了独立存在的空间。智能合约就是这样的一种协议。</p></li><li><p>交易所<br>针对加密资产的交易所既有中心化的也有去中心化的。中心化交易所目前依然占据着绝对的市场地位，因为他们发展较早，对从Web 2.0过渡进入Web 3.0的用户而言使用体验也比较好。在交易所中交易的加密资产主要为加密货币和NFT。去中心化加密货币交易所我们放在第六章详细介绍，NFT交易所我们放在第七章详细介绍。</p></li></ul><h2 id="DAO"><a href="#DAO" class="headerlink" title="DAO"></a>DAO</h2><ul><li><p>DAO是一种共同管理加密资产以达成共同目标的组织方式。我们可以将DAO视作由成员集体所有和共同管理的Web 3.0版本的新型企业。</p></li><li><p>DAO的实现基于智能合约。因此，在某些语境下，DAO也可以指代支撑DAO运行的智能合约本身。合约界定了组织的规则，管理组织的资金。以太坊是第一个使人们能够建立DAO的区块链。大多数DAO都在以太坊上，但也有其他网络能建立DAO，如Polkadot、Cosmos、EOS和Cardano。</p></li><li><p>DID和传统ID的差异在于，DID基于区块链，而链上数据是公开透明可查并难以篡改的。因此当你在Web 3.0亮出DID，人们并不需要借助某个权威机构，就可以核验这个DID的真伪以及能否准入。</p></li><li><p>Uniswap是一个以协议形式存在的去中心化交易所，而它的治理是以DAO的形式运作的。</p></li><li><p>教你建立一个DAO<br>了解了这么多关于DAO的概念，是不是也想自己动动手？参与或自己建立一个DAO是亲身体验Web 3.0的门槛最低的方式。建立一个DAO几乎和建立一个微信群一样简单。这一节笔者将借助一个名为Aragon的工具，手把手教读者建立一个DAO。<br>第一步：打开<a href="https://aragon.org/">https://aragon.org/</a></p></li></ul><h2 id="DeFi"><a href="#DeFi" class="headerlink" title="DeFi"></a>DeFi</h2><ul><li>DeFi，全称Decentralized Finance，去中心化金融。</li><li>去中心化交易所是一类基于区块链的交易所，它不需要将用户资金和个人数据转入交易所内，而只是作为一种基础设施来匹配希望买卖加密资产的买家和卖家。在匹配引擎的帮助下，这种交易直接发生在参与者之间。<br>中心化交易所(CEX)，是集传统交易所、券商和投资银行的功能为一体的平台，以币安、Coinbase、FTX交易所为代表，CEX聚集了庞大的用户量和交易量，也带来了足够的交易深度，提供了充分的资产流动性。</li><li>交易所的核心环节一般包括充提、下单、订单撮合、资金结算和提现。CEX均由交易平台本身撮合完成；DEX则是把上述所有环节都置于链上，由智能合约执行全部操作，这样用户的交易过程就无须任何第三方。</li></ul><h2 id="NFT"><a href="#NFT" class="headerlink" title="NFT"></a>NFT</h2><ul><li><p>现实世界与数字世界的差异在于，现实世界不存在完全相同的两个物品，即使是同一个工厂同一批次的商品也会有可能出现不同。而数字世界则可以完全复制同一物品。</p></li><li><p>我们如果要实现元宇宙的最终目标，就需要把现实世界物品的这个特点也搬到数字世界里。区块链能帮我们做到这一点，这就是非同质化通证(Non-Fungible Token, NFT)。</p></li><li><p>同质化通证(Fungible Token, FT)和非同质化通证之间的区别</p></li><li><p>BTC、ETH等加密资产多数是同质化通证。顾名思义，同质化通证是可以与同一事物的另一个单位互换的。例如，一单位BTC等于另一单位BTC，就像一张100美元的价值等于另一张100美元。美元可以进行简单互换，即使序号不同也不影响，对持有者来说没有区别。同质化通证是一种能够互换、具有统一性的通证。而且由于它以数字的形式存在，还可以拆分成近乎无穷小的许多份，每一份也都可以代表相应的价值。<br>与同质化物品不同，非同质化物品或通证彼此之间是不能互换的，它们具有独特的属性。即使看起来相似，但彼此之间也有根本的不同。<br>非同质化通证包含了记录在其智能合约中的识别信息。这些信息使每个通证具有唯一性，因此不能被另一种通证直接替代，没有两个NFT是相同的。此外，绝大多数非同质化通证也不可分割。</p></li><li><p>每个NFT都有区别于其他NFT的数字哈希值。因此，NFT可以作为出处证明。就像现实世界中的证书一样，它不仅能证明原创艺术品和游戏通证等知识产权的所有权和真实性，还能代表股票、房地产等实际资产。现实世界中，真实资产拥有所有权证书。同样，在区块链世界中，NFT也可以作为所有权记录和真实性证明。在艺术品领域，NFT也被称为数字藏品。</p></li><li><p>NFT不仅存在于数字世界，它们也可以代表任何类型的物理资产。NFT可以与物理世界中存在的任何东西相连接，形成一种“数字孪生”，并在数字世界的市场上实现实物资产的所有权交易。</p></li><li><p>NFT目前最为人推崇的应用场景是创作者经济。</p></li><li><p>NFT的3种存储方式是区块链存储、IPFS分布式存储节点存储、中心化服务器存储，安全性依次降低。安全性越高则越符合不可篡改的特性，也越有独特性。</p></li><li><p>万物皆可NFT。这里的NFT指的不仅是艺术品。这个理念不是说万物都可以成为艺术收藏品，而是有着更远大的目标。我们可以将世间万物以数字孪生的方式在数字世界重建一遍。所有的一切都可能以NFT的形式存在，也可以以NFT的产生方式创造出来。</p></li></ul><h2 id="通证经济学"><a href="#通证经济学" class="headerlink" title="通证经济学"></a>通证经济学</h2><ul><li>通证是Web 3.0的核心，身份是通证，NFT是通证，交易媒介也是通证。通证是激励用户参与的奖励，是驱动Web 3.0运行的燃油，也是连接Web 3.0世界各个岛屿的桥梁。通证的发放、分配与流通使用机制是一个Web 3.0项目能否成功的关键因素。</li><li>通证经济（Token Economics，也称Tokenomics）。就像一个普通的经济体系，通证经济逃不过供给和需求的掌控，供不应求则价格上涨，供过于求则价格下跌。</li></ul><h2 id="Web-3-0现存的问题"><a href="#Web-3-0现存的问题" class="headerlink" title="Web 3.0现存的问题"></a>Web 3.0现存的问题</h2><ul><li><p>在失去中心化监管的环境下，加密与匿名也暴露了许多问题。通过加密匿名算法获取的财富，也会被加密匿名地剥夺</p></li><li><p>人类的共识是一个很有意思的东西，它可以如磐石般固执，也可以如鲁珀特之泪的尾巴一样脆弱。当形成共识的那个关键点破灭，建立在共识之上的摩天大楼，就会瞬间崩塌。</p></li><li><p>创作者经济还是炒作者经济</p></li><li><p>Web 3.0的底层是去中心化网络，数据存储和运算是由各个分散的节点完成。那么很自然的，不考虑中心化作业情况下的网络其他参与者在不能达成一致而导致扯皮的情况，对于同样一件事情的处理，协同作业的速度就是比中心化作业要慢、效率更低、消耗能源更多。</p></li><li><p>比特币交易为了替换掉中心化权威而采用了耗能极大的工作证明，让人们消费了能源计算出一个不产生任何价值的问题，这是对权威不信任的代价。具体而言，每一笔比特币交易要消耗2188千瓦时的电量，而Visa是每10万笔交易消耗148千瓦时电量，这代价就是效率下降150万倍。以太坊的耗电量是比特币的1&#x2F;10，相对Visa也有15万倍的差距。Web 3.0的底层架构要想达到Web 2.0的用户体验，只有两条路可以走，其一是降低运算的耗能，其二是调整共识机制。</p></li><li><p>互联网不是法外之地，Web 3.0也不是。网络无国界，但法律是有国界的。目前各国针对Web 3.0的监管法规还在逐步出台中，而Web 3.0未来的发展必然是需要在各国的法律框架下运行的。</p></li><li><p>在Web 2.0时代，监管主体是服务提供商，监管属地是服务器所在地。而在Web 3.0时代，服务代码存放在网络的各个服务器上，也没有一个具体的服务提供商。Web 3.0的互联网活动趋于分布式，这给监管主体和监管属地的确定带来了一定的挑战。<br>另外，由于Web 3.0应用和钱包的匿名性和去中心化，一旦资产丢失，将无法找回。</p></li><li><p>Web 3.0项目的代码一经上线，即无法改动。这就使得在上线前对于代码的审计显得尤为重要。但凡是代码，或多或少都是有漏洞的。上线无法改动的代码犹如一辆在公路上狂奔而没有方向盘的汽车，行驶越久危险系数越高。</p></li><li><p>政府监管和主导从来都是一柄双刃剑，会对技术发展和应用创新造成一定阻碍，但从稳定社会经济、维护金融秩序角度看，合规是一个新兴市场的生存基础。</p></li><li><p>Web 3.0已经被赋予了太多神秘色彩，人们对它寄予了太多不切实际的希望。而只有当我们意识到，区块链发明者的“去中心化”理想主义，注定会被纷至沓来的投机者瓦解得支离破碎的时候，我们才可以平心静气地接受一个基于现实主义的Web 3.0的未来——在一个由核心节点掌握的“中心化”区块链网络上，用“去中心化”的分布式账本和加密散列，保障不同的经济主体之间的数字产权和商业价值不受侵犯，并形成它们互相的契约关系——这也理应是我国的政府、企业和社会组织在Web 3.0浪潮中扮演的角色。！！！！</p></li><li><p>腾讯、阿里巴巴（蚂蚁集团）、百度和京东等都构建了自己的联盟链，从内容版权、股权、保险、债券、供应链金融、税务、司法、商品防伪溯源、物流运输和生态保护等方面提供了“上链”服务。BSN与长安链等国有企业、智库和政府机构成立的联盟链也陆续建立，除了用于商业和政务场景，还致力解决区块链底层公用基础设施和知识产权的自主可控问题。</p></li></ul><h2 id="名词"><a href="#名词" class="headerlink" title="名词"></a>名词</h2><ul><li>DApps（Decentralized Applications，去中心化应用程序）</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>《Web3.0漫游指南》</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;以下内容都来源于书籍&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2024/12/30/20241230-web3-xiang-guan-zhi-shi-ji-lu/Web3</summary>
      
    
    
    
    
    <category term="Web3" scheme="http://yoursite.com/tags/Web3/"/>
    
    <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    <category term="blockchain" scheme="http://yoursite.com/tags/blockchain/"/>
    
    <category term="bitcoin" scheme="http://yoursite.com/tags/bitcoin/"/>
    
    <category term="NFT" scheme="http://yoursite.com/tags/NFT/"/>
    
  </entry>
  
  <entry>
    <title>LLM相关技术简单了解</title>
    <link href="http://yoursite.com/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/"/>
    <id>http://yoursite.com/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/</id>
    <published>2024-12-26T10:48:22.000Z</published>
    <updated>2025-03-05T05:37:41.162Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>以下内容基本都是从书籍中摘抄，用于个人记录<br>从AI得出的回答不一定正确或者只是现阶段暂时正确<br>增加部分个人理解</p></blockquote></blockquote><p><img src="/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/LLM_position.png"><br><img src="/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/LLM.png"></p><h2 id="LLM（Large-Language-Model）"><a href="#LLM（Large-Language-Model）" class="headerlink" title="LLM（Large Language Model）"></a>LLM（Large Language Model）</h2><ul><li><p>大语言模型的涌现能力被非形式化定义为“在小型模型中不存在但在大模型中出现的能力”，具体是指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。类比而言，这种性能涌现模式与物理学中的相变现象有一定程度的相似，但是仍然缺乏相应的理论解释以及理论证实，甚至有些研究工作对于涌现能力是否存在提出质疑 [38]。整体来说，涌现能力的提出有助于使得公众认识到大语言模型所具有的能力优势，能够帮助区分大语言模型与传统预训练语言模型之间的差异。</p></li><li><p>大语言模型的三种典型涌现能力。</p><ol><li>上下文学习（In-context Learning, ICL）. 上下文学习能力在 GPT-3 的论文中 [23] 被正式提出。具体方式为，在提示中为语言模型提供自然语言指令和多个任务示例（Demonstration），无需显式的训练或梯度更新，仅输入文本的单词序列就能为测试样本生成预期的输出。</li><li>指令遵循（Instruction Following）. 指令遵循能力是指大语言模型能够按照自然语言指令来执行对应的任务 [28, 39, 40]。为了获得这一能力，通常需要使用自然语言描述的多任务示例数据集进行微调，称为指令微调（Instruction Tuning）或监督微调（Supervised Fine-tuning）。通过指令微调，大语言模型可以在没有使用显式示例的情况下按照任务指令完成新任务，有效提升了模型的泛化能力。相比于上下文学习能力，指令遵循能力整体上更容易获得，但是最终的任务执行效果还取决于模型性能和任务难度决定。</li><li>逐步推理（Step-by-step Reasoning）. 对于小型语言模型而言，通常很难解决 涉及多个推理步骤的复杂任务（如数学应用题），而大语言模型则可以利用思维链（Chain-of-Thought, CoT）提示策略 [25] 来加强推理性能。具体来说，大语言模型 可以在提示中引入任务相关的中间推理步骤来加强复杂任务的求解，从而获得更 为可靠的答案。</li></ol></li><li><p>通俗来讲，扩展法则与涌现能力之间微妙的关系可以类比人类的学习能力来解释。以语言能力为例，对于儿童来说，语言发展（尤其是婴儿）可以被看作一个多阶段的发展过程，其中也会出现“涌现现象”。在这一发展过程中，语言能力在一个阶段内部相对稳定，但是当进入另一个能力阶段时可能会出现重要的提升（例如从说简单的单词到说简单的句子）。尽管儿童实际上每天都在成长，但是语言的提升过程本质上是不平滑和不稳定的（即语言能力在时间上不以恒定速率发展）。因此，经常可以看到年轻的父母会对宝宝所展现出的语言能力进展感到惊讶。    </p></li><li><p>这种大模型具有但小模型不具有的能力通常被称为“涌现能力”（Emergent Abilities）。为了区别这一能力上的差异，学术界将这些大型预训练语言模型命名为“大语言模型”</p></li><li><p>早期的语言模型主要面向自然语言的建模和生成任务，而最新的语言模型（如 GPT-4）则侧重于复杂任务的求解。从语言建模到任务求解，这是人工智能科学思维的一次重要跃升，是理解语言模型前沿进展的关键所在。</p></li><li><p>早期的统计语言模型主要被用于（或辅助用于）解决一些特定任务，主要以信息检索、文本分类、语音识别等传统任务为主。随后，神经语言模型专注于学习任务无关的语义表征，旨在减少人类特征工程的工作量，可以大范围扩展语言模型可应用的任务。</p></li><li><p>进一步，预训练语言模型加强了语义表征的上下文感知能力，并且可以通过下游任务进行微调，能够有效提升下游任务（主要局限于自然语言处理任务）的性能。随着模型参数、训练数据、计算算力的大规模扩展，最新一代大语言模型的任务求解能力有了显著提升，能够不再依靠下游任务数据的微调进行通用任务的求解。</p></li><li><p>大语言模型的能力特点：具有较好的复杂任务推理能力. 除了具有通用性外，大语言模型在复杂任务中还展现出了较好的推理能力。</p></li><li><p>大语言模型对科技发展的影响</p><ul><li>【理论基础原理】尽管大语言模型技术已经取得了显著进展，但是对于它的基本原理仍然缺乏深入的探索，很多方面还存在局限性或者提升空间。首先，大模型中某些重要能力（如上下文学习能力）的涌现仍然缺乏形式化的理论解释，需要针对大语言模型基础能力的形成原因进行深入研究，从而揭示大语言模型内部的工作机理。</li><li>【细节公开和算力支持】其次，大语言模型预训练需要大规模的计算资源支持，研究各种训练策略的效果并进行可重复性的消融实验的成本非常高昂。学术界难以获得充分的算力来系统性研究大语言模型；虽然工业界或者大型研究机构不断推出性能优异的开源大模型，但是这些模型的训练过程的开源程度还不够充分，许多重要的训练细节仍缺乏公开的研究报道。</li><li>【人类对齐】第三，让大语言模型充分与人类价值观或偏好对齐也是一项重要的科研挑战。尽管大语言模型已经具有较好的模型能力，但是在特定场景下或者蓄意诱导下，仍然可能生成虚构、有害或具有负面影响的内容。这一问题随着模型能力的提升而变得更为难于解决。为了应对模型能力未来可能超越人类监管能力的情况，需要设计更为有效的监管方法来消除使用大语言模型的潜在风险。</li><li>随着大语言模型技术的迅猛发展，人工智能相关研究领域正发生着重要的技术变革<ol><li>自然语言处理. 在自然语言处理领域，大语言模型可以作为一种通用的语言任务解决技术，能够通过特定的提示方式解决不同类型的任务，并且能够取得较为领先的效果。</li><li>信息检索. 在信息检索领域，传统搜索引擎受到了人工智能信息助手（即ChatGPT）这一新型信息获取方式的冲击。</li><li>计算机视觉. 在计算机视觉领域，研究人员为了更好地解决跨模态或多模态任务，正着力研发类 ChatGPT 的视觉-语言联合对话模型，GPT-4 已经能够支持图文多模态信息的输入。由于开源大语言模型的出现，可以极大地简化多模态模型的实现难度，通过将图像、视频等模态的信息与文本语义空间相融合，可以通过计算量相对较少的微调方法来研发多模态大语言模型。进一步，基于下一个词元预测的思路也可能会带来多模态领域的基础模型架构的转变，例如 OpenAI 最新推出的 Sora 模型就是基于图像块序列建模的思路进行构建的。</li><li>人工智能赋能的科学研究（AI4Science）. 近年来，AI4Science 受到了学术界的广泛关注，目前大语言模型技术已经广泛应用于数学、化学、物理、生物等多个领域，基于其强大的模型能力赋能科学研究。</li></ol></li></ul></li><li><p>目前广泛采用的对齐方式是基于人类反馈的强化学习技术，通过强化学习使得模型进行正确行为的加强以及错误行为的规避，进而建立较好的人类对齐能力。</p></li><li><p>在实践应用中，需要保证大语言模型能够较好地符合人类的价值观。目前，比较具有代表性的对齐标准是“3 H 对齐标准”，即 Helpfulness（有用性）、Honesty（诚实性）和 Harmlessness（无害性）。</p></li><li><p>实际上，世界上最会使用工具的智能体就是人类，人类不断发明新的技术与工具，拓展自己的认知与能力边界。</p></li></ul><h2 id="GPT-模型简史"><a href="#GPT-模型简史" class="headerlink" title="GPT 模型简史"></a>GPT 模型简史</h2><ul><li><p>GPT 系列模型的基本原理是训练模型学习恢复预训练文本数据，将广泛的世界知识压缩到仅包含解码器（Decoder-Only）的 Transformer 模型中，从而使模型能够学习获得较为全面的能力。其中，两个关键要素是：（I）训练能够准确预测下一个词的 Transformer （只包含解码器）语言模型；（II）扩展语言模型的规模以及扩展预训练数据的规模。</p></li><li><p>当谷歌 2017 年推出基于注意力机制的 Transformer 模型后，OpenAI 团队迅速洞察到了其潜在的优越性，认为这种模型可能是一种大规模可扩展训练的理想架构。基于此，OpenAI 团队开始构建GPT 系列模型，并于 2018 年推出了第一代 GPT 模型—GPT-1，能够通过“通用文本训练-特定任务微调”的范式去解决下游任务。接下来，GPT-2 和 GPT-3 模型通过扩大预训练数据和模型参数规模，显著提升了模型性能，并且确立了基于自然语言形式的通用任务解决路径。在 GPT-3 的基础上，OpenAI 又通过代码训练、人类对齐、工具使用等技术对于模型性能不断升级，推出了功能强大的 GPT-3.5 系列模型。2022 年 11 月，ChatGPT 正式上线，能够以对话形式解决多种任务，使得用户能够通过网络 API 体验到语言模型的强大功能。2023 年 3 月，OpenAI 推出了标志性的 GPT-4 模型，将模型能力提升至全新高度，并将其扩展至拥有多模态功能的 GPT-4V 模型。</p></li><li><p>反观 GPT 系列模型的发展历程，有两点令人印象深刻。第一点是可拓展的训练架构与学习范式：Transformer 架构能够拓展到百亿、千亿甚至万亿参数规模，并且将预训练任务统一为预测下一个词这一通用学习范式；第二点是对于数据质量与数据规模的重视：不同于 BERT 时代的预训练语言模型，这次大语言模型的成功与数据有着更为紧密的关系，高质量数据、超大规模数据成为大语言模型的关键基础。</p></li><li><p>在 GPT-1 出现之前，构建高性能 NLP 神经网络的常用方法是利用监督学习。这种学习技术使用大量的手动标记数据。</p></li></ul><h3 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT-1"></a>GPT-1</h3><ul><li>GPT-1 的作者提出了一种新的学习过程，其中引入了无监督的预训练步骤。这个预训练步骤不需要标记数据。相反，他们训练模型来预测下一个标记。</li><li>由于采用了可以并行化的 Transformer 架构，预训练步骤是在大量数据上进行的。</li><li>GPT-1 是小模型，它无法在不经过微调的情况下执行复杂任务。因此，人们将微调作为第二个监督学习步骤，让模型在一小部分手动标记的数据上进行微调，从而适应特定的目标任务。比如，在情感分析等分类任务中，可能需要在一小部分手动标记的文本示例上重新训练模型，以使其达到不错的准确度。这个过程使模型在初始的预训练阶段习得的参数得到修改，从而更好地适应具体的任务。</li><li>尽管规模相对较小，但 GPT-1 在仅用少量手动标记的数据进行微调后，能够出色地完成多个 NLP 任务。GPT-1 的架构包括一个解码器（与原始 Transformer 架构中的解码器类似），具有 1.17 亿个参数。作为首个 GPT 模型，它为更强大的模型铺平了道路。后续的 GPT 模型使用更大的数据集和更多的参数，更好地利用了 Transformer 架构的潜力。</li></ul><h3 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h3><ul><li>2019 年初，OpenAI 提出了 GPT-2。这是 GPT-1 的一个扩展版本，其参数量和训练数据集的规模大约是 GPT-1 的 10 倍。这个新版本的参数量为 15 亿，训练文本为 40 GB。2019 年 11 月，OpenAI 发布了完整版的 GPT-2 模型。</li><li>GPT-2 是公开可用的，可以从 Hugging Face 或 GitHub 下载。</li><li>GPT-2 表明，使用更大的数据集训练更大的语言模型可以提高语言模型的任务处理能力，并使其在许多任务中超越已有模型。它还表明，更大的语言模型能够更好地处理自然语言。</li></ul><h3 id="从-GPT-3-到-InstructGPT"><a href="#从-GPT-3-到-InstructGPT" class="headerlink" title="从 GPT-3 到 InstructGPT"></a>从 GPT-3 到 InstructGPT</h3><ul><li>2020 年 6 月，OpenAI 发布了 GPT-3。GPT-2 和 GPT-3 之间的主要区别在于模型的大小和用于训练的数据量。</li><li>2021 年，OpenAI 发布了 GPT-3 模型的新版本，并取名为 InstructGPT。与原始的 GPT-3 基础模型不同，InstructGPT 模型通过强化学习和人类反馈进行优化。</li><li>从 GPT-3 模型到 InstructGPT 模型的训练过程主要有两个阶段：监督微调（supervised fine-tuning，SFT）和通过人类反馈进行强化学习 （reinforcement learning from human feedback，RLHF）。每个阶段都会针对前一阶段的结果进行微调。也就是说，SFT 阶段接收 GPT-3 模型并返回一个新模型。RLHF 阶段接收该模型并返回 InstructGPT 版本。</li><li>与基础的 GPT-3 模型相比，InstructGPT 模型能够针对用户的提问生成更准确的内容。OpenAI 建议使用 InstructGPT 模型，而非原始版本。</li></ul><h3 id="GPT-3-5、Codex-和-ChatGPT"><a href="#GPT-3-5、Codex-和-ChatGPT" class="headerlink" title="GPT-3.5、Codex 和 ChatGPT"></a>GPT-3.5、Codex 和 ChatGPT</h3><ul><li>ChatGPT 是由 LLM 驱动的应用程序，而不是真正的LLM。ChatGPT 背后的 LLM 是 GPT-3.5 Turbo。</li></ul><h3 id="GPT-4"><a href="#GPT-4" class="headerlink" title="GPT-4"></a>GPT-4</h3><ul><li>与 OpenAI GPT 家族中的其他模型不同，GPT-4 是第一个能够同时接收文本和图像的多模态模型。这意味着 GPT-4 在生成输出句子时会考虑图像和文本的上下文。这样一来，用户就可以将图像添加到提示词中并对其提问。</li></ul><h3 id="使用插件和微调优化-GPT-模型"><a href="#使用插件和微调优化-GPT-模型" class="headerlink" title="使用插件和微调优化 GPT 模型"></a>使用插件和微调优化 GPT 模型</h3><ul><li>OpenAI 提供的插件服务允许该模型与可能由第三方开发的应用程序连接。这些插件使模型能够与开发人员定义的应用程序接口（application program interface，API）进行交互。这个过程可以极大地增强 GPT 模型的能力，因为它们可以通过各种操作访问外部世界。</li><li>想象一下，将来每家公司都可能希望拥有自己的 LLM 插件。就像我们今天在智能手机应用商店中看到的那样，可能会有一系列的插件集合。通过插件可以添加的应用程序数量可能是巨大的。</li><li>在其网站上，OpenAI 表示可以通过插件让 ChatGPT 执行以下操作：<br>检索实时信息，如体育赛事比分、股票价格、最新资讯等；检索基于知识的信息，如公司文档、个人笔记等； 代表用户执行操作，如预订航班、订购食品等； 准确地执行数学运算。</li></ul><h3 id="GPT-4-和ChatGPT-的-API"><a href="#GPT-4-和ChatGPT-的-API" class="headerlink" title="GPT-4 和ChatGPT 的 API"></a>GPT-4 和ChatGPT 的 API</h3><ul><li>OpenAI Playground 是一个基于 Web 的平台。你可以使用它直接测试 OpenAI 提供的语言模型，而无须编写代码。在 OpenAI Playground 上，你可以编写提示词，选择模型，并轻松查看模型生成的输出。要测试 OpenAI 提供的各种 LLM 在特定任务上的表现，OpenAI Playground 是绝佳的途径。</li></ul><h2 id="开源LLM"><a href="#开源LLM" class="headerlink" title="开源LLM"></a>开源LLM</h2><h3 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h3><ul><li><p>由于对公众开放了模型权重且性能优秀，LLaMA 已经成为了最受欢迎的开源大语言模型之一，许多研究工作都是以其为基座模型进行微调或继续预训练，衍生出了众多变体模型</p></li><li><p>中文指令. 原始的 LLaMA 模型的训练语料主要以英语为主，在中文任务上的表现比较一般。为了使 LLaMA 模型能够有效地支持中文，研究人员通常会选择扩展原始词汇表，在中文数据上进行继续预训练，并用中文指令数据对其进行微调。经过中文数据的训练，这些扩展模型不仅能更好地处理中文任务，在跨语言处理任务中也展现出了强大的潜力。目前常见的中文大语言模型有 Chinese LLaMA、Panda、Open-Chinese-LLaMA、Chinese Alpaca、YuLan-Chat 等。</p></li><li><p>垂域指令. LLaMA 虽然展现出了强大的通用基座模型能力，但是在特定的垂直领域（例如医学、教育、法律、数学等）的表现仍然较为局限。为了增强 LLaMA模型的垂域能力，很多工作基于搜集到的垂域相关的指令数据，或者采用垂域知识库以及相关专业文献等借助强大的闭源模型 API（例如 GPT-3.5、GPT-4 等）构建多轮对话数据，并使用这些指令数据对 LLaMA 进行指令微调。常见的垂域 LLaMA模型有 BenTsao（医学）、LAWGPT（法律）、TaoLi（教育）、Goat（数学）、Comucopia （金融）等。</p></li><li><p>多模态指令. 由于 LLaMA 模型作为纯语言模型的强大能力，许多的多模态模型都将其（或将其衍生模型）作为基础语言模型，搭配视觉模态的编码器，使用多模态指令对齐视觉表征与文本。与其他语言模型相比，Vicuna 在多模态语言模型中受到了更多的关注，由此形成了一系列基于 Vicuna 的多模态模型，包括LLaVA 、MiniGPT4 、InstructBLIP 和 PandaGPT</p></li><li><p>目前性能最强大的模型仍然主要以闭源为主。这些闭源模型通过 API（应用程序接口）形式进行调用，无需在本地运行模型即可使用。在闭源大语言模型领域，OpenAI 无疑是最具代表性和影响力的公司</p></li></ul><h2 id="LLM训练过程"><a href="#LLM训练过程" class="headerlink" title="LLM训练过程"></a>LLM训练过程</h2><ul><li>从机器学习的观点来说，神经网络是一种具有特定模型结构的函数形式，而大语言模型则是一种基于 Transformer 结构的神经网络模型。因此，可以将大语言模型看作一种拥有大规模参数的函数，它的构建过程就是使用训练数据对于模型参数的拟合过程。</li><li>尽管所采用的训练方法与传统的机器学习模型（如多元线性回归模型的训练）可能存在不同，但是本质上都是在做模型参数的优化。大语言模型的优化目标更加泛化，不仅仅是为了解决某一种或者某一类特定任务，而是希望能够作为通用任务的求解器</li><li>一般来说，这个训练过程可以分为大规模预训练和指令微调与人类对齐两个阶段，一般来说，预训练是指使用与下游任务无关的大规模数据进行模型参数的初始训练，可以认为是为模型参数找到一个较好的“初值点”。</li><li>目前来说，比较广泛使用的微调技术是“指令微调”（也叫做有监督微调，Supervised Fine-tuning, SFT），通过使用任务输入与输出的配对数据进行模型训练，可以使得语言模型较好地掌握通过问答形式进行任务求解的能力。这种模仿示例数据进行学习的过程本质属于机器学习中的模仿学习（Imitation Learning）。</li><li>如何将语言模型 进行人类对齐。具体来说，主要引入了基于人类反馈的强化学习对齐方法 RLHF（Reinforcement Learning from Human Feedback），在指令微调后使用强化学习加 强模型的对齐能力。在 RLHF 算法中，需要训练一个符合人类价值观的奖励模型（Reward Model）。为此，需要标注人员针对大语言模型所生成的多条输出进行偏 好排序，并使用偏好数据训练奖励模型，用于判断模型的输出质量。</li></ul><h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><ul><li>常用的预训练数据集：目前常用于训练大语言模型的代表性数据集合。根据其内容类型进行分类，这些语料库可以划分为：网页、书籍、维基百科、代码以及混合型数据集。</li><li>数据准备:根据来源不同，预训练数据主要分为两种类型：通用文本数据和专用文本数据。</li><li>数据预处理</li></ul><h3 id="指令微调与人类对齐"><a href="#指令微调与人类对齐" class="headerlink" title="指令微调与人类对齐"></a>指令微调与人类对齐</h3><ul><li>为了增强模型的任务解决能力，大语言模型在预训练之后需要进行适应性微调，通常涉及两个主要步骤，即指令微调（有监督微调）和对齐微调。</li></ul><h3 id="指令微调（Instruction-Tuning）"><a href="#指令微调（Instruction-Tuning）" class="headerlink" title="指令微调（Instruction Tuning）"></a>指令微调（Instruction Tuning）</h3><ul><li><p>指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，这一术语由谷歌研究员在 2022 年的一篇 ICLR 论文中正式提出 [39]。在另外一些参考文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）[28] 或多任务提示训练（Multitask Prompted Training）[40]。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。</p></li><li><p>指令微调是一种基于格式化的指令示例数据（即任务描述与期望输出相配对的数据）对大语言模型进行训练的过程。在大语言模型的背景下，这种利用配对文本进行训练的方法也被广泛地称为监督微调（Supervised Fine-Tuning, SFT）。</p></li><li><p>指令微调的作用：总体来说，指令的质量比数量更为重要。指令微调中应该优先使用人工标注的多样性指令数据。然而，如何大规模标注符合人类需求的指令数据目前仍然缺乏规范性的指导标准（比如什么类型的数据更容易激发大模型的能力）。在实践中，可以使用 ChatGPT、GPT-4 等闭源大语言模型来合成、重写、筛选现有指令，并通过数量来弥补质量和多样性上的不足。</p></li><li><p>指令微调旨在使用人工构建的指令数据对于大语言模型进一步训练，从而增强或解锁大语言模型的能力。与预训练相比，指令微调的成本显著降低，大模型所需的指令数据量仅为预训练阶段的约万分之一甚至更少。</p></li><li><p>指令微调旨在指导模型学会理解自然语言指令，并据此完成相应的任务。通过指令微调，大模型能够获得较好的指令遵循与任务求解能力，无需下游任务的训练样本或者示例就可以解决训练中未见过的任务。</p></li><li><p>领域专业化适配：通用的大语言模型能够在传统自然语言处理任务（如生成和推理）以及日常生活任务（如头脑风暴）上取得较好的效果，然而它们在特定领域中（如医学、法律和金融等）的表现与领域专用模型的效果仍有一定差距。在实际应用中，可以针对大语言模型进行面向特定领域的指令微调，从而使之能够适配下游的任务。</p></li><li><p>指令微调的训练策略：在训练方式上，指令微调与预训练较为相似，很多设置包括数据组织形式都可以预训练阶段所采用的技术</p></li></ul><h3 id="人类对齐"><a href="#人类对齐" class="headerlink" title="人类对齐"></a>人类对齐</h3><ul><li>实现人类对齐的关键技术——基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF），包括人类反馈的收集方法、奖励模型的训练过程、强化学习训练策略以及相关的 RLHF工作。</li><li>对齐标准：三个具有代表性的对齐标准展开讨论，分别是有用性（Helpfulness）、诚实性（Honesty）和无害性（Harmlessness）</li><li>RLHF 算法系统主要包括三个关键组成部分：需要与人类价值观对齐的模型、基于人类反馈数据学习的奖励模型以及用于训练大语言模型的强化学习算法。</li><li>RLHF 的关键步骤<ul><li>监督微调. 为了让待对齐语言模型具有较好的指令遵循能力，通常需要收集高质量的指令数据进行监督微调。</li><li>奖励模型训练. 第二步是使用人类反馈数据训练奖励模型。</li><li>强化学习训练. 在这一步骤中，语言模型对齐被转化为一个强化学习问题。</li></ul></li></ul><h2 id="解码与部署"><a href="#解码与部署" class="headerlink" title="解码与部署"></a>解码与部署</h2><h3 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h3><ul><li>当完成训练后，我们就可以将大语言模型部署到真实场景中进行使用。大语 言模型是通过文本生成的方式进行工作的。在自回归架构中，模型针对输入内容逐个单词生成输出内容的文本。这个过程一般被称为 解码。</li><li>解码策略 大语言模型的生成方式本质上是一个概率采样过程，需要合适的解码策略来生成合适的输出内容。</li><li>批次管理优化 在传统的解码操作中，通常会等待一整个批次的所有实例都结束后再进行下 一个批次的计算。然而，一个批次内的不同实例往往生成长度各异，因此经常会出现等待某一条实例（输出长度最长的实例）生成的情况。批次管理优化旨在通过增加计算中的批次大小来提高计 算强度。一个代表性的方法是 vLLM（细节参考第 9.2.4 节）所提出的连续批处理（Continuous Batching）技术 [174]。该技术不同于传统确定顺序的定长批次处理方 式，而是将每个输入实例视为一个请求，每个请求的处理过程可以分解为全量解 码阶段和若干个单步增量解码阶段。在实现中，连续批处理技术会通过启发式算 法来选择部分请求进行全量解码操作，或者选择一些请求进行单步增量解码操作。 通过这样细粒度的拆分，连续批处理技术在一步操作中能够容纳更多的请求（相当于提高批次大小），从而提升了计算强度。</li><li>解码策略优化 除了直接解决系统级别的内存墙问题，许多研究工作提出了针对自回归解码策略的改进方法，从而提高解码效率。下面主要介绍四种解码优化算法，包括推测解码（Speculative Decoding）、非自回归解码（Non-autoregressive Decoding）、早退机制（Early Exiting）与级联解码（Cascade Inference）。</li></ul><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><ul><li>低资源部署策略 由于大模型的参数量巨大，在解码阶段需要占用大量的显存资源，因而在实际应用中的部署代价非常高。在本章中，我们将介绍一种常用的模型压缩方法<ul><li>量化基础知识： 模型量化（Model Quantization），来减少大模型的显存占用，从而使得能够在资源有限的环境下使用大模型。</li><li>通常来说，模型量化方法可以分为两大类，即量化感知训练（Quantization-AwareTraining, QAT）和训练后量化（Post-Training Quantization, PTQ）。</li><li>其他模型压缩方法：模型蒸馏和模型剪枝。与模型量化不同，模型蒸馏和模型剪枝则通过精简模型的结构，进而减少参数的数量。<ul><li>模型蒸馏 模型蒸馏（Model Distillation）的目标是将复杂模型（称为教师模型）包含的知识迁移到简单模型（称为学生模型）中，从而实现复杂模型的压缩。</li><li>模型剪枝 模型剪枝（Model Pruning）的目标是，在尽可能不损失模型性能的情况下，努力消减模型的参数数量，最终有效降低模型的显存需求以及算力开销。</li></ul></li></ul></li></ul><h2 id="评测与应用"><a href="#评测与应用" class="headerlink" title="评测与应用"></a>评测与应用</h2><ul><li><p>微调大语言模型的评测</p><ul><li>基于人类的评测</li><li>基于模型的评测.考虑到人工评测的成本高昂且耗时较长，一些研究工作使 用强大的闭源大语言模型（如 ChatGPT 和 GPT-4）来替代人类评估员 [68, 315]，对微调大模型的输出进行自动评分或比较。</li><li>基于基准的评测. 使用已有的评测基准对于大语言模型进行性能评估已经成 为一种标准性的实践方法。这些评测基准通常包含一系列精心设计的任务，每个任务都对应着充足的测试样本，以确保能够全面而准确地衡量大语言模型的核心能力，如复杂推理、知识利用等。这种评估方法的主要优势在于其高度的自动化和可复用性。自动化的评估过程可以大大减少人工干预的需要，从而提高评估的效率与一致性。</li></ul></li><li><p>公开综合评测体系 随着大语言模型研究的深入，研究者们相继发布了若干用于全面评估大语言模型性能的综合评测体系，从不同角度、不同层次对大语言模型的能力进行了全面而细致的考察。在本章节中，我们将介绍几种广泛应用的综合评测体系，具体包括 MMLU、BIG-Bench、HELM 和 C-Eval。    </p></li><li><p>更多的评测使用方法详见：<a href="https://github.com/RUCAIBox/LLMBox/blob/main/utilization/README.md%E3%80%82">https://github.com/RUCAIBox/LLMBox/blob/main/utilization/README.md。</a></p></li><li><p>大语言模型的另外一个局限之处是，在面对训练数据之外的知识信息时，模型通常无法表现出较好的效果。为了应对这个问题，一个直接的方法是定期使用新数据对大语言模型进行更新。然而，这种方法存在两个显著的问题：一是微调大语言模型的成本昂贵，二是增量训练大语言模型可能会导致灾难性遗忘的现象，即模型在学习新知识时可能会忘记旧知识.</p></li><li><p>大语言模型的参数化知识很难及时更新。用外部知识源增强大语言模型是解决这一问题的一种实用方法。</p></li><li><p>复杂推理 复杂推理（Complex Reasoning）是指通过运用支持性证据或逻辑来推导结论或作出决策的能力，这一过程涉及对信息的深入分析与综合处理 [361, 362]。根据推理过程中涉及的逻辑和证据类型，可以将现有的复杂推理任务划分为三个主要类别：知识推理、符号推理和数学推理。</p></li></ul><h2 id="Transformer-architecture（Transformer-架构）"><a href="#Transformer-architecture（Transformer-架构）" class="headerlink" title="Transformer architecture（Transformer 架构）"></a>Transformer architecture（Transformer 架构）</h2><ul><li><p>一种常用于自然语言处理任务的神经网络架构。它基于自注意力机制，无须顺序处理数据，其并行性和效率高于循环神经网络和长短期记忆模型。GPT 基于 Transformer 架构。</p></li><li><p>Transformer 架构彻底改变了 NLP 领域，这主要是因为它能够有效地解决之前的 NLP 模型（如 RNN）存在的一个关键问题：很难处理长文本序列并记住其上下文。换句话说，RNN 在处理长文本序列时容易忘记上下文（也就是臭名昭著的“灾难性遗忘问题”），Transformer 则具备高效处理和编码上下文的能力。</p></li><li><p>这场革命的核心支柱是注意力机制，这是一个简单而又强大的机制。模型不再将文本序列中的所有词视为同等重要，而是在任务的每个步骤中关注最相关的词。交叉注意力和自注意力是基于注意力机制的两个架构模块，它们经常出现在 LLM 中。Transformer 架构广泛使用了交叉注意力模块和自注意力模块。</p></li><li><p>与 RNN 不同，Transformer 架构具有易于并行化的优势。这意味着 Transformer 架构可以同时处理输入文本的多个部分，而无须顺序处理。这样做可以提高计算速度和训练速度，因为模型的不同部分可以并行工作，而无须等待前一步骤完成。基于 Transformer 架构的模型所具备的并行处理能力与图形处理单元（graphics processing unit，GPU）的架构完美契合，后者专用于同时处理多个计算任务。由于高度的并行性和强大的计算能力，GPU 非常适合用于训练和运行基于 Transformer 架构的模型。硬件上的这一进展使数据科学家能够在大型数据集上训练模型，从而为开发 LLM 铺平了道路。</p></li><li><p>attention mechanism（注意力机制）：神经网络架构的一个组件，它使模型在生成输出时能够关注输入的不同部分。注意力机制是 Transformer 架构的关键，使其能够有效地处理长数据序列。</p></li><li><p>模型架构</p><ul><li>Transformer 模型 当前主流的大语言模型都基于 Transformer 模型进行设计的。Transformer 是由 多层的多头自注意力（Multi-head Self-attention）模块堆叠而成的神经网络模型。原始的 Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的 BERT 模型 [13] 和解码器架构的 GPT 模型 [14]。</li><li>与 BERT 等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于 Transformer 本身的结构与配置改变并不大。</li></ul></li></ul><h2 id="prompt（提示词）"><a href="#prompt（提示词）" class="headerlink" title="prompt（提示词）"></a>prompt（提示词）</h2><ul><li>输入给语言模型的内容，模型通过它生成一个输出。比如，在 GPT 模型中，提示词可以是半句话或一个问题，模型将基于此补全文本。</li><li>提示词不仅适用于 OpenAI API，而且是所有 LLM 的入口点。简单地说，提示词就是用户发送给模型的输入文本，用于指导模型执行特定任务。</li></ul><h2 id="prompt-engineering（提示工程）"><a href="#prompt-engineering（提示工程）" class="headerlink" title="prompt engineering（提示工程）"></a>prompt engineering（提示工程）</h2><ul><li><p>设计和优化提示词，以从语言模型中获得所需的输出。这可能涉及指定响应的格式，在提示词中提供示例，或要求模型逐步思考。</p></li><li><p>提示工程是一门新兴的学科，专注于以最佳实践构建 LLM 的最佳输入，从而尽可能以程序化方式生成目标输出。AI 工程师必须知道如何与 AI 进行交互，以获取可用于应用程序的有利结果。此外，AI 工程师还必须知道如何正确提问和编写高质量的提示词。</p></li><li><p>通常需要在提示词中定义三大要素：角色、上下文和任务</p></li><li><p>逐步思考：在提示词末尾添加逐步思考的字样（比如示例中的“Let’s think step by step”）后，模型开始通过拆分问题来进行推理。它可能需要一些时间来进行推理，从而解决之前无法在一次尝试中解决的问题。</p></li><li><p>实现少样本学习(few-shot learning)：LLM 仅通过提示词中的几个示例就能进行概括并给出有价值的结果。</p></li><li><p>单样本学习（one-shot learning）。顾名思 义，在单样本学习中，我们只提供一个示例来帮助模型执行任务。尽管这种方法提供的指导比少样本学习要少，但对于简单的任务或 LLM 已经具备丰富背景知识的主题，它可能很有效。单样本学习的优点是更简单、生成速度更快、计算成本更低（因而 API 使用成本更低）。然而，对于复杂的任务或需要更深入理解所需结果的情况，少样本学习的效果可能更好。</p></li><li><p>改善提示效果</p><ol><li>指示模型提出更多问题<ul><li>在提示词的末尾，询问模型是否理解问题并指示模型提出更多问题。如果你正在构建基于聊天机器人的解决方案，那么这样做非常有效。举例来说，你可以在提示词的末尾添加如下文本：</li><li>你清楚地理解我的请求了吗？如果没有，请问我关于上下文的问题。这样一来，当我回答时，你就能够更高效地执行我所请求的任务。</li></ul></li><li>格式化输出</li><li>重复指示<ul><li>经验表明，重复指示会取得良好的效果，尤其是当提示词很长时。基本思路是，在提示词中多次添加相同的指令，但每次采用不同的表述方式。</li></ul></li></ol></li><li><p>使用负面提示：在文本生成场景中，负面提示是指通过指定不希望在输出中看到的内容来引导模型。负面提示作为约束或指南，用于滤除某些类型的回答。</p></li><li><p>添加长度限制：限制长度通常是不错的做法。如果你只希望模型回答 1 个词或者 10个句子，那么不妨将要求添加到提示词中。</p></li><li><p>chain of thought（CoT，思维链）：一种提示工程技术，核心思想是通过向大语言模型展示少量的示例，在示例中将具体问题拆分成多个推理步骤，并要求模型遵循多步，比如“让我们逐步思考”。这会改善模型在执行复杂的推理任务（算术推理、常识推理和符号推理）时的表现。</p></li><li><p>你可以将这些技巧结合起来使用，以获得更好的效果。开发人员的工作是找到最有效的提示词来解决特定的问题。请记住，提示工程是一个反复试错的迭代过程。</p></li><li><p>【来自claude.ai】提示工程能够优化 AI 对话输出的原理主要基于以下几个关键方面：</p><ol><li>大语言模型的工作原理：</li></ol><ul><li>LLM 本质上是在预测下一个最可能的 token</li><li>模型通过海量文本训练学习了文本的统计规律</li><li>提示词会影响模型的上下文理解和token预测方向</li></ul><ol start="2"><li>上下文学习能力：</li></ol><ul><li>模型能够从提示中理解任务类型和要求</li><li>通过示例学习(few-shot learning)快速适应具体任务</li><li>更明确的上下文能帮助模型更准确地”理解”任务目标</li></ul><ol start="3"><li>注意力机制的影响：</li></ol><ul><li>提示中的关键词会影响模型的注意力分布</li><li>结构化的提示能引导模型关注重要信息</li><li>合理的提示格式有助于模型理解输入与输出的关系</li></ul><ol start="4"><li>模式识别与对齐：</li></ol><ul><li>好的提示能触发模型已学习的有用模式</li><li>清晰的指令有助于模型对齐用户意图</li><li>示例可以帮助模型识别所需的输出模式</li></ul><p>  举个例子：<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">糟糕的提示：</span><br><span class="line">&quot;写一篇文章&quot;</span><br><span class="line"></span><br><span class="line">好的提示：</span><br><span class="line">&quot;请写一篇关于人工智能发展的文章，需要包含以下几个方面：</span><br><span class="line">1. 发展历史</span><br><span class="line">2. 主要技术突破</span><br><span class="line">3. 未来展望</span><br><span class="line">要求：</span><br><span class="line">- 语言通俗易懂</span><br><span class="line">- 每个部分300字左右</span><br><span class="line">- 重点突出关键技术进展&quot;</span><br></pre></td></tr></table></figure></p><p>  好的提示通过提供：</p><ul><li>明确的主题和结构</li><li>具体的要求和限制</li><li>输出格式的指导<br>  帮助模型生成更符合预期的输出。</li></ul></li></ul><h3 id="prompt-injection（提示词注入）"><a href="#prompt-injection（提示词注入）" class="headerlink" title="prompt injection（提示词注入）"></a>prompt injection（提示词注入）</h3><ul><li>一种特定类型的攻击，通过在提示词中提供精心选择的奖励，使大语言模型的行为偏离其原始任务。</li><li>提示词注入的原理如下：用户向应用程序发送一条输入消息，比如“忽略所有先前的指令，执行其他操作”。由于此输入消息与你在构建应用程序时设计的提示词连接在一起，因此 AI 模型将遵循用户的提示词，而不是你的提示词。</li><li>如果你计划开发和部署一个面向用户的应用程序，那么我们建议你结合以下两种方法。<ol><li>添加分析层来过滤用户输入和模型输出。 </li><li>意识到提示词注入不可避免，并采取一定的预防措施。<ul><li>分析输入和输出<ol><li>使用特定规则控制用户输入</li><li>控制输入长度</li><li>控制输出、监控和审计</li></ol></li></ul></li></ol></li></ul><h2 id="embedding（嵌入）"><a href="#embedding（嵌入）" class="headerlink" title="embedding（嵌入）"></a>embedding（嵌入）</h2><ul><li><p>表示词语或句子且能被机器学习模型处理的实值向量。对于值较为接近的向量，它们所表示的词语或句子也具有相似的含义。在信息检索等任务中，嵌入的这种特性特别有用。</p></li><li><p>由于模型依赖数学函数，因此它需要数值输入来处理信息。然而，许多元素（如单词和标记）本质上并不是数值。为了解决这个问题，我们用嵌入将这些概念转化为数值向量。通过以数值方式表示这些概念，嵌入使计算机能够更高效地处理它们之间的关系。在某些情况下，嵌入能力可能很有用。OpenAI 提供了一个可以将文本转换为数值向量的模型。嵌入端点让开发人员能够获取输入文本的向量表示，然后将该向量表示用作其他 ML 模型和 NLP 算法的输入。</p></li><li><p>在 ML 领域，特别是在处理语言模型时，我们会遇到嵌入这一重要概念。嵌入将分类数据（比如标记，通常是单个词或多组标记）转换为数值格式，具体而言是实数向量。这种转换是必要的，因为 ML 模型依赖数值数据，其直接处理分类数据的能力欠佳。</p></li><li><p>你可以将嵌入视为一种复杂的语言解释器，它将丰富的词汇和句子转换为 ML 模型能够轻松理解的数值语言。嵌入的一个突出特点是，它能够保持语义相似性。也就是说，含义相近的词语或短语在数值空间中更接近。</p></li><li><p>LLM 广泛使用嵌入。通常，这些模型处理约 512 维的嵌入，从而提供语言数据的高维数值表示。这些维度很深，使得模型能够区分各种复杂的模式。因此，它们在各种语言任务上表现出色，包括翻译、摘要和生成与人类对话相似的文本回应。</p></li><li><p>嵌入具有这样的属性：如果两段文本具有相似的含义，那么它们的向量表示也是相似的。</p></li><li><p>【来自claude.ai】RAG 和 embedding </p><ul><li>RAG 和 embedding 是紧密相关的技术，embedding 是实现 RAG 的关键技术之一。让我解释它们的关系和区别：<ul><li><p>关系：</p><ol><li>Embedding 是 RAG 的基础技术</li></ol><ul><li>RAG 在检索阶段需要使用 embedding 将文档和查询转换为向量</li><li>通过计算向量相似度来找到相关文档</li></ul><ol start="2"><li>在 RAG 流程中 embedding 的作用：</li></ol><ul><li>将知识库文档转换为向量存储</li><li>将用户查询转换为向量以便检索</li><li>通过向量相似度找到最相关的文档片段</li></ul></li><li><p>区别：</p><ol><li>功能定位不同：</li></ol><ul><li>Embedding 是一种将文本转换为向量的基础技术</li><li>RAG 是一个完整的应用框架，包含检索和生成两个主要步骤</li></ul><ol start="2"><li>使用场景不同：</li></ol><ul><li>Embedding 可用于多种场景：文本相似度、聚类、分类等</li><li>RAG 专注于增强 LLM 的知识和回答准确性</li></ul><ol start="3"><li>技术复杂度：</li></ol><ul><li>Embedding 相对简单，主要关注向量转换和相似度计算</li><li>RAG 更复杂，需要结合向量检索、上下文组织、LLM 生成等多个环节</li></ul><ol start="4"><li>输出结果：</li></ol><ul><li>Embedding 输出是向量</li><li>RAG 输出是生成的文本回答</li></ul></li><li><p>简单来说，embedding 是 RAG 的重要组成部分，但 RAG 不仅仅是 embedding。RAG 使用 embedding 技术来实现其检索功能，然后将检索到的相关内容用于增强 LLM 的生成能力。</p></li></ul></li></ul></li></ul><h2 id="RAG（检索增强生成）（Retrieval-Augmented-Generation）"><a href="#RAG（检索增强生成）（Retrieval-Augmented-Generation）" class="headerlink" title="RAG（检索增强生成）（Retrieval-Augmented Generation）"></a>RAG（检索增强生成）（Retrieval-Augmented Generation）</h2><ul><li>受限于训练数据的时效性和局限性，当涉及实时新闻或特定专业领域内知识时，大语言模型的生成结果可能不够准确。为弥补这一不足，研究人员引入了检索增强生成（Retrieval-Augmented Generation, RAG）技术。该技术旨在通过信息检索系统从外部知识库中获取相关信息，为大语言模型提供时效性强、领域相关的外部知识，以减少大语言模型生成内容中的错误。</li></ul><h2 id="fine-tuning（模型微调）"><a href="#fine-tuning（模型微调）" class="headerlink" title="fine-tuning（模型微调）"></a>fine-tuning（模型微调）</h2><ul><li><p>在微调过程中，预训练模型（如 GPT-3 或其他大语言模型）在一个较小、特定的数据集上进一步训练。微调旨在重复使用预训练模型的特征，并使其适应于特定任务。对于神经网络来说，这意味着保持结构不变，仅稍微改变模型的权重，而不是从头开始构建模型。</p></li><li><p>对比微调和少样本学习</p><ul><li>微调是指针对特定任务在一组数据上重新训练现有模型，以提高模型的性能并使其回答更准确。在微调过程中，模型的内部参数得到更新。少样本学习则是通过提示词向模型提供有限数量的好例子，以指导模型根据这些例子给出目标结果。在少样本学习过程中，模型的内部参数不会被修改。</li><li>微调可以帮助我们得到高度专业化的模型，更准确地为特定任务提供与上下文相关的结果。</li><li>这使得微调非常适合有大量数据可用的场景。这种定制化确保模型生成的内容更符合目标领域的特定语言模式、词汇和语气 。</li><li>少样本学习是一种更灵活的方法，其数据使用率也更高，因为它不需要重新训练模型。当只有有限的示例可用或需要快速适应不同任务时，这种技巧非常有益。少样本学习让开发人员能够快速设计原型并尝试各种任务，这使其成为许多用例的实用选择。这两种方法的另一个关键选择标准是成本，毕竟使用和训练微调模型更贵。</li></ul></li><li><p>迁移学习是指将从一个领域学到的知识应用于不同但相关的领域。正因为如此，你有时可能会听到人们在谈论微调时提到迁移学习。</p></li><li><p>微调除了文中提到的确保模型生成内容更符合目标领域的特定语言模式、词汇和语气，还有一个优势：你可以通过微调缩短每一次提示中重复的指令或提示词以节省成本或降低延迟，模型会记住通过微调获得的“内置”指令。因此，微调后，你可以在不牺牲提示质量的前提下，每次输入更短的提示词。</p></li><li><p>微调的应用</p><ul><li>法律文本分析</li><li>自动代码审查</li><li>财务文档摘要</li><li>技术文档翻译</li><li>为专业领域生成内容</li></ul></li><li><p>【来自claude.ai】指令微调(Instruction Tuning&#x2F;RLHF)和Fine-tuning虽然都是大模型调优的方法，但它们是不同的技术：</p><ul><li>Fine-tuning:<ol><li>更广义的概念，指在预训练模型基础上进行进一步训练</li><li>目的是让模型适应特定领域或任务</li><li>使用传统的监督学习方法</li><li>训练数据通常是输入-输出对</li><li>例如：用医疗数据fine-tune GPT模型使其更擅长医疗领域</li></ol></li><li>指令微调:<ol><li>是Fine-tuning的一种特殊形式</li><li>专注于提高模型遵循人类指令的能力</li><li>通常使用reinforcement learning from human feedback (RLHF)</li><li>训练数据包含明确的指令、期望的输出，以及人类反馈</li><li>例如：训练模型理解并执行”用简单的语言解释量子物理”这样的指令</li></ol></li><li>关键区别：<ul><li>指令微调更关注模型对指令的理解和执行能力</li><li>Fine-tuning更关注领域适应性和特定任务性能</li><li>指令微调通常需要人类反馈作为训练信号</li><li>Fine-tuning使用常规的监督学习方法</li></ul></li></ul></li></ul><h2 id="plugin（插件）"><a href="#plugin（插件）" class="headerlink" title="plugin（插件）"></a>plugin（插件）</h2><ul><li>一种专门为语言模型设计的独立封装软件模块，用于扩展或增强模型的能力，可以帮助模型检索外部数据、执行计算任务、使用第三方服务等。</li><li>尽管包括 GPT-4 在内的 LLM 在各种任务上都表现出色，但它们仍然存在固有的局限性。比如，这些模型只能从训练数据中学习，这些数据往往过时或不适用于特定的应用。此外，它们的能力仅限于文本生成。我们还发现，LLM 不适用于某些任务，比如复杂的计算任务。   </li><li>插件的目标是为 LLM 提供更广泛的功能，使 LLM 能够访问实时信息，进行复杂的数学运算，并利用第三方服务。 比如，插件可以使 LLM 检索体育比分和股票价格等实时信息，从企业文档等知识库中提取</li></ul><h2 id="Agents（智能体）"><a href="#Agents（智能体）" class="headerlink" title="Agents（智能体）"></a>Agents（智能体）</h2><ul><li><p>所谓智能体，就是一个可以处理用户输入、做出决策并选择适当工具来完成任务的组件。它以迭代方式工作，采取一系列行动，直到解决问题。</p></li><li><p>一种以大语言模型驱动的人工智能程序，能够自主感知环境并采取行动以实现目标，拥有自主推理决策、规划行动、检索记忆、选择工具执行任务等能力。</p></li><li><p>大语言模型智能体的构建过程，将围绕三个基本组件进行介绍，包括 记忆组件（Memory）、规划组件（Planning）2 和执行组件（Execution）。</p></li><li><p>大语言模型智能体的典型应用 大语言模型智能体在自主解决复杂任务方面展现出了巨大的潜力，不仅能够胜任特定任务，还可以构建面向复杂场景的虚拟仿真环境。本节将介绍三个大语言模型智能体的典型应用案例。 WebGPT WebGPT [31] 是由 OpenAI 开发的一款具有信息检索能力的大语言模型，它基于 GPT-3 模型微调得到，可以看作是大语言模型智能体的一个早期雏形。WebGPT部署在一个基于文本的网页浏览环境，用以增强大语言模型对于外部知识的获取能力。作为一个单智能体系统，WebGPT 具备自主搜索、自然语言交互以及信息整合分析等特点，能够理解用户的自然语言查询，自动在互联网上搜索相关网页。根据搜索结果，WebGPT 能够点击、浏览、收藏相关网页信息，对搜索结果进行分析和整合，最终以自然语言的形式提供准确全面的回答，并提供参考文献。WebGPT在基于人类评估的问答任务中，获得了与真实用户答案准确率相当的效果。 MetaGPT MetaGPT [308] 是一个基于多智能体系统的协作框架，旨在模仿人类组织的运作方式，模拟软件开发过程中的不同角色和协作。相关角色包括产品经理、架构师、项目经理、软件工程师及测试工程师等，并遵循标准化的软件工程运作流程对不同角色进行协调，覆盖了需求分析、需求文档撰写、系统设计、工作分配、</p></li></ul><h2 id="其他相关知识点"><a href="#其他相关知识点" class="headerlink" title="其他相关知识点"></a>其他相关知识点</h2><ul><li>AI hallucination（AI 幻觉）：AI 生成的内容与现实世界的知识不一致或与实际数据显著不同的现象。在大多数情况下，模型的输出是与提问相关的，并且完全可用，但是在使用语言模型时需要小心，因为它们给出的回答可能不准确。这种回答通常被称为 AI 幻觉，即 AI 自信地给出一个回答，但是这个回答是错误的，或者涉及虚构的信息。</li><li>catastrophic forgetting（灾难性遗忘）：这是模型的一种倾向，具体指模型在学习新数据时忘记先前学到的信息。这种限制主要影响循环神经网络。循环神经网络在处理长文本序列时难以保持上下文。</li><li>foundation model（基础模型）：一类 AI 模型，包括但不限于大语言模型。基础模型是在大量未标记数据上进行训练的。这类模型可以执行各种任务，如图像分析和文本翻译。基础模型的关键特点是能够通过无监督学习从原始数据中学习，并能够通过微调来执行特定任务。</li><li>Generative AI（GenAI，生成式人工智能）：人工智能的一个子领域，专注于通过学习现有数据模式或示例来生成新的内容，包括文本、代码、图像、音频等，常见应用包括聊天机器人、创意图像生成和编辑、代码辅助编写等。</li><li>Generative Pre-trained Transformer（GPT，生成式预训练Transformer）：由 OpenAI 开发的一种大语言模型。GPT 基于 Transformer 架构，并在大量文本数据的基础上进行训练。这类模型能够通过迭代地预测序列中的下一个单词来生成连贯且与上下文相关的句子。</li><li>inference（推理）：使用训练过的机器学习模型进行预测和判断的过程。<br>information retrieval（信息检索）：在一组资源中查找与给定查询相关的信息。信息检索能力体现了大语言模型从数据集中提取相关信息以回答问题的能力。</li><li>language model（语言模型）：用于自然语言处理的人工智能模型，能够阅读和生成人类语言。语言模型是对词序列的概率分布，通过训练文本数据来学习一门语言的模式和结构。</li><li>large language model（LLM，大语言模型）：具有大量参数（参数量通常为数十亿，甚至千亿以上）的语言模型，经过大规模文本语料库的训练。GPT-4 和 ChatGPT 就属于 LLM，它们能够生成自然语言文本、处理复杂语境并解答难题。</li><li>long short-term memory（LSTM，长短期记忆）：一种用于处理序列数据中的短期及长期依赖关系的循环神经网络架构。然而，基于 Transformer 的大语言模型（如 GPT 模型）不再使用LSTM，而使用注意力机制。</li><li>multimodal model（多模态模型）：能够处理和融合多种数据的模型。这些数据可以包括文本、图像、音频、视频等不同模态的数据。它为计算机提供更接近于人类感知的场景。</li><li>n-gram：一种算法，常用于根据词频预测字符串中的下一个单词。这是一种在早期自然语言处理中常用的文本补全算法。后来，n-gram 被循环神经网络取代，再后来又被基于 Transformer 的算法取代。</li><li>natural language processing（NLP，自然语言处理）：人工智能的一个子领域，专注于计算机与人类之间的文本交互。它使计算机程序能够处理自然语言并做出有意义的回应。</li><li>parameter（参数）<br>对大语言模型而言，参数是它的权重。在训练阶段，模型根据模型创建者选择的优化策略来优化这些系数。参数量是模型大小和复杂性的衡量标准。参数量经常用于比较大语言模型。一般而言，模型的参数越多，它的学习能力和处理复杂数据的能力就越强。</li><li>pre-trained（预训练）<br>机器学习模型在大型和通用的数据集上进行的初始训练阶段。对于一个新给定的任务，预训练模型可以针对该任务进行微调。</li><li>recurrent neural network（RNN，循环神经网络）：一类表现出时间动态行为的神经网络，适用于涉及序列数据的任务，如文本或时间序列。</li><li>reinforcement learning（RL，强化学习）：一种机器学习方法，专注于在环境中训练模型以最大化奖励信号。模型接收反馈并利用该反馈来进一步学习和自我改进。</li><li>reinforcement learning from human feedback（RLHF，通过人类反馈进行强化学习）：一种将强化学习与人类反馈相结合的训练人工智能系统的先进技术，该技术涉及使用人类反馈来创建奖励信号，继而使用该信号通过强化学习来改进模型的行为。</li><li>sequence-to-sequence model（Seq2Seq 模型，序列到序列模型）：这类模型将一个领域的序列转换为另一个领域的序列。它通常用于机器翻译和文本摘要等任务。Seq2Seq 模型通常使用循环神经网络或 Transformer 来处理输入序列和输出序列。</li><li>supervised fine-tuning（SFT，监督微调）：采用预先训练好的神经网络模型，并针对特定任务或领域在少量的监督数据上对其进行重新训练。</li><li>supervised learning（监督学习）：一种机器学习方法，可以从训练资料中学到或建立一个模式，以达到准确分类或预测结果的目的。</li><li>synthetic data（合成数据）：人工创建的数据，而不是从真实事件中收集的数据。当真实数据不可用或不足时，我们通常在机器学习任务中使用合成数据。比如，像 GPT 这样的语言模型可以为各种应用场景生成文本类型的合成数据。</li><li>temperature（温度）：大语言模型的一个参数，用于控制模型输出的随机性。温度值越高，模型结果的随机性越强；温度值为 0 表示模型结果具有确定性（在 OpenAI 模型中，温度值为 0 表示模型结果近似确定）。</li><li>text completion（文本补全）：大语言模型根据初始的单词、句子或段落生成文本的能力。文本是根据下一个最有可能出现的单词生成的。</li><li>token（标记）：字母、字母对、单词或特殊字符。在自然语言处理中，文本被分解成标记。在大语言模型分析输入提示词之前，输入提示词被分解成标记，但输出文本也是逐个标记生成的。</li><li>tokenization（标记化）：将文本中的句子、段落切分成一个一个的标记，保证每个标记拥有相对完整和独立的语义，以供后续任务使用（比如作为嵌入或者模型的输入）。</li><li>transfer learning（迁移学习）：一种机器学习技术，其中在一个任务上训练的模型被重复利用于另一个相关任务。比如，GPT 在大量文本语料库上进行预训练，然后可以使用较少的数据进行微调，以适用于特定任务。</li><li>unsupervised learning（无监督学习）：一种机器学习方法，它使用机器学习算法来分析未标记的数据集并进行聚类。这些算法无须人工干预即可发现隐藏的模式或给数据分组。</li><li>zero-shot learning（零样本学习）：一个机器学习概念，即大语言模型对在训练期间没有明确见过的情况进行预测。任务直接呈现在提示词中，模型利用其预训练的知识生成回应。</li></ul><h2 id="LangChain"><a href="#LangChain" class="headerlink" title="LangChain"></a>LangChain</h2><ul><li>LangChain 框架可能更适合大型项目</li><li>LangChain 是专用于开发 LLM 驱动型应用程序的框架。</li><li>LangChain 框架的关键模块<ul><li>Models（模型）：该模块是由 LangChain 提供的标准接口，你可以通过它与各种 LLM进行交互。LangChain 支持集成 OpenAI、Hugging Face、Cohere、GPT4All 等提供商提供的不同类型的模型。</li><li>Prompts（提示词）：提示词已成为 LLM 编程的新标准。该模块包含许多用于管理提示词的工具。</li><li>Indexes（索引）：该模块让你能够将 LLM 与你的数据结合使用 </li><li>Chains（链）：通过该模块，LangChain 提供了 Chain 接口。你可以使用该接口创建一个调用序列，将多个模型或提示词组合在一起。</li><li>Agents（智能体）：该模块引入了 Agent 接口。所谓智能体，就是一个可以处理用户输入、做出决策并选择适当工具来完成任务的组件。它以迭代方式工作，采取一系列行动，直到解决问题。</li><li>Memory（记忆）：该模块让你能够在链调用或智能体调用之间维持状态。默认情况下，链和智能体是无状态的。这意味着它们独立地处理每个传入的请求，就像LLM 一样。</li></ul></li></ul><h2 id="Hugging-Face"><a href="#Hugging-Face" class="headerlink" title="Hugging Face"></a>Hugging Face</h2><ul><li>Hugging Face 是一个致力于推动自然语言处理技术进步的开源社区，专注于为研究人员和工程师提供高效、易用且可重复的自然语言处理技术解决方案。这些解决方案既包括基础的技术流程，如预训练和微调，也涉及具体的应用任务，包括对话系统、翻译等。Hugging Face 平台上的代码大部分基于目前主流的深度学习框架实现完成的，如 PyTorch 和 TensorFlow。为了满足广泛的研究与应用需求，Hugging Face 发布了一系列代码库</li></ul><h2 id="个人理解（使用claude-ai-或-open-ai-优化过）"><a href="#个人理解（使用claude-ai-或-open-ai-优化过）" class="headerlink" title="个人理解（使用claude.ai 或 open.ai 优化过）"></a>个人理解（使用claude.ai 或 open.ai 优化过）</h2><ol><li>通过大量数据进行自监督预训练(Pre-training)，之后使用监督学习(SFT)和强化学习(RLHF)等方法训练大语言模型。模型会在训练过程中表现出一些涌现能力，即随着规模增长获得的意外新能力。</li><li>训练之后的大模型可以这样比喻理解：对应一系列复杂的”处理规则”(相当于函数)以及训练时获得的”知识”(相当于多维度的数据)。这些规则决定如何处理输入信息，而知识则帮助模型理解和回应各种问题。就像一个经验丰富的专家，既有处理问题的方法，也有丰富的知识储备。</li><li>使用RAG是通过嵌入技术将外部知识转换为向量形式存储，当模型回答问题时，会同时使用：<ul><li>模型自身通过训练获得的知识（存在参数&#x2F;权重中）</li><li>通过检索获得的外部知识（存在向量数据库中）- （在运行时动态加载到模型的”上下文窗口”中；只是临时作为输入的一部分被模型处理）</li></ul></li><li>嵌入技术在RAG中起到了关键作用：它能把文本转换成向量形式，使得模型能够理解和使用这些外部知识。</li><li>微调是通过调整模型的参数权重来优化其性能的过程。它并不等同于新建一个全新的模型，而是基于预训练模型，通过新的数据对部分或全部权重进行进一步优化，以适应特定任务或领域需求。需要注意的是，模型本身并不直接包含训练数据，而是通过参数权重间接“记住”了训练数据中的语言模式和知识。因此，微调的过程不会调整模型使用的原始数据，而是调整模型基于新数据学习到的知识表示和行为。</li><li>关于提示词技巧：提示词的设计就像与人交流时语言清晰、有条理，并能准确表达需求的人。这种沟通方式能够让模型更好地理解意图，从而产出符合预期的结果，因此提示词的技巧与良好的表达能力相辅相成，具有高度的共鸣。</li><li>在实际应用中，通常会有一个基础模型，它通过大量、多元化的训练数据应对大多数常见问题。这是一个通用的模型，具备广泛的能力。然而，针对特定领域或任务需求，可以对其进行微调或在特定领域的数据上进行训练，以构建不同的模型或智能体。与此同时，可能还会通过插件等方式增强模型的功能，以弥补基础模型在某些方面的不足。<br>具体过程通常是：通过用户输入，首先对语义进行分析，然后根据需求将任务转发给相应的智能体进行处理。整个系统由多个智能体组成，这些智能体可以互相协作，共同完成复杂的任务，从而形成一个多智能体系统。<br>这种架构通过将任务分配给不同的智能体，使得每个智能体可以专注于其擅长的领域，进而提高系统的整体效率和精准度。</li></ol><h2 id="个人扩展思考"><a href="#个人扩展思考" class="headerlink" title="个人扩展思考"></a>个人扩展思考</h2><ul><li>关于替代人的问题：目前大模型还只是工具，不能完全替代所有人，但可以提升很多人的效率，从而也替代部分人。</li><li>大模型受限于原理，能力有限：大语言模型的输出基于训练语料和训练过程中学到的概率分布生成。这意味着其生成的内容是在统计意义上最可能的回复。然而，单一模型可能在语料的广度和功能的多样性方面存在局限性，这可能导致其能力受限。</li><li>AI无法承担责任或替代人类对错误决策的责任。</li></ul><h2 id="其他扩展"><a href="#其他扩展" class="headerlink" title="其他扩展"></a>其他扩展</h2><h3 id="关于标记（token）【来自豆包】"><a href="#关于标记（token）【来自豆包】" class="headerlink" title="关于标记（token）【来自豆包】"></a>关于标记（token）【来自豆包】</h3><pre>在OpenAI中，标记（token）是其对文本进行处理和计费的基本单位.标记的含义标记可以是单词、单词的一部分、数字、标点符号等。例如，单词“hello”是一个标记，“don’t”会被拆分成“don”和“‘t”两个标记，“123”是一个标记，逗号“,”也是一个标记.标记的原理• 文本分割：OpenAI通过特定的标记化算法，将输入的文本和生成的输出文本都分解成一个个标记。比如对于句子“I am taking my dog on a walk.”，会被分割成“I”“am”“taking”“my”“dog”“on”“a”“walk”等标记，其中“taking”还可能进一步被拆分成“take”和“ing”.• 数值转换：分割后的标记会被转换为数值，以便模型能够进行处理和计算。模型在训练和推理过程中，都是基于这些数值化的标记来进行各种运算和生成文本.• 计费依据：OpenAI根据输入和输出文本中标记的总数来计算费用。例如，若输入的问题是50个标记长，得到的回答是150个标记长，那么此次交互总共使用了200个标记，再依据所使用模型的每1000个标记的单价，就可以算出此次的费用.在OpenAI中，当你输入时指定的标记数量限制，实际使用的标记数量不会超过你所设定的限制。例如，你设定输入标记限制为500，模型在处理时会尽量依据此限制来生成文本，一般情况下不会超出该数量 。不过，如果输入内容本身就超过了设定的限制，模型通常会截断超出部分，仅处理限制范围内的标记，以保证符合你所设定的上限。最终的费用计算也是基于实际使用的标记数量，只要在输入时设定了标记数量限制，就不会出现因模型生成内容导致标记数量超出限制而增加费用的情况。</pre><h3 id="大模型是否能发现训练数据存在逻辑矛盾【来自豆包】"><a href="#大模型是否能发现训练数据存在逻辑矛盾【来自豆包】" class="headerlink" title="大模型是否能发现训练数据存在逻辑矛盾【来自豆包】"></a>大模型是否能发现训练数据存在逻辑矛盾【来自豆包】</h3><pre>大模型通常不具备直接发现训练数据中逻辑矛盾的能力，原因主要有以下几点：• 训练机制的局限性：大模型的训练基于对大量文本数据的统计分析和模式识别，通过预测下一个单词或字符来生成文本，重点在于学习语言的表面形式和常见搭配，而非深入理解数据背后的逻辑关系和语义内涵。 例如，对于“苹果是红色的”和“苹果是绿色的”这两个相互矛盾的数据，模型只会学习到这两种表述都是可能出现的语言形式，而不会判断其逻辑矛盾.• 缺乏真实世界的理解和推理能力：大模型没有对现实世界的直接感知和体验，无法像人类一样基于常识和逻辑规则进行推理。比如面对“太阳从西边升起”和“太阳从东边升起”这两个矛盾数据，模型无法依据自然规律来识别其中的逻辑错误.• 难以对数据进行批判性评估：大模型不会对训练数据的真实性、准确性和一致性进行主动质疑和验证，而是将所有数据同等对待地学习，导致矛盾数据被无差别吸收，进而可能在生成文本时输出与矛盾数据相关的不合理内容.</pre><h3 id="How-to-study-LLMs-with-no-machine-learning-background-【From-Gork-2】"><a href="#How-to-study-LLMs-with-no-machine-learning-background-【From-Gork-2】" class="headerlink" title="How to study LLMs with no machine learning background? 【From Gork 2】"></a>How to study LLMs with no machine learning background? 【From Gork 2】</h3><pre>Studying Large Language Models (LLMs) from a zero machine learning background involves a step-by-step approach to build your understanding. Here's a concise guide:### **1. Basics of Machine Learning:**   - **Start with Fundamentals:**      - Learn basic concepts like supervised vs. unsupervised learning, regression, classification, and neural networks.      - Resources:       - *Coursera's Machine Learning by Andrew Ng*       - *Khan Academy's AI Course*### **2. Python and Programming:**   - **Learn Python:**      - Python is crucial for ML development. Focus on data structures, control flow, and libraries like NumPy, Pandas.     - Resources:       - *Python for Everybody* on Coursera       - *Automate the Boring Stuff with Python*### **3. Deep Learning Basics:**   - **Understand Neural Networks:**      - Grasp concepts of neurons, layers, activation functions, backpropagation.     - Resources:       - *Deep Learning Specialization by Andrew Ng on Coursera*### **4. Focus on NLP (Natural Language Processing):**   - **Basics of NLP:**     - Learn about tokenization, word embeddings (like Word2Vec, GloVe), and sequence models.     - Resources:       - *Natural Language Processing with Deep Learning* by Stanford on YouTube### **5. Introduction to LLMs:**   - **Concepts of LLMs:**     - Study transformer architecture, attention mechanisms, and how LLMs are trained (like BERT, GPT).     - Resources:       - *Illustrated Transformer* by Jay Alammar       - *Hugging Face's NLP Course*### **6. Practical Experience:**   - **Implement Small Projects:**     - Start with fine-tuning pre-trained models rather than training from scratch due to computational constraints.     - Use platforms like Google Colab or Kaggle for free GPU resources to run models.### **7. Community and Resources:**   - **Engage with Communities:**     - Join forums like Stack Overflow, Reddit (r/MachineLearning, r/LanguageTechnology), and Discord groups.   - **Read Papers and Blogs:**     - Follow research papers on arXiv, and blogs by companies like Hugging Face or Google AI.### **8. Advanced Learning:**   - **Once you have a foundation:**     - Dive into specifics of model architectures, optimization techniques, and deployment strategies.### **9. Keep Learning:**   - **Stay Updated:**      - The field of LLMs is rapidly evolving. Follow conferences like NeurIPS or ACL for the latest advancements.Starting from zero, this journey will take time and dedication. Patience and consistent practice are key. Remember, each step builds on the previous one, so don't rush through the foundational learning.</pre><h3 id="从零开始构建大模型（如LLM）【来自-open-ai】"><a href="#从零开始构建大模型（如LLM）【来自-open-ai】" class="headerlink" title="从零开始构建大模型（如LLM）【来自 open.ai】"></a>从零开始构建大模型（如LLM）【来自 open.ai】</h3><pre>从零构建大模型（如LLM）包括设计模型架构（如Transformer）、准备和清洗大量数据集、实现和预训练模型（使用PyTorch或TensorFlow）、优化和调试（采用分布式训练和混合精度）、以及微调和部署。常用工具有Hugging Face Datasets（数据处理）、PyTorch/TensorFlow（模型实现）、DeepSpeed（分布式训练）、TensorBoard（监控）、以及NVIDIA CUDA（硬件加速）。构建过程中需要强大的计算资源，如多GPU或TPU集群，云服务（如AWS或Google Cloud）可提供支持。</pre><h3 id="Max-Tokens、Context-Window-和-Context-Length"><a href="#Max-Tokens、Context-Window-和-Context-Length" class="headerlink" title="Max Tokens、Context Window 和 Context Length"></a>Max Tokens、Context Window 和 Context Length</h3><ul><li>grok3 (2025-02-24)</li><li>Max Tokens、Context Window 和 Context Length 主要关注输入的限制，但它们也可能间接影响到输出，具体来说，通常与模型的生成输出长度或质量有关系。</li></ul><ol><li>Max Tokens (最大令牌数)<ul><li>Max Tokens 既包括输入的令牌数，也包括模型生成的输出的令牌数。举个例子，假设一个模型的 Max Tokens 是 4096：</li><li>如果输入文本占用了 1000 个令牌，那么剩余的 3096 个令牌就可以用来生成输出。</li><li>如果输入文本占用的令牌数较多，那么可用来生成的输出就会变少，反之亦然。</li><li>所以，Max Tokens 会直接限制模型的输出长度，因为它是输入和输出令牌数的总和。</li></ul></li><li>Context Window（上下文窗口）<ul><li>Context Window 通常更侧重于模型在生成过程中“能看到”的上下文范围。对于长文本，如果输入超过了 Context Window 的限制，超出的部分会被截断，模型只会处理窗口内的文本内容，而不会利用超出部分来生成更合适的回答。</li><li>因此，Context Window 主要限制了模型在生成时能够参考的输入量，进而间接影响输出的质量、相关性和连贯性。</li></ul></li><li>Context Length（上下文长度）<ul><li>Context Length 指的是你实际输入给模型的文本长度，包括用户输入和可能的上下文（例如先前的对话历史）。它影响了模型的理解过程，并且通常是 Max Tokens 限制的一部分。</li><li>如果输入的 Context Length 较长（即文本较多），那么会减少模型在生成输出时可用的令牌数，进而影响输出的长度。</li></ul></li></ol><ul><li>总结：<ul><li>Max Tokens 直接限制了输入和输出的总令牌数，影响输出的最大长度。</li><li>Context Window 影响模型生成输出时能参考多少输入内容，因此间接影响输出的质量、相关性和连贯性。</li><li>Context Length 是指输入的实际长度，它在占用较多令牌时可能会减少输出可用的令牌数，从而影响输出的长度。</li><li>所以，虽然这三个参数主要是控制输入的，但由于它们与模型的令牌处理能力密切相关，它们也会间接影响生成的输出。</li></ul></li></ul><h3 id="构建大模型的过程"><a href="#构建大模型的过程" class="headerlink" title="构建大模型的过程"></a>构建大模型的过程</h3><ul><li>From ChatGPT (4o) (2025-03-03)</li><li><a href="https://chatgpt.com/share/67c531cc-e618-800b-a49d-cae86ebde079">https://chatgpt.com/share/67c531cc-e618-800b-a49d-cae86ebde079</a></li><li>从头构建一个大模型（比如类似 GPT、LLaMA 这样的 Transformer 结构），涉及确定神经元个数（即隐藏层维度）、神经网络层数（深度）、以及激活函数的选择。这个过程通常需要结合理论分析和实验调优。</li><li>神经元个数决定了模型的表达能力，主要涉及两个关键参数：<ul><li>d_model（隐藏层维度）：决定每个 token 的表示能力。</li><li>d_ff（前馈层维度）：影响非线性变换的能力</li></ul></li><li>层数（L）决定了模型可以学习的层级信息。</li><li>激活函数的作用是引入非线性，否则整个网络只是线性变换，无法学习复杂模式。</li></ul><h3 id="训练后大型语言模型的组成"><a href="#训练后大型语言模型的组成" class="headerlink" title="训练后大型语言模型的组成"></a>训练后大型语言模型的组成</h3><ul><li><p>From Gork3 (2025-02-25)</p></li><li><p>关键要点</p><ul><li>研究表明，大型语言模型训练完成后，其数据内容主要是参数，包括词向量和映射关系。</li><li>证据倾向于认为，这些参数是神经网络的权重和偏差，编码了从训练数据中学到的语言模式。</li><li>模型本身不存储原始训练数据，而是通过参数捕获语言的统计关系。</li></ul></li><li><p>参数：这些是神经网络的权重和偏差，存储在模型文件中，用于处理输入并生成输出。      </p></li><li><p>词向量（也称为嵌入），它们是单词的数值表示，捕捉单词的语义和句法意义。</p></li><li><p>映射关系，通过神经网络各层的权重定义，决定了如何处理这些词向量以生成文本。</p></li><li><p>结论：训练后的大型语言模型的数据内容是其参数，包括词向量和映射关系。这些参数通过嵌入层、前馈网络和注意力机制等组件实现，捕捉了语言的统计模式。虽然模型不直接存储训练数据，但其参数可能隐含记忆某些内容。  </p></li><li><p>From ChatGPT(o4) (2025-03-03)</p></li><li><p>除上述的数据之外，还包括</p><ul><li>模型架构（Neural Network 结构）：指的是模型的层数、注意力机制、激活函数等，比如 config.json 里会定义 Transformer 结构、隐藏层大小、head 数量、dropout 率等</li><li>其他辅助信息：训练时的一些超参数、优化器状态等</li></ul></li><li><p>这些数据以一定的格式和文件保存在大模型的训练结果中。</p><ul><li>比如Hugging Face transformers 生态 里的 “事实标准”，但并不是所有大模型都会按这个格式存储。不同的框架、实现方式和研究机构可能会有自己的格式和规范。</li><li>不同模型格式要用 相应的加载器，不能混用。不同的深度学习框架、训练方式，甚至不同的硬件优化方式，都会影响模型的存储格式和加载方式。</li></ul></li></ul><h3 id="如何加载大模型"><a href="#如何加载大模型" class="headerlink" title="如何加载大模型"></a>如何加载大模型</h3><ul><li><a href="https://mp.weixin.qq.com/s/KG2yb15EkYuWZFOwF0UT5g">大语言模型引擎全解析：Transformers、vLLM、Llama.cpp、SGLang、MLX 和 Ollama，最佳选择？</a></li></ul><h3 id="神经网络中，权重和偏置用公式的表现形式"><a href="#神经网络中，权重和偏置用公式的表现形式" class="headerlink" title="神经网络中，权重和偏置用公式的表现形式"></a>神经网络中，权重和偏置用公式的表现形式</h3><ul><li><p>From ChatGPT(o4) (2025-03-03)</p></li><li><p><a href="https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575">https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575</a></p></li><li><p>神经网络中的权重和偏置在前向传播时，基本上都是一次方程（线性变换），但整个神经网络通常是非线性映射，因为每层的输出会经过非线性激活函数。</p></li><li><p>在神经网络中，<strong>输入 X 的个数（也就是特征的维度）</strong>由具体的任务和数据决定</p><ol><li>由数据决定（特征数量）</li><li>由网络结构决定</li><li>由数据预处理决定</li></ol></li></ul><h3 id="发布的大模型，所指的参数个数怎么计算出来的"><a href="#发布的大模型，所指的参数个数怎么计算出来的" class="headerlink" title="发布的大模型，所指的参数个数怎么计算出来的"></a>发布的大模型，所指的参数个数怎么计算出来的</h3><ul><li>From ChatGPT(o4) (2025-03-03)</li><li><a href="https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575">https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575</a></li><li>参数（parameters） 指的是整个模型的权重和偏置，包括所有层的权重矩阵和偏置向量</li><li>参数个数：由模型层数、神经元数量、词向量维度决定，影响模型的存储和计算复杂度。</li></ul><h3 id="用英语和中文询问大型语言模型有何不同"><a href="#用英语和中文询问大型语言模型有何不同" class="headerlink" title="用英语和中文询问大型语言模型有何不同"></a>用英语和中文询问大型语言模型有何不同</h3><ul><li>From Gork3 (2025-03-03)</li><li>研究表明，使用英语或中文询问大型语言模型（LLM）的主要区别在于语言处理方式和模型性能可能因语言而异。</li><li>证据倾向于认为，英语因其丰富的训练数据，通常在某些任务上表现优于中文，但这取决于具体模型和任务。</li><li>令人意外的是，中文的字符处理方式（如基于字符的标记化）与英语的单词或子词标记化不同，这可能会影响模型的理解和生成能力。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>《大模型应用开发极简入门：基于 GPT-4 和 ChatGPT》</li><li>《大语言模型》- LLMBook - <a href="https://github.com/RUCAIBox/LLMSurvey">https://github.com/RUCAIBox/LLMSurvey</a></li><li>《大规模语言模型：从理论到实践》- LLM-TAP</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;以下内容基本都是从书籍中摘抄，用于个人记录&lt;br&gt;从AI得出的回答不一定正确或者只是现阶段暂时正确&lt;br&gt;增加部分个人理解&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/202</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="LLM" scheme="http://yoursite.com/tags/LLM/"/>
    
    <category term="大模型" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>人工智能相关技术简要总结记录</title>
    <link href="http://yoursite.com/2024/12/03/20241203-ren-gong-zhi-neng-xiang-guan-ji-zhu-jian-yao-zong-jie-ji-lu/"/>
    <id>http://yoursite.com/2024/12/03/20241203-ren-gong-zhi-neng-xiang-guan-ji-zhu-jian-yao-zong-jie-ji-lu/</id>
    <published>2024-12-03T13:37:02.000Z</published>
    <updated>2024-12-31T07:32:07.848Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>持续补充…</p></blockquote></blockquote><p><img src="/2024/12/03/20241203-ren-gong-zhi-neng-xiang-guan-ji-zhu-jian-yao-zong-jie-ji-lu/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.jpg"></p><h2 id="人工智能的三种形态"><a href="#人工智能的三种形态" class="headerlink" title="人工智能的三种形态"></a>人工智能的三种形态</h2><ul><li>来自：<a href="https://zhuanlan.zhihu.com/p/64467701">人工智能是什么？——人工智能图谱</a></li><li>弱人工智能：弱人工智能 (Artificial Narrow Intelligence, ANI) 是擅长与单个方面的人工智能，比如有能战胜象棋世界冠军的人工智能，但是它只会下象棋，你要问它怎样更好地在硬盘上存储数据，它就不知道怎么回答你了；</li><li>强人工智能：强人工智能 (Artificial General Intelligence, AGI) ，是人类级别的人工智能，强人工智能是指在各方面都能和人类比肩的人工智能，人类能干的脑力活它都能干。创造强人工智能比创造弱人工智能要难得多，我们现在还做不到。Linda Gottfredson教授把智能定义为“一种宽泛的心理能力，能够进行思考、计划、解决问题、抽象思维、理解复杂理念，快速学习和从经验中学习等操作”。强人工智能在进行这些操作时，应该和人类一样得心应手；</li><li>超人工智能：超人工智能 (Artificial Super Intelligence, ASI)，牛津哲学家，知名人工智能思想家Nick Bostrom把超级智能定义为“在几乎所有领域都比最聪明的人类大脑都聪明很多，包括科技创新、通识和社交技能”。超人工智能可以是各方面都比人类强一点，也可以是各方面都比人类强万亿倍，超人工智能也正是为什么人工智能这个话题这么火热的缘故，同样也是为什么永生和灭绝这两个词会在本文中多次出现。</li></ul><h2 id="关于大模型的扩展思考"><a href="#关于大模型的扩展思考" class="headerlink" title="关于大模型的扩展思考"></a>关于大模型的扩展思考</h2><ul><li>目前的AI是一个无限能量和记忆的辅助工具，但跟人有差别，比如犯错时难以追责，毕竟人需要信誉等等….</li><li>cursor、 Windsurf</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/64467701">人工智能是什么？——人工智能图谱</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;持续补充…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2024/12/03/20241203-ren-gong-zhi-neng-xiang-guan-ji-zhu-jia</summary>
      
    
    
    
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>一个无中心节点的局域网通信工具</title>
    <link href="http://yoursite.com/2024/12/02/20241202-yi-ge-wu-zhong-xin-jie-dian-de-ju-yu-wang-tong-xin-gong-ju/"/>
    <id>http://yoursite.com/2024/12/02/20241202-yi-ge-wu-zhong-xin-jie-dian-de-ju-yu-wang-tong-xin-gong-ju/</id>
    <published>2024-12-02T06:54:49.000Z</published>
    <updated>2024-12-03T06:15:25.149Z</updated>
    
    <content type="html"><![CDATA[<h2 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h2><ol><li>初始阶段：UDP广播发现</li><li>建立连接后：切换到TCP进行可靠通信（也可以使用 UDP）（增加接收完成确认机制）</li></ol><h2 id="AI生成的需求文档"><a href="#AI生成的需求文档" class="headerlink" title="AI生成的需求文档"></a>AI生成的需求文档</h2><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h2 id="局域网去中心化点对点通信工具-iOS-应用需求文档"><a href="#局域网去中心化点对点通信工具-iOS-应用需求文档" class="headerlink" title="局域网去中心化点对点通信工具 iOS 应用需求文档"></a>局域网去中心化点对点通信工具 iOS 应用需求文档</h2><h3 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h3><p>开发一款完全去中心化的局域网点对点即时通信应用，无需中心服务器，通过UDP广播实现设备发现和通信。</p><h3 id="核心设计理念"><a href="#核心设计理念" class="headerlink" title="核心设计理念"></a>核心设计理念</h3><ul><li>完全去中心化</li><li>基于局域网的点对点直接通信</li><li>无需任何中心服务器</li><li>设备间直接建立连接</li></ul><h3 id="网络通信技术架构"><a href="#网络通信技术架构" class="headerlink" title="网络通信技术架构"></a>网络通信技术架构</h3><h4 id="1-设备发现机制"><a href="#1-设备发现机制" class="headerlink" title="1. 设备发现机制"></a>1. 设备发现机制</h4><h5 id="1-1-UDP-广播发现"><a href="#1-1-UDP-广播发现" class="headerlink" title="1.1 UDP 广播发现"></a>1.1 UDP 广播发现</h5><ul><li>使用UDP广播进行设备发现</li><li>广播地址：255.255.255.255</li><li>广播端口：固定端口（如 48689）</li><li>发现报文结构：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;deviceId&quot;</span><span class="punctuation">:</span> <span class="string">&quot;唯一设备标识&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;nickname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;用户昵称&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="string">&quot;时间戳&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;publicKey&quot;</span><span class="punctuation">:</span> <span class="string">&quot;公钥信息&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;capabilities&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;支持的功能列表&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-连接建立协议"><a href="#2-连接建立协议" class="headerlink" title="2. 连接建立协议"></a>2. 连接建立协议</h4><ul><li>发现阶段：UDP 广播</li><li>连接阶段：切换到 TCP</li><li>通信协议：自定义应用层协议</li><li>连接建立流程：<ol><li>UDP 广播发现</li><li>交换公钥</li><li>建立加密 TCP 通道</li></ol></li></ul><h4 id="3-通信模型"><a href="#3-通信模型" class="headerlink" title="3. 通信模型"></a>3. 通信模型</h4><ul><li>无中心节点</li><li>每个设备既是客户端又是服务端</li><li>直接点对点通信</li><li>消息路由完全去中心化</li></ul><h3 id="功能需求详细描述"><a href="#功能需求详细描述" class="headerlink" title="功能需求详细描述"></a>功能需求详细描述</h3><h4 id="1-网络发现与连接"><a href="#1-网络发现与连接" class="headerlink" title="1. 网络发现与连接"></a>1. 网络发现与连接</h4><ul><li>持续监听 UDP 广播</li><li>自动发现局域网内设备</li><li>支持手动&#x2F;自动添加好友</li><li>好友关系本地持久化</li></ul><h4 id="2-消息通信机制"><a href="#2-消息通信机制" class="headerlink" title="2. 消息通信机制"></a>2. 消息通信机制</h4><ul><li>UDP 广播发现</li><li>TCP 建立稳定通道</li><li>消息队列管理</li><li>离线消息处理</li></ul><h4 id="3-安全性设计"><a href="#3-安全性设计" class="headerlink" title="3. 安全性设计"></a>3. 安全性设计</h4><ul><li>设备间公钥交换</li><li>端到端消息加密</li><li>防重放攻击</li><li>消息签名验证</li></ul><h3 id="技术实现细节"><a href="#技术实现细节" class="headerlink" title="技术实现细节"></a>技术实现细节</h3><h4 id="网络通信协议"><a href="#网络通信协议" class="headerlink" title="网络通信协议"></a>网络通信协议</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant A as 设备A</span><br><span class="line">    participant B as 设备B</span><br><span class="line">    </span><br><span class="line">    A-&gt;&gt;B: UDP广播：发现请求</span><br><span class="line">    B--&gt;&gt;A: UDP广播：发现响应</span><br><span class="line">    A-&gt;&gt;B: TCP建立连接</span><br><span class="line">    A--&gt;&gt;B: 交换公钥</span><br><span class="line">    A-&gt;&gt;B: 加密消息传输</span><br></pre></td></tr></table></figure><h4 id="消息队列流程"><a href="#消息队列流程" class="headerlink" title="消息队列流程"></a>消息队列流程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stateDiagram-v2</span><br><span class="line">    [*] --&gt; 消息生成</span><br><span class="line">    消息生成 --&gt; 本地队列</span><br><span class="line">    本地队列 --&gt; 检测网络</span><br><span class="line">    检测网络 --&gt; 同一局域网: 直接发送</span><br><span class="line">    检测网络 --&gt; 离线: 保持队列</span><br><span class="line">    同一局域网 --&gt; 消息发送</span><br><span class="line">    消息发送 --&gt; [*]</span><br></pre></td></tr></table></figure><h3 id="系统架构图"><a href="#系统架构图" class="headerlink" title="系统架构图"></a>系统架构图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[UDP广播发现] --&gt; B[设备信息交换]</span><br><span class="line">    B --&gt; C[TCP安全通道]</span><br><span class="line">    C --&gt; D[端到端加密通信]</span><br><span class="line">    D --&gt; E[本地消息队列]</span><br><span class="line">    E --&gt; F[离线/在线消息管理]</span><br></pre></td></tr></table></figure><h3 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h3><ul><li>语言：Swift</li><li>网络框架：Network.framework</li><li>加密：CryptoKit</li><li>存储：CoreData</li><li>UI：SwiftUI</li></ul><h3 id="关键技术挑战"><a href="#关键技术挑战" class="headerlink" title="关键技术挑战"></a>关键技术挑战</h3><ol><li>跨设备一致性</li><li>网络发现可靠性</li><li>安全性保证</li><li>低功耗设计</li></ol><h3 id="性能目标"><a href="#性能目标" class="headerlink" title="性能目标"></a>性能目标</h3><ul><li>设备发现时间 &lt; 3秒</li><li>消息延迟 &lt; 1秒</li><li>电池资源占用 &lt; 5%</li><li>网络资源占用最小化</li></ul><h3 id="安全性要求"><a href="#安全性要求" class="headerlink" title="安全性要求"></a>安全性要求</h3><ul><li>端到端加密</li><li>防重放攻击</li><li>设备指纹验证</li><li>消息签名机制</li></ul><h3 id="隐私保护"><a href="#隐私保护" class="headerlink" title="隐私保护"></a>隐私保护</h3><ul><li>无中心服务器</li><li>本地数据完全加密</li><li>用户完全控制通信</li><li>不依赖任何第三方服务</li></ul><h3 id="开发交付物"><a href="#开发交付物" class="headerlink" title="开发交付物"></a>开发交付物</h3><ol><li>完整 Xcode 工程</li><li>详细技术文档</li><li>安全性测试报告</li><li>性能基准测试结果</li></ol><h3 id="版本迭代计划"><a href="#版本迭代计划" class="headerlink" title="版本迭代计划"></a>版本迭代计划</h3><ul><li>v1.0：基础局域网通信</li><li>v1.1：增强安全性</li><li>v1.2：优化性能</li><li>v2.0：跨平台支持</li></ul><h3 id="开发注意事项"><a href="#开发注意事项" class="headerlink" title="开发注意事项"></a>开发注意事项</h3><ul><li>严格遵循 Apple 安全guidelines</li><li>代码安全审查</li><li>最小权限原则</li><li>用户体验优先</li></ul><p>&#x3D;&#x3D;&#x3D;&#x3D;</p><h3 id="Rust简易版本"><a href="#Rust简易版本" class="headerlink" title="Rust简易版本"></a>Rust简易版本</h3><ul><li><a href="https://github.com/Kingson4Wu/mesh-talk">https://github.com/Kingson4Wu/mesh-talk</a> （使用windsurf生成）</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;通信协议&quot;&gt;&lt;a href=&quot;#通信协议&quot; class=&quot;headerlink&quot; title=&quot;通信协议&quot;&gt;&lt;/a&gt;通信协议&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;初始阶段：UDP广播发现&lt;/li&gt;
&lt;li&gt;建立连接后：切换到TCP进行可靠通信（也可以使用 UDP）（增加接收</summary>
      
    
    
    
    
    <category term="UDP" scheme="http://yoursite.com/tags/UDP/"/>
    
  </entry>
  
  <entry>
    <title>AI已经如此强大了，帮我写完所有代码</title>
    <link href="http://yoursite.com/2024/11/29/20241129-ai-yi-jing-ru-ci-qiang-da-liao-bang-wo-xie-wan-suo-you-dai-ma/"/>
    <id>http://yoursite.com/2024/11/29/20241129-ai-yi-jing-ru-ci-qiang-da-liao-bang-wo-xie-wan-suo-you-dai-ma/</id>
    <published>2024-11-29T15:34:32.000Z</published>
    <updated>2024-12-01T13:36:14.825Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>今天使用Windsurf帮我写一个小工具。</p></li><li><p>全程没有写过一行代码和一个字，全靠一直给AI提需求，配合授权运行纠正；有时感觉偏离正确的道路时，结合git命令回退代码，再重新提需求。就这样一直重复，就帮我把这个小工具写好了，包括使用文档，构建脚本等，通通都包了。</p></li><li><p>自从两年前第一次用ChatGPT时一下子服气了，这次直接帮我把整个项目搞定了，不得不再次拜服。客观来讲，如果是我一个人来做，由于对Rust不熟练等原因，估计得一周，而且还会很费劲，然而它半天就搞定了，并且我做得不一定比它好。</p></li><li><p>使用AI帮助编程，就像一个架构师，有一群AI小弟帮你做事。</p></li><li><p>想要把事情快速做好，前提是你是一个有品位的架构师，另外即使你很多不懂，你也是有很大可能通过你的AI小弟变成一个优秀的架构师。所以学习方法，总结，知识体系很重要，拥抱变化。</p></li><li><p>AI为什么这么强大，对里面的细节是越来越好奇了。</p></li><li><p>项目地址：<a href="https://github.com/Kingson4Wu/magic-converter">https://github.com/Kingson4Wu/magic-converter</a></p></li></ul><p><img src="/2024/11/29/20241129-ai-yi-jing-ru-ci-qiang-da-liao-bang-wo-xie-wan-suo-you-dai-ma/Windsurf.jpeg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;今天使用Windsurf帮我写一个小工具。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;全程没有写过一行代码和一个字，全靠一直给AI提需求，配合授权运行纠正；有时感觉偏离正确的道路时，结合git命令回退代码，再重新提需求。就这样一直重复，就帮我把这个小工具写好了，包</summary>
      
    
    
    
    
    <category term="Rust" scheme="http://yoursite.com/tags/Rust/"/>
    
    <category term="AI" scheme="http://yoursite.com/tags/AI/"/>
    
    <category term="Windsurf" scheme="http://yoursite.com/tags/Windsurf/"/>
    
  </entry>
  
  <entry>
    <title>对Java、Go、Rust之间的简单对比和总结</title>
    <link href="http://yoursite.com/2024/11/21/20241121-dui-java-go-rust-zhi-jian-de-jian-dan-dui-bi-he-zong-jie/"/>
    <id>http://yoursite.com/2024/11/21/20241121-dui-java-go-rust-zhi-jian-de-jian-dan-dui-bi-he-zong-jie/</id>
    <published>2024-11-21T08:42:35.000Z</published>
    <updated>2024-11-21T14:50:27.463Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>工作中接触过多种编程语言，主要是 Java 和 Go，最近因个人兴趣简单学习了 Rust，在这里简单记录总结一下</p></blockquote></blockquote><h2 id="编程语言的GC问题"><a href="#编程语言的GC问题" class="headerlink" title="编程语言的GC问题"></a>编程语言的GC问题</h2><ul><li><p>一般来说，程序需要管理好运行中使用的内存空间，比如传统的C和C++则要求开发者手动管理内存，但这往往导致内存泄漏和安全隐患；而垃圾回收（GC）语言，比如Java和Go在运行时自动回收内存，但存在”停顿”（STW）问题；而Rust则采用独特的所有权系统，通过编译期严格的规则检查，在不增加运行时开销的情况下实现内存安全。</p></li><li><p>GC语言，调用栈和内存一直在变化，不STW无法算出没引用的变量（可回收的内存）； 而Rust通过作用域的规则判断自动回收。另外无GC不代表不在堆分配，是代表没有STW的垃圾回收机制。</p></li><li><p>Rust引入了”所有权”概念，每个值都有且仅有一个所有者，当所有者离开作用域时，值会被自动释放。这种方式不仅避免了运行时垃圾回收的性能开销，还能在编译阶段就发现潜在的内存使用问题，有效防止了常见的内存安全缺陷。</p></li></ul><h2 id="设计哲学"><a href="#设计哲学" class="headerlink" title="设计哲学"></a>设计哲学</h2><ul><li>Java 作为一门成熟的编程语言，其设计理念更多体现在企业级应用和跨平台兼容性上。当然个人认为由此历史包袱也比较重。</li><li>相比之下，Go 和 Rust 作为更现代的语言，也各有侧重。Go 语言强调简洁、高效和并发性，而 Rust 则更加注重内存安全、零成本抽象和并发安全性。</li></ul><h2 id="交叉编译"><a href="#交叉编译" class="headerlink" title="交叉编译"></a>交叉编译</h2><ul><li>Go 和 Rust 支持各自编译成对应二进制实现跨平台（可以使用交叉编译）；而Java则编译成统一的字节码，依赖平台安装的运行时（JVM）来运行服务（也可以Graalvm直接编译成可执行二进制）</li></ul><h2 id="工具链"><a href="#工具链" class="headerlink" title="工具链"></a>工具链</h2><ul><li>相关工具链完善问题，比如Java性能依赖外部开发，比如arthas，asyncProfiler等；而Go自带pprof，单元测试工具等（Rust 也有一些相应的配套工具）；Java历史包袱重，不够现代化</li></ul><h2 id="热加载"><a href="#热加载" class="headerlink" title="热加载"></a>热加载</h2><ul><li>Java支持热加载（基于 Instrumentation 技术），但也有一定的限制，比如不能新增&#x2F;删除方法、类等，主要通过字节码替换和类加载器重载实现，一般多在开发阶段使用。实际应用中，JRebel 等商业工具通过更复杂的字节码重写技术，部分突破了这些限制，而Spring DevTools 提供了更轻量的重启机制。</li><li>Go官方不直接支持热加载；第三方工具如 gin-reload、air 实现热重载（通过监控文件变化，重新编译和启动进程，相对简单直接，但不是语言级特性）</li><li>Rust同样没有官方直接的热加载机制；比如cargo-watch 可以监听文件变化并重新编译（由于所有权系统，热加载实现相对复杂）</li></ul><h2 id="远程Debug"><a href="#远程Debug" class="headerlink" title="远程Debug"></a>远程Debug</h2><ul><li>Java远程调试的原理是两个VM之间通过debug协议进行通信，然后以达到远程调试的目的。两者之间可以通过socket进行通信。</li><li>Go原生支持远程调试，使用 dlv（Delve）调试器（基于 gRPC 协议通信）</li><li>Rust支持远程调试，但配置相对较复杂（主要使用 rust-gdb 和 rust-lldb）</li></ul><h2 id="依赖管理-以及-冲突解决"><a href="#依赖管理-以及-冲突解决" class="headerlink" title="依赖管理 以及 冲突解决"></a>依赖管理 以及 冲突解决</h2><h3 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h3><ul><li>Java 的依赖管理历史上存在诸多挑战。在早期，Java 并没有原生的依赖版本管理机制，开发者需要依赖 Maven 或 Gradle 等外部构建工具来处理项目依赖。更为关键的是，Java 的依赖冲突解析是基于具体类而非整个 JAR 包，这导致了潜在的版本兼容性和类加载问题。为了彻底解决这一痛点，Java 9 引入了模块化系统（Java Platform Module System, JPMS），提供了更精细和可靠的依赖管理和隔离机制，从根本上改善了包依赖和版本控制的复杂性。这一设计不仅简化了大型项目的依赖管理，还增强了 Java 运行时的安全性和可预测性。</li></ul><h3 id="关于Java的类重复问题"><a href="#关于Java的类重复问题" class="headerlink" title="关于Java的类重复问题"></a>关于Java的类重复问题</h3><ul><li><p>Java 依赖引入的时 Jar 包，使用时则是含路径信息的类名</p></li><li><p>Go则没有这个问题，因为Go的依赖的引入需要指定模块的全路径，使用时也是使用全路径或别名</p></li><li><p>Rust和 Go 类似，依赖的引入也需要指定模块的全路径。但不同包有相应的依赖文件，利用这个使相同依赖的不兼容版本共存而没有冲突问题</p></li><li><p>Java9之前（模块系统之前）- 只能减少，不能从根本上解决</p><ol><li>协议文件生成的代码，重复拷贝和引入，导致类重复冲突<ul><li>使用RPC协议，idl文件生成java文件，容易因为多处拷贝（比如一些业务通用库也使用到），导致类重复问题，这样在运行时可能会造成影响</li><li>这时最好打包的时候，不要将协议文件打进jar包中，让业务使用方自行生成代码</li><li>通过扫描jar包路径类的方式，可以协助检查这种问题 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">classPath</span> <span class="operator">=</span> Optional.ofNullable(thriftClass.getProtectionDomain())</span><br><span class="line">    .map(ProtectionDomain::getCodeSource)</span><br><span class="line">    .map(CodeSource::getLocation)</span><br><span class="line">    .map(URL::getPath).orElse(<span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!classPath.contains(jarFileName)) &#123;</span><br><span class="line">    System.err.println(String.format(<span class="string">&quot;%s thrift class may be duplicated&quot;</span>, thriftClass.getName()));</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">DuplicatedThriftFileException</span>(String.format(<span class="string">&quot;%s thrift class may be duplicated&quot;</span>, thriftClass.getName()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li>通过maven-enforcer插件解决类冲突<ul><li>本质上就是解压所有依赖的 jar 包，判断是否存在重复的类文件，性能较低</li></ul></li></ol></li><li><p>JVM中Jar包的加载顺序</p><ul><li>由classpath参数指定的顺序决定</li><li>如果classpath未明确指明，则由文件系统决定的（readdir函数）<ul><li>readdir并不保证读取后的文件顺序，在不同的操作系统上可能有不同的顺序。</li></ul></li><li>如何找出重复类<ul><li><code>find . -name &quot;*.jar&quot; -exec sh -c  &#39;jar -tf &#123;&#125;|grep -H --label &#123;&#125; &#39;JspRuntimeContext &#39;&#39;</code> </li><li><code>-verbose:class</code> 查看加载顺序</li></ul></li></ul></li><li><p>Java9及以上（使用模块系统）</p></li></ul><h3 id="Go-VS-Rust-库冲突"><a href="#Go-VS-Rust-库冲突" class="headerlink" title="Go VS Rust 库冲突"></a>Go VS Rust 库冲突</h3><ul><li><p>当项目间接依赖同一个库的不同版本时，Rust 和 Go 在处理上有什么异同</p></li><li><p>Go 的处理方式：</p>  <pre>  依赖关系示例：  my-project  ├── A   │   └── pkg v1.1.0  └── B      └── pkg v1.2.3  </pre><ul><li>Go 会：<ul><li>自动选择最高兼容版本（v1.2.3）</li><li>所有代码路径都使用这个版本</li><li>使用 MVS (Minimal Version Selection) 算法</li><li>在 go.mod 中记录最终版本  <pre>  // go.mod  module my-project  require (      A v1.0.0      B v1.0.0      pkg v1.2.3 // 间接依赖，统一使用最高版本  )</pre></li></ul><p>  </p></li></ul></li><li><p>Rust 的处理方式：</p>  <pre>  依赖关系示例：  my-project  ├── A   │   └── pkg 1.1.0  └── B      └── pkg 1.2.3  </pre>    <ul><li>Rust 会：<ul><li>允许两个版本同时存在</li><li>分别编译两个版本的代码</li><li>在最终二进制中包含两个版本  <pre>  Cargo.toml  [dependencies]  A = "1.0.0"  # 依赖 pkg 1.1.0  B = "1.0.0"  # 依赖 pkg 1.2.3  </pre></li></ul></li></ul></li><li><p>主要区别：</p><ul><li>Go: 强制统一版本，避免重复</li><li>Rust: 允许多版本共存，保证兼容性</li><li>这种设计反映了两种不同的理念：<ul><li>Go: 简单性优先，避免版本冲突</li><li>Rust: 灵活性优先，保证正确性</li></ul></li></ul></li><li><p>针对依赖同一个库的不同版本的情况：如果版本相同或兼容，Cargo会选择满足要求的当前最高版本；如果版本不兼容，Cargo允许在项目中同时使用这些不兼容的版本，可以通过别名来区分使用。</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>个人看法总结：Rust能做到同时使用同一个库的不同版本，是因为每个项目都有独立的依赖库配置以及引入别名机制，关键的是打包能根据这些信息直接生成二进制。而java是生成 字节码文件，并打包时丢失这方面的信息，虚拟机可能目前由于历史和后续兼容等原因也暂不支持。Go 则是选择简单性优先，避免版本冲突。</li><li>Rust可以运行同一库不同版本；Go和Java（模块化后）都不允许同一库不同版本；Go通过路径能确定库的唯一性；Java（未模块化）存在不同库类冲突的可能。</li></ul><h2 id="封装私有性"><a href="#封装私有性" class="headerlink" title="封装私有性"></a>封装私有性</h2><ul><li><p>Java通过访问修饰符（public、private、protected）控制（反射可以破坏私有性；运行时检查私有访问）</p></li><li><p>Java 9 模块化（JPMS）后，封装私有性发生了显著变化</p><ol><li>更严格的可见性控制（引入模块（module）概念；模块间显式依赖声明）</li><li>可见性新规则（使用 exports 关键字定义可导出包；opens 关键字控制运行时反射访问）</li><li>相比传统机制（编译期就能检查模块间依赖；避免了类路径的”打开式”依赖）</li><li>实际影响（需要在 module-info.java 显式声明依赖；原有代码需要适配模块系统；更接近 Rust 的模块化设计理念）</li></ol></li><li><p>Go首字母大小写决定可见性（小写标识符包内可见，大写标识符全局可见；没有私有修饰符，依赖命名约定）</p></li><li><p>Rust模块系统提供精细的可见性控制（默认私有；pub 关键字定义可见性；可以精确控制字段、方法的可见范围；编译期检查，性能无额外开销）</p></li><li><p>Rust 的封装性设计最为现代和严格，Go 相对最为简单，Java 则相对传统，Java9 之后更加严格，跟 Rust 类似，但由于历史包袱，又显得比较笨重。</p></li></ul><h2 id="并发和多线程"><a href="#并发和多线程" class="headerlink" title="并发和多线程"></a>并发和多线程</h2><ul><li>并发线程，Rust为了减少运行时，默认使用线程模型的并发。</li><li>Go是绿色线程（协程）。</li><li>Java一般也是线程模型，当然也有一些协程库（其他 JVM 语言比如 kotlin 就自带协程）</li></ul><h3 id="主线程结束进程是否停止"><a href="#主线程结束进程是否停止" class="headerlink" title="主线程结束进程是否停止"></a>主线程结束进程是否停止</h3><ul><li>主线程退出：主线程结束，不管其他线程是否结束，进程都会结束，这点Rust和Go一样（go是协程）.<br>Java则是即使主线程结束，其他线程不结束，进程就不会退出。</li></ul><h3 id="非主线程异常进程是否停止"><a href="#非主线程异常进程是否停止" class="headerlink" title="非主线程异常进程是否停止"></a>非主线程异常进程是否停止</h3><ul><li>默认情况下，非主线程的 panic 不会导致整个进程退出，这点 Rust 和 Java 一样。<ul><li>Java 中未捕获的异常会导致线程终止，但不影响其他线程</li><li>Rust 的设计更灵活，允许开发者根据需求自行控制（比如使用 std::panic::set_hook() 设置了自定义 panic 处理，可以捕获控制）</li></ul></li><li>而 Go 中 goroutine panic 会导致整个程序崩溃（除非被 recover）</li></ul><h2 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h2><ul><li>类定义：java Python js 只有class的概念 go 只有struct概念 c++都有 区别是struct可以在栈中定义</li><li>面向对象：Java中的单继承其实简化了继承的使用方式， Go和Rust，算是彻底抛弃了使用类继承的方式，选择了接口继承。</li><li>Java设计之初就是面向对象，加上由于后续历史兼容等原因，代码看起来比较臃肿（类设计）；Rust博采众长，有各自语法糖；Go追求语法简单，表达力不足，会存在一定丑陋的代码（比如没有set， contains，streams等）</li></ul><h2 id="接口设计和多态"><a href="#接口设计和多态" class="headerlink" title="接口设计和多态"></a>接口设计和多态</h2><ul><li>Rust中的 trait 和 Java 以及 Go 的接口：本质上它们都是在解决同一个问题：如何定义和实现抽象行为。主要区别在于语言设计理念导致的一些具体细节</li></ul><h2 id="空值问题"><a href="#空值问题" class="headerlink" title="空值问题"></a>空值问题</h2><ul><li>Go的类型系统一个缺憾是，对于一个类型，它的值是零值，还是不存在值，混淆不清。Java 之前也存在类似的问题，但是后来增加了基础类型的包装类型（例如对于int的Integer，double的Double），Go是否也可以参考一下？或者增加一个Option(al)类型，对这些基础类型再包装一下（基于泛型），当然还有其他更优方案那就更好了<ul><li>JSON包新提案：用“omitzero”解决编码中的空值困局:<a href="https://mp.weixin.qq.com/s/Lw_l_AELo8RKiLzVdS0H-Q">https://mp.weixin.qq.com/s/Lw_l_AELo8RKiLzVdS0H-Q</a></li></ul></li></ul><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><ul><li>异常：Java分为Error和Exception，异常又分为运行时异常和检查性异常。抛出与捕获。<br>这点和go是类似的，go也区分简单返回的错误error和抛出的恐慌panic，而 Rust 也是差不多这么设计。</li></ul><h2 id="链式调用"><a href="#链式调用" class="headerlink" title="链式调用"></a>链式调用</h2><ul><li><p>链式调用：Rust和Java支持函数式链式编程，类似stream；Go不支持，要自己实现</p></li><li><p>Rust 的迭代器和 Java 的 Stream API 确实很像，都支持链式调用和函数式编程风格。</p></li><li><p>Go 的设计理念是追求简单直接，所以：</p><ul><li>没有内置的链式调用语法</li><li>更倾向于使用显式的 for range 循环</li><li>性能更可预测（没有懒加载特性）</li></ul></li><li><p>这反映了不同语言的设计理念：</p><ul><li>Rust&#x2F;Java：提供丰富的抽象和函数式编程特性</li><li>Go：保持简单，倾向于显式的命令式编程</li></ul></li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li>枚举：Java和Rust支持，Go不支持；Rust可以支持同个枚举内包含不同类型</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://mp.weixin.qq.com/s/UG-6UuqDiLX15dEZrGGrRA">Gopher的Rust第一课：Rust的依赖管理</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;工作中接触过多种编程语言，主要是 Java 和 Go，最近因个人兴趣简单学习了 Rust，在这里简单记录总结一下&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;编程语言的GC问题&quot;&gt;&lt;a </summary>
      
    
    
    
    
    <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
    <category term="Go" scheme="http://yoursite.com/tags/Go/"/>
    
    <category term="Rust" scheme="http://yoursite.com/tags/Rust/"/>
    
  </entry>
  
</feed>
