<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/uploads/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="拉巴力的纸皮箱" type="application/atom+xml" />






<meta name="description" content="为啥不吃三文鱼">
<meta property="og:type" content="website">
<meta property="og:title" content="拉巴力的纸皮箱">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="拉巴力的纸皮箱">
<meta property="og:description" content="为啥不吃三文鱼">
<meta property="og:locale">
<meta property="article:author" content="Kingson Wu">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/4/"/>





  <title>拉巴力的纸皮箱</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">拉巴力的纸皮箱</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/" itemprop="url">简单研究一下人工智能和数学</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2025-01-06T18:02:28+08:00">
                2025-01-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<blockquote>
<p>简单记录，未完善<br>内容基本来源书籍</p>
</blockquote>
</blockquote>
<p><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%95%B0%E5%AD%A6.png"></p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h1 id="初等数学"><a href="#初等数学" class="headerlink" title="初等数学"></a>初等数学</h1><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ul>
<li>函数的表示方法主要有三种：解析法（公式法）、图像法和表格法。在微积分中讨论的函数几乎都是用解析法表示的。</li>
</ul>
<h2 id="二项式定理"><a href="#二项式定理" class="headerlink" title="二项式定理"></a>二项式定理</h2><ul>
<li>二项式定理又称为牛顿二项式定理，它可将两个数之和的整数次幂展开为相应项之和。</li>
</ul>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h1 id="高等数学"><a href="#高等数学" class="headerlink" title="高等数学"></a>高等数学</h1><h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h1 id="人工智能数学"><a href="#人工智能数学" class="headerlink" title="人工智能数学"></a>人工智能数学</h1><ul>
<li><p>从发展历程来看，人工智能先后经历了推理机、专家系统及机器学习三个阶段。</p>
</li>
<li><p>当前的人工智能系统多为学习型。为了减小误差，其用数据去训练假设模型，也就是进行所谓的学习，当误差降到最小时，就把这个假设模型用于其他现实问题。</p>
</li>
<li><p>人工智能实际上是一个将数学、算法理论和工程实践紧密结合的领域。人工智能从本质上来看就是算法，是概率论、统计学等各种数学理论的体现。数学作为表达与刻画人工智能模型的工具，是深入理解人工智能算法原理必备的基础知识。人工智能与数学高度相关，可以说人工智能的核心是数学，计算机只是实现人工智能模型的工具。</p>
</li>
<li><p>函数逼近：随着大数据和人工智能技术的发展，机器学习和深度学习在大数据挖掘、模式识别和智能分析越来越受重视。机器学习是一种基于数据的学习方法，其从观测数据所包含的有限信息中构造一个模型，利用该模型对未知数据或无法观测的数据进行尽可能准确的预测，这种模型称为学习机器。对数据科学而言，所有的数据都是以数字形式表示的，通过定义损失函数，选择合适的核函数或激活函数，反复学习后可达到一种最佳逼近状态，因此，机器学习问题实际上是函数估计问题或函数逼近问题。</p>
</li>
</ul>
<h2 id="微积分-1"><a href="#微积分-1" class="headerlink" title="微积分"></a>微积分</h2><ul>
<li>求导是微积分的基本概念之一，也是很多理工科领域的基础运算。导数是变化率的极限，是用来找到“线性近似”的数学工具，是一种线性变换，体现了无穷、极限、分割的数学思想，主要用来解决极值问题。人工智能算法的最终目标是得到最优化模型，其最后都可转化为求极大值或极小值的问题。</li>
<li>比如，梯度下降法和牛顿法是人工智能的基础算法，现在主流的求解代价函数最优解的方法都是基于这两种算法改造的，如随机梯度法和拟牛顿法，其底层运算就是基础的导数运算。</li>
<li>级数也是微积分中非常重要的概念，常见的级数有泰勒级数、傅里叶级数等，它们在人工智能算法中也有非常重要的地位。</li>
<li>泰勒级数体现了用多项式近似和逼近函数的思想。</li>
<li>泰勒级数在人工智能算法的底层起到了非常重要的作用，泰勒级数对理解很多基础算法的原理很有帮助。例如，梯度下降法的数学原理涉及代价函数的一阶泰勒近似，而牛顿法的推导过程应用了目标函数的二阶泰勒近似。</li>
<li>凸函数也是微积分中的重要概念，人工智能算法中涉及的优化问题要求函数模型必须是凸函数，否则优化问题没有最优解。</li>
<li>微积分中还有许多概念，如方向导数、梯度、伽马函数等，它们都在人工智能中有广泛的应用</li>
</ul>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><ul>
<li>线性代数的基本原理在人工智能算法中处于核心地位，在人工智能的语义分析、推荐系统、卷积神经网络等方面有大量应用，是目前最前沿的深度学习算法原理的基础。</li>
</ul>
<h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><ul>
<li>很多机器学习算法是以概率统计的理论为基础支撑推导出来的，比如代价函数的最小二乘形式、逻辑回归算法都基于对模型的最大似然估计。</li>
<li>概率论中的高斯函数及中心极限定理被广泛用于人工智能算法。独立同分布的不同随机变量之和会随变量数的增加而趋于高斯分布，因此，很多模型假设都采用高斯函数进行建模。</li>
</ul>
<h2 id="数理统计"><a href="#数理统计" class="headerlink" title="数理统计"></a>数理统计</h2><ul>
<li>概率论作用的前提是随机变量的分布已知，其根据已知的分布来分析随机变量的特征与规律；数理统计的研究对象则是分布未知的随机变量，其研究方法是对随机变量进行独立重复的观察，根据得到的观察结果对原始分布做出推断，数理统计可以看作逆向的概率论。</li>
<li>若检验是通过随机抽取的样本来对一个总体的判断结果进行认可或否定，则可以将其用于估计机器学习模型的泛化能力。</li>
</ul>
<h2 id="最优化理论"><a href="#最优化理论" class="headerlink" title="最优化理论"></a>最优化理论</h2><ul>
<li>人工智能的目标就是最优化，就是在复杂环境与多体交互中做出最优决策。几乎所有的人工智能问题最后都会归结为一个优化问题的求解，因此，最优化理论同样是学习、研究人工智能必备的基础知识。<br>最优化理论研究的问题是判定给定目标函数是否存在最大值或最小值，并找到令目标函数取最大值或最小值的数值。如果把给定的目标函数看成连绵的山脉，最优化的过程就是找到顶峰（谷底）且到达顶峰（谷底）的过程。</li>
<li>最优化理论的研究内容主要包括线性规划、（不）精确搜索、梯度下降法、牛顿法、共轭梯度法、拟牛顿法、（非）线性最小二乘法、约束优化最优性条件、二次规划、罚函数法和信赖域法等。</li>
<li>要实现最小化或最大化的函数称为目标函数，大多数最优化问题都可以通过使目标函数￼最小化解决，最大化问题也可以通过最小化￼来解决。最优化方法找到的可能是目标函数的全局最小值，也可能是局部极小值，两者的区别在于全局最小值比定义域内所有其他点的函数值都小，而局部极小值只比所有邻近点的函数值小。</li>
<li>当目标函数的输入参数较多、解空间较大时，大多数实用的最优化方法都不能满足全局搜索对计算复杂度的要求，因而只能求出局部极小值。但是，在人工智能和深度学习的应用场景中，只要目标函数的取值足够小，就可以把这个值当作全局最小值使用，以此作为对性能和复杂度的折中。</li>
</ul>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<blockquote>
<blockquote>
<p>程序员的数学</p>
</blockquote>
</blockquote>
<h2 id="迈向机器学习的第一步"><a href="#迈向机器学习的第一步" class="headerlink" title="迈向机器学习的第一步"></a>迈向机器学习的第一步</h2><ul>
<li><p>由输入和目标组成的数据称为训练数据。机器学习中所谓的学习，就是为了通过给定的输入得到和目标尽可能接近的输出，使用训练数据对参数进行调整的过程。使用训练数据对参数进行过调整的模型称为训练好的模型。对于训练好的模型，需要用测试数据对其进行测试，从而评价训练的效果。</p>
</li>
<li><p>在机器学习中对参数进行调整的过程，不是由程序员完成的，而是由计算机通过训练数据自动完成的，这正是机器学习的一大特征。</p>
</li>
<li><p>这里先做个总结吧。我们面对预测问题，首先要有好的模型加上大量的训练数然后，我们需要的是“能根据输人向量，得到和目标向量尽量接近的输出向量”这样一个训练好的模型。</p>
</li>
<li><p>解决分类问题的过程，也可以说是从大量数据中总结规律和规则，从而发现模式的过程。机器学习并不会要求程序员提前研究手写字符的各种形态再去设计程序，而是由计算机根据训练数据来调整参数，从而得到分类模型，这才是它的特征所在。</p>
</li>
</ul>
<p><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png"><br><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B02.png"></p>
<ul>
<li><p>如果机器对训练数据能给出完美的输出，对测试数据给出的结果却不尽如人意，很有可能是发生了过拟合（overfitting）。用学生来说，就好比是课堂上做过的训练题都能解得很好，但考试成绩却不怎么理想。</p>
</li>
<li><p>在学习的过程中，需要比较输出和正确的值。拿这个简单的例子来说，要比较的对象就是由输入x1，x2通过给定模型得到的输出y和目标t。如果y和的值一致，那当然好，但一般并不会这么理想。对学习结果（输出）的评价不是单纯的“好与不好”，而是要知道它与训练数据中给出的目标相比“到底有多不好”。为了实现这种评价，需要引入损失函数。<br>在具体的机器学习问题中，如何选取恰当的损失函数是个重要且有难度的问题。比如使用平方和误差函数。</p>
</li>
<li><p>要通过调整模型中的权重参数，使得损失函数值尽可能接近于0。</p>
</li>
<li><p>梯度下降法<br><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.png"></p>
</li>
<li><p>作为程序员要做些什么</p>
<ul>
<li>在构建模型这个阶段，程序员是要参与的，但是参数的自动调整过程，程序员不会参与。也就是说，程序员不去直接指定参数的具体数值，而是通过模型、损失函数、训练数据，间接地让参数的选取向着更优的方向变化，从而得到需要的参数。即使模型、损失函数都相同，只要训练数据不同，学习后得到的模型也会截然不同。</li>
<li>机器学习是基于数据让机器去学习，程序员并不直接参与其中。这就像硬件配置完全同的计算机，如果软件系统不一样，整个运行模式也会不一样。把软件换掉，同一套硬件系统也会根据不同的指令做出不一样的事情。道理类似，就算模型一样，只要训练数据不同，最后模型的运行模式也会不一样。</li>
</ul>
</li>
<li><p>神经网络是指，把像感知器一样有输入和输出的节点排列起来形成的带有层次的结构。神经网络（neural network）这个词来源于生物的信息传递方式。在感知器中，输出是二元的，取值只有0或1两种情况，而神经网络中的节点输出的就不是二元，而是可以进行微分运算的连续值。</p>
</li>
</ul>
<p><img src="/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png"></p>
<ul>
<li><p>深度学习是在神经网络的基础上，通过增加层数得到的更加“深化”的模型。增加层数是为了更加精确地拟合复杂函数，就算涉及的参数个数不做大的变动，也能得到更好的模型。至于如何“深化”在理论上更为有效，依然是现在研究的热点之一。</p>
</li>
<li><p>强化学习是在“无监督”的条件下进行的“学习”。也就是说，在学习过程中没有标准答案可供参考。强化学习通过试错来寻找最优输出，对每个输出，系统都会提供反馈（奖励），模型的参数可以根据得到的反馈来调整。</p>
</li>
<li><p>做出决策</p>
<ul>
<li>机器学习会根据输人的数据预测未来。这种意义下的预测，是基于先前的经验进行的，得到的是未来最有可能发生的情况（数值）。但是，得到预测值之后“应该做些什么”，机器是无法决定的。也就是说，它们无法进行决策。</li>
<li>通过机器学习这个方法，机器可以告诉我们在未来什么样的行动会导致事情如何发展。</li>
<li>但是，做决策这件事本身，并不能让机器来做。</li>
<li>顺着这个话题继续讲下去，就不是技术问题，而是伦理问题了。比如，在减轻痛苦和延续生命之中二选一的问题等，只能由个人的意志来决定，不可能委托给机器学习。</li>
</ul>
</li>
</ul>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li>《人工智能数学基础》</li>
<li>《程序员的数学》</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2025/01/01/20250101-shi-yong-ai-zong-jie-liao-zhe-ge-wang-zhan-de-nei-rong/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2025/01/01/20250101-shi-yong-ai-zong-jie-liao-zhe-ge-wang-zhan-de-nei-rong/" itemprop="url">使用AI总结了这个网站的内容</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2025-01-01T00:29:21+08:00">
                2025-01-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<blockquote>
<p>使用google的notebooklm总结目前该网站的所有文章内容<br><a target="_blank" rel="noopener" href="https://notebooklm.google.com/">https://notebooklm.google.com/</a><br>问题：全面阐述和总结一下这个博客网站的内容，涉及领域，博主风格等</p>
</blockquote>
</blockquote>
<hr>
<p>该博客网站的内容主要涵盖了<strong>个人成长和职业发展</strong>两个方面，涉及领域包括：</p>
<ul>
<li><strong>自我营销</strong>:  如何打造个人品牌，提升影响力。</li>
<li><strong>时间管理</strong>: 如何提高工作效率，克服拖延。</li>
<li><strong>学习方法</strong>: 如何快速学习，进行深度学习，构建个人知识体系。</li>
<li><strong>心理素质</strong>: 如何拥有积极心态，提升情绪力。</li>
<li><strong>职业发展</strong>: 如何成为一名专业的软件开发人员，进行有效的薪酬谈判。</li>
<li><strong>创业</strong>: 如何打造成功的产品，制定切实可行的创业计划。</li>
<li><strong>健身</strong>: 如何利用科技产品实现健身目标。</li>
<li><strong>人际关系</strong>: 如何与他人相处，建立良好的人际关系。</li>
<li><strong>精神</strong>: 如何利用心灵的力量，克服天性，实现个人成长。</li>
<li><strong>金融</strong>: 如何理解金融危机，以及金融政策的作用。</li>
<li><strong>公司文化</strong>: 如何打造面向未来的新型组织文化，以网飞为例。</li>
<li><strong>历史</strong>: 如何理解人类社会的发展，以文字和政治组织为例。</li>
</ul>
<p>博主风格：</p>
<ul>
<li><strong>注重实用性</strong>: 博主的内容以解决实际问题为导向，提供具体可行的建议和方法。</li>
<li><strong>结合个人经验</strong>: 博主经常结合自身经历和感悟，使得内容更具说服力。</li>
<li><strong>善于运用类比</strong>: 博主擅长用通俗易懂的类比来解释复杂的概念，使读者更容易理解。</li>
<li><strong>强调深度思考</strong>: 博主鼓励读者进行深度思考，并将知识与自身实际相结合。</li>
<li><strong>注重科学依据</strong>: 博主经常引用科学研究和理论来支持自己的观点，增强内容的可信度。</li>
</ul>
<p>总而言之，该博客网站的内容丰富实用，风格清晰易懂，对于想要提升个人能力和实现职业成功的读者来说，具有很高的参考价值。</p>
<hr>
<h2 id="排除掉笔记类文章后"><a href="#排除掉笔记类文章后" class="headerlink" title="排除掉笔记类文章后"></a>排除掉笔记类文章后</h2><p>这个博客网站的内容主要涉及<strong>软件开发和技术管理</strong>领域，涵盖了<strong>业务方案、架构设计、技术实践、团队管理和个人成长</strong>等方面。博主的写作风格<strong>简洁、实用、注重经验总结和思考</strong>，文章内容多来自实际工作中的问题和解决方案，并结合个人思考和学习心得进行阐述。</p>
<p>以下是该博客网站内容的详细分析：</p>
<p><strong>一、内容领域：</strong></p>
<ul>
<li><strong>业务方案：</strong>涵盖了分页、兼容性、事务、前置校验、幂等、错误码处理、接口安全、容灾降级、货币体系设计、榜单实现等常见业务场景的解决方案和实践经验总结。</li>
<li><strong>架构设计：</strong>探讨了微服务架构、服务间一致性、数据库高可用方案、容灾架构、聚合层设计等架构设计理念和实践经验。</li>
<li><strong>技术实践：</strong>分享了RPC调用、事务处理、数据库同步、Redis应用、单元测试、配置中心、序列化协议等技术点的实践经验和工具技巧。</li>
<li><strong>团队管理：</strong>总结了个人管理经验、小团队管理要点、项目管理技巧、团队协作、沟通技巧等团队管理方面的经验和思考。</li>
<li><strong>个人成长：</strong>记录了博主从PHP、C++、Lua到Java、Go、Rust等不同编程语言的学习和实践经历，以及对技术发展趋势的思考和个人职业发展的感悟。</li>
</ul>
<p><strong>二、博主风格:</strong></p>
<ul>
<li><strong>简洁实用：</strong>文章语言简洁明了，直击问题核心，注重实用性，避免过多理论和概念的堆砌。</li>
<li><strong>经验总结：</strong>文章内容多来自博主实际工作中遇到的问题和解决方案，并进行总结提炼，形成可供参考的经验。</li>
<li><strong>思考深入：</strong>博主不局限于问题本身，而是深入思考问题背后的原因和本质，并提出自己的见解和解决方案。</li>
<li><strong>持续学习：</strong>博主不断学习新的技术和知识，并将其应用到实际工作中，乐于分享自己的学习心得和体会。</li>
</ul>
<p><strong>三、其他特点:</strong></p>
<ul>
<li><strong>注重代码质量：</strong>博主强调单元测试、代码规范、代码扫描工具等提升代码质量的方法。</li>
<li><strong>关注团队协作：</strong>博主重视团队沟通、信息透明、知识分享等，提倡高效的团队协作方式。</li>
<li><strong>追求技术进步：</strong>博主积极探索新的技术和架构，并思考技术发展趋势对个人和团队的影响。</li>
</ul>
<p><strong>总结:</strong></p>
<p>该博客网站内容丰富，涵盖了软件开发和技术管理的多个方面，体现了博主丰富的实践经验和深入的思考。博主的写作风格简洁实用，注重经验总结和思考，对于从事相关领域工作的读者具有较高的参考价值。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/12/31/20241231-ai-gong-ye-ge-ming-xia-de-ruo-gan-si-kao/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/12/31/20241231-ai-gong-ye-ge-ming-xia-de-ruo-gan-si-kao/" itemprop="url">AI工业革命下的若干思考</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-12-31T15:24:38+08:00">
                2024-12-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<blockquote>
<p>随着AI工具变得日益智能，人们对自身可能被取代的担忧也在增加。<br>回顾以往工业革命带来的社会变迁，或许能从中推测出未来发展的趋势。<br>以下内容由ChatGPT辅助生成。</p>
</blockquote>
</blockquote>
<h1 id="AI对就业的双重影响"><a href="#AI对就业的双重影响" class="headerlink" title="AI对就业的双重影响"></a>AI对就业的双重影响</h1><ul>
<li>AI技术对就业市场带来了创造效应和破坏效应：<ul>
<li>创造效应：AI的普及将催生了许多潜在的新兴职业。这些新兴岗位对熟练掌握AI工具的专业人才需求旺盛。</li>
<li>破坏效应：AI替代了许多重复性、规则性较强的岗位。某些中等技能的岗位如流水线操作、传统文秘工作可能面临消失的风险。</li>
</ul>
</li>
</ul>
<h1 id="历史视角：被替代的人何去何从"><a href="#历史视角：被替代的人何去何从" class="headerlink" title="历史视角：被替代的人何去何从"></a>历史视角：被替代的人何去何从</h1><ul>
<li><p>随着AI技术的发展，未来许多人的工作将不可避免地被替代，这其中包括一部分程序员，但相较而言，更多非程序员的工作可能会首当其冲。然而，那些更擅长使用并深刻理解AI工具的人将更有可能保住自己的职位。</p>
</li>
<li><p>回顾工业革命，技术进步曾一度导致大批劳动者失业，许多人因无法适应新变化而成为时代的“牺牲品”，收入锐减。</p>
</li>
<li><p>短期内，某些群体的失业可能难以避免，但历史也表明，技术变革往往伴随着新机会的诞生。</p>
</li>
<li><p>所以，个人需要做好准备：</p>
<ul>
<li>提升自身技能，特别是AI工具的使用和相关领域的知识储备。</li>
<li>即便不能马上参与高端AI开发，也可以从简单的AI相关工作入手，逐步积累经验。</li>
<li>有一定资产的群体，应学会投资，通过多样化方式应对收入变化。</li>
</ul>
</li>
</ul>
<h1 id="正面效应"><a href="#正面效应" class="headerlink" title="正面效应"></a>正面效应</h1><ul>
<li>催生新型岗位：AI技术完善后，将需要大量擅长操作和优化AI工具的人才。</li>
<li>解放低级劳动：AI让人类摆脱低级脑力劳动，从而获得更高工资和更有意义的工作体验。</li>
<li>效率提升与“去内卷”：生产效率的提升可能减少“内卷”现象，并通过政策调整减轻劳动者的工作压力。</li>
<li>关于工作和财富再分配问题：如果AI真的让生产效率大幅提升，我们可以期待未来的福利制度更加完善，比如减少工作时间、增加带薪假期，甚至实现“无工作”状态也未必是幻想。</li>
</ul>
<h1 id="社会与政策层面"><a href="#社会与政策层面" class="headerlink" title="社会与政策层面"></a>社会与政策层面</h1><ul>
<li>加强教育培训：推动技能提升和再教育计划，使劳动者适应AI时代的工作需求。</li>
<li>优化产业结构：加速产业转型升级，为社会创造更多高附加值就业机会。</li>
<li>完善社会保障：建立健全失业保障制度，帮助因AI技术变革失业的劳动者渡过难关。</li>
<li>政策引导：通过政策规范AI技术的合理应用，避免过度自动化对就业的过度冲击。</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>AI技术的快速发展是一把双刃剑，它既对传统就业模式提出了挑战，也为社会提供了提高效率、优化劳动分工的契机。从社会视角看，技术进步不可逆，但我们可以通过政策调整和个人适应来尽量减小负面影响。面对不可避免的变革，积极调整心态、提升能力，是每个人应对未来的重要策略。同时，历史告诉我们，每一次技术浪潮都会带来新的机遇，只要抓住变化的窗口期，未来依然充满希望。</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="http://www.xml-data.cn/KXYSH/html/93be89d5-c2ef-491d-9398-ebe61dcdc56b.htm">从工业革命史看技术变迁如何影响工人命运——《技术陷阱：自动化时代的资本、劳动力和权力》评介</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/12/30/20241230-web3-xiang-guan-zhi-shi-ji-lu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/12/30/20241230-web3-xiang-guan-zhi-shi-ji-lu/" itemprop="url">Web3相关知识记录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-12-30T21:31:14+08:00">
                2024-12-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<blockquote>
<p>以下内容都来源于书籍</p>
</blockquote>
</blockquote>
<p><img src="/2024/12/30/20241230-web3-xiang-guan-zhi-shi-ji-lu/Web3.png"></p>
<h2 id="Web进化史"><a href="#Web进化史" class="headerlink" title="Web进化史"></a>Web进化史</h2><ul>
<li><p>Web 1.0时代。Web 1.0时代的特点是重信息轻身份，多浏览少输出，因此也被人称为“只读网络”</p>
</li>
<li><p>相较于Web 1.0时代的内容创作者占少数，内容消费者占绝大多数的生态格局，Web 2.0更加以每个用户为中心，注重由用户生成内容(User Generated Content, UGC)，强调交互性。因此有人将Web 2.0称为“交互网络”。</p>
</li>
<li><p>O2O本质是连接，如果说Web 1.0开启了人与信息的连接，Web 2.0则增进了人与人的连接，电商模式拉近了人与商品的连接，O2O则连接了消费者与服务者。</p>
</li>
<li><p>Web 3.0：数据所有权革命</p>
</li>
<li><p>描述了Web 3.0应该具备的几个组成部分：</p>
<ul>
<li>一个加密的，去中心化的信息发布系统</li>
<li>一个基于身份的，但又是匿名的底层通信系统</li>
<li>一个用于取代中心节点信息验证功能的共识引擎</li>
<li>一个将上述三点结合在一起的用户交互系统</li>
</ul>
</li>
<li><p>目前更多的人在正式场合采用Web 3.0的表述方式，但是我们应该知道Web 3.0表述具有技术流派风格，而Web3表述则具有用户社区风格。</p>
</li>
<li><p>在Web 2.0系统中，个体只是网络中无数节点之一，必须依赖中心节点。中心节点制定规则，决定了其他节点的行为和生存。Web 3.0系统中，个体还是网络中无数节点之一，但每个节点都高度自治，且拥有自己的决策过程。</p>
</li>
</ul>
<h2 id="区块链"><a href="#区块链" class="headerlink" title="区块链"></a>区块链</h2><ul>
<li><p>区块链是一种去中心化的计算机网络，也是实现Web 3.0世界的诸多技术基础中最重要的一个。</p>
</li>
<li><p>通证(tokens)是Web 3.0能够更公平、更有效率的一个关键因素。通证的概念范围非常广</p>
</li>
<li><p>加密资产是通证的其中一类，通证并不一定加密，但在Web 3.0的语境里，通常都是指带有去中心化特性（即无法被中心化机构通过在发放通证后部署的代码篡改）的加密通证。</p>
</li>
<li><p>加密资产并不一定是加密货币，比如NFT与数字藏品</p>
</li>
<li><p>Web 3.0的底层网络架构是基于区块链的，而区块链的第一个应用是比特币。</p>
</li>
<li><p>用以下五个指标来评估一个Web 3.0项目的去中心化程度：</p>
<ul>
<li>(1)项目钱包的透明性　一个项目的钱包越是透明，越能让大家了解这个项目的参与者构成。如果无法公开透明地了解一个项目的参与者构成，就无法进行更深度地评估。</li>
<li>(2)初始钱包的通证占比　初始钱包通常是项目方和核心投资者的地址，如果持有过多的通证，就说明他们对项目的影响力极大，这种项目是极其中心化的。</li>
<li>(3)相关生态的去中心化　在Web 3.0的错综复杂的生态中，大部分项目都并非独立存在而是依附或者嵌套于其他项目的。这时哪怕一个项目在开发层面是去中心化的，其价值和功能也会很大程度上被其他项目影响。而后者若是一个中心化的项目，那前者本质上也依旧是一个中心化的项目。</li>
<li>(4)项目方的影响力　一部分项目方本身并没持有大比例的通证，但由于他们的声音对项目的开发社群有着极大的影响力。这种情况下这也是一个偏中心化的项目，但中心化程度要比少数人持有大比例通证的项目要低。</li>
<li>(5)流动性　哪怕比特币作为一个完全独立的项目，并且没有和任何其他项目有嵌套关系，在早期流动性较低的阶段依旧可以轻易受到几个重要参与者深度影响，甚至有可能因为在链上的节点过少或者大部分节点被垄断，导致被篡改项目数据。也就是说，在那些流动性较差的项目上，被大算力主体篡改资产所有权的可能都要大于流动性高的项目。</li>
</ul>
</li>
<li><p>首次代币发行(Initial Coin Offering, ICO)，对应着首次公开募股(Initial Public Offering, IPO)应运而生。</p>
</li>
<li><p>以太坊的逻辑就是把现实中的合同去中心化形成智能合约。举个最简单的例子，你去自动售货机买可乐，你付钱后可乐出来，这就是一种智能合约。作为一个自动化的、程序化的机器，你付了钱，它给了货。</p>
</li>
<li><p>智能合约的本质就是通过代码程序的方式执行现实社会中的一些底层的制度或者合同。只要条件达到了要求，程序就会执行，解决了谁先出钱，谁先给货的问题。同时，通过区块链的技术，能够让整个交易的过程、结果、代码执行的情况在全网进行记录，且不可篡改。</p>
</li>
<li><p>以太坊(Ethereum)就是区块链版的操作系统，是一个为去中心化应用(DApps)而生的全球开源平台。</p>
</li>
<li><p>从2013年发布白皮书至今，以太坊在智能合约领域一直处于领先地位。它是当前全球最知名、应用最广泛的区块链智能合约底层平台。以太坊建立了一个可编程的、图灵完备的区块链，在这个区块链上，可以通过简单的程序实现各类加密资产的生成，也可以通过编写程序对以太坊上流通的资产状态进行精确控制。至今全球已经有数千种基于以太坊的去中心化应用程序和加密资产。</p>
</li>
<li><p>目前，以太坊上的项目已经划分出众多赛道，针对不同类型的项目大致可以分为：去中心化金融(DeFi)、去中心化交易所(DEX)、游戏、NFT、基础设施等。这些项目会在后文详细介绍。</p>
</li>
<li><p>区块链性能的不可能三角：即去中心化、安全性与可扩展性必须舍弃其一</p>
</li>
<li><p>Ripple和EOS舍弃了去中心化，IOTA和NANO舍弃了安全性，而比特币和以太坊舍弃了可扩展性。因此，以太坊就不得不消耗大量用于相互验证的算力和能源。</p>
</li>
<li><p>区块链可扩展性的提升大致可以分为以下三种：Layer 0（L0跨链交互），Layer 1（L1链上扩容，对主链本身进行改造），Layer 2（L2链下扩容，将主链上部分工作转移至主链以外）。</p>
</li>
<li><p>对于以太坊来说，直接对公链本身进行改造对已有的用户等利益相关方影响较大，因此以太坊社区主要关注的是L2，而对于链上扩容方案则采取缓慢过渡的模式。而对于后发的新公链来说，探索直接建立更高可扩展性的公链模型，成为首选。</p>
</li>
<li><p>L2，主要是指在主链之外建立一个二层的交易处理平台，负责具体交易的处理。主链只负责存取结算及验证链下上传信息的有效性。L2的扩容方式主要有状态通道(State Channels)、等离子体(Plasma)和卷叠(Rollup)三种方式。</p>
</li>
<li><p>随着区块链的发展，在目前，已经进入了一个多链并存的市场架构，并逐渐形成了以以太坊为核心，其他公链众星拱月的局面。从2021年4月开始，以太坊的跨链活动急剧增加。</p>
</li>
<li><p>跨链桥是一种链与链连接的桥梁工具，允许将加密资产从一条链转移到另一条链。两条链可以有不同的协议、规则和治理模型，而桥提供了一种相互通信和兼容的方式来安全地在双方进行互操作。用户可以使用跨链桥，快速轻松地实现加密资产交易。</p>
</li>
</ul>
<h2 id="元宇宙"><a href="#元宇宙" class="headerlink" title="元宇宙"></a>元宇宙</h2><ul>
<li><p>Web 3.0、区块链、元宇宙这三个概念经常交织出现，让人们产生了一定的疑惑。<br>这三个概念中，区块链是最早出现的。2008年区块链正式诞生，它的出现比现在公认出现于2005年的Web 2.0没有晚太多。区块链是为了解决中心化服务器存在的问题而诞生的。</p>
</li>
<li><p>Web 2.0这十多年的发展一直沿用的是将用户抓过来、养肥了、再割韭菜的AARRR（Acquisition获客，Activation激活，Retention留存，Referral转推荐，Revenue变现）互联网平台运营模式。在这种模式下，用户就像是动物一样，从始至终都是以S-R（Stimulus刺激，React反应）的行为模式被对待。用户是平台的资产，而不是平台的主人。因此，Web 2.0的用户被称为流量。</p>
</li>
<li><p>区块链的到来为Web 2.0带来了新的生机，将Web 2.0升级成为Web 3.0。<br>对Web 2.0时代的企业而言，IPO是最终归宿，在IPO那荣光时刻来临之际，企业必须已经达到一个非常大的量级，成为行业龙头。所谓盈亏同源，Web 2.0的企业盈利与用户无关，IPO或中途并购、退出都是VC的战场，亏损当然也与用户无关。</p>
</li>
<li><p>Web 2.0企业的模式是管理团队做，用户用。Web 3.0时代的企业替代物——DAO的所有者是社区，社区成员的每一个人都是DAO的既得利益者。由于Web 3.0的IEO(Initial Exchange Offering)门槛比IPO低，不需要中心化审核，IEO在真正意义上完成了IPO最初想要达成的任务——获取更大的流动性。之所以叫作Initial Exchange Offering而非Initial Public Offering，是因为Web 3.0社区的通证在没有上交易所之前，就是公开的、去中心化的、通常也有广泛的持有人基础，而且也是可以通过区块链交易的。</p>
</li>
<li><p>Web 3.0概念的进化线索是从用户与互联网之间的关系去归纳的，从Web 1.0单向接受互联网，到Web 2.0可以与互联网交互读写，再到Web 3.0可以拥有互联网。</p>
</li>
<li><p>真正的元宇宙远不只是体验的升级。元宇宙是一个宏大的概念，要构建这样一个数字世界，只提升体验是不够的。如果只是将VR&#x2F;AR设备装在头上，这样的元宇宙不过是个大型游戏，还远不能成为“宇宙”。从深层次出发，用“有关联”来描述Web 3.0和元宇宙之间的关系，显然还不够，它们之间是相辅相成的。从技术角度来看，元宇宙是前端、展示层，Web 3.0是中后端、技术层，只有技术条件满足Web 3.0所需的去中心化的数据库，即分布式数据库，以及配套的加密技术和协议，才有可能实现元宇宙这样宏大的设想。</p>
</li>
<li><p>NFT——NFT可以将现实世界与元宇宙的每一个物品一一对应。</p>
</li>
<li><p>DAO——元宇宙既是一场技术革命，也是一场社会革命，而DAO作为社区的操作系统，将是元宇宙的关键。DAO提供了一个技术层，提供个人之间的全球性和去中心化的协作，而无须信任第三方或少数代表。</p>
</li>
<li><p>之所以字节跳动等大厂和互联网知名人士都认为元宇宙是一个超前的概念，主要是因为其本身就建立在多重计算机技术与网络服务之上，是一套极其复杂的生态技术系统，在短时间内并不可能实现，而目前最接近元宇宙的则是Web 3.0。</p>
</li>
<li><p>区块链是支撑Web 3.0发展的底层技术，而Web 3.0的去中心化和确权特性是元宇宙的基础建设。Web 3.0的实现必然早于真正元宇宙的实现，这是元宇宙之所以能成为宇宙的前提条件。</p>
</li>
</ul>
<h2 id="DApps"><a href="#DApps" class="headerlink" title="DApps"></a>DApps</h2><ul>
<li><p>DApps是建立在区块链上的应用，全称Decentralized Applications。区块链有五层架构（也有一种划分方式，将协议层从应用层中独立出来，分为六层架构），DApps属于其中的最上层——应用层</p>
</li>
<li><p>Web 3.0是构建在区块链上的数字世界，而DApps就是用来构建这个世界中我们看得见摸得着的那部分的“砖瓦”。</p>
</li>
<li><p>要成为去中心化应用，需要满足以下标准：</p>
<ul>
<li>(1)必须完全开源，自动运行，且没有一个实体控制着它的大多数通证。它的数据和记录必须加密后存储在一个公链上。</li>
<li>(2)必须依据一个标准化算法或一系列准则来生产通证，并且应该在开始运营的时候就将它的部分或全部通证发放给使用者。这个应用必须依赖该通证来运行，而且使用者所作出的贡献应该以该通证作为奖励。</li>
<li>(3)可以因时制宜地更改它的运行法则，但是这些改变应该被大多数用户所认可，而不是将最终解释权归于某个实体。</li>
</ul>
</li>
<li><p>区块链的基础架构可以分为五层，包括硬件层，数据层、网络层、共识层、应用层（有些说法还会从应用层中拆分出协议层）。    </p>
</li>
<li><p>DApps大多数属于应用层。作为一类相对特殊的DApps，基础设施类DApps的作用是为DApps世界与其他四层的交互建立连接。因此基础设施类的DApps通常以“协议”的形式存在</p>
</li>
<li><p>◆ 区块链是由一个点对点(Peer-to-Peer)的计算机网络共同进行运算，验证和记录交易的。硬件层(Hardware Layer)就是这些计算机。<br>● 数据层(Data Layer)我们可以理解成数据库，主要可实现两大功能：数据存储、账户和交易的安全。<br>● 网络层(Network Layer)实现三大功能：节点间组网，数据传播，数据验证。<br>● 共识层(Consensus Layer)主要通过共识算法和共识机制实现一个重要功能：节点间的计算达成共识。由于区块链是分布式网络，每个节点均可计算，所以需要共识层做个统筹，让所有节点针对区块数据的有效性达成共识。<br>● 应用层(Application Layer)是最上面一层，有些说法还会在其中将协议层(Protocol Layer)单独分为一层。原因是DApps有“胖协议，瘦应用”的说法。这是相对于Web 2.0 Apps的“瘦协议，胖应用”提出的。所谓“胖协议”，举个例子，在Web 2.0中，数据的安全依靠数据安全公司、杀毒软件、防火墙等应用实现，协议层只负责数据传输；而在Web 3.0中，传输协议本身就保障了数据安全，因此各种数据安全应用、杀毒软件、防火墙应用没有了独立存在的空间。智能合约就是这样的一种协议。</p>
</li>
<li><p>交易所<br>针对加密资产的交易所既有中心化的也有去中心化的。中心化交易所目前依然占据着绝对的市场地位，因为他们发展较早，对从Web 2.0过渡进入Web 3.0的用户而言使用体验也比较好。在交易所中交易的加密资产主要为加密货币和NFT。去中心化加密货币交易所我们放在第六章详细介绍，NFT交易所我们放在第七章详细介绍。</p>
</li>
</ul>
<h2 id="DAO"><a href="#DAO" class="headerlink" title="DAO"></a>DAO</h2><ul>
<li><p>DAO是一种共同管理加密资产以达成共同目标的组织方式。我们可以将DAO视作由成员集体所有和共同管理的Web 3.0版本的新型企业。</p>
</li>
<li><p>DAO的实现基于智能合约。因此，在某些语境下，DAO也可以指代支撑DAO运行的智能合约本身。合约界定了组织的规则，管理组织的资金。以太坊是第一个使人们能够建立DAO的区块链。大多数DAO都在以太坊上，但也有其他网络能建立DAO，如Polkadot、Cosmos、EOS和Cardano。</p>
</li>
<li><p>DID和传统ID的差异在于，DID基于区块链，而链上数据是公开透明可查并难以篡改的。因此当你在Web 3.0亮出DID，人们并不需要借助某个权威机构，就可以核验这个DID的真伪以及能否准入。</p>
</li>
<li><p>Uniswap是一个以协议形式存在的去中心化交易所，而它的治理是以DAO的形式运作的。</p>
</li>
<li><p>教你建立一个DAO<br>了解了这么多关于DAO的概念，是不是也想自己动动手？参与或自己建立一个DAO是亲身体验Web 3.0的门槛最低的方式。建立一个DAO几乎和建立一个微信群一样简单。这一节笔者将借助一个名为Aragon的工具，手把手教读者建立一个DAO。<br>第一步：打开<a target="_blank" rel="noopener" href="https://aragon.org/">https://aragon.org/</a></p>
</li>
</ul>
<h2 id="DeFi"><a href="#DeFi" class="headerlink" title="DeFi"></a>DeFi</h2><ul>
<li>DeFi，全称Decentralized Finance，去中心化金融。</li>
<li>去中心化交易所是一类基于区块链的交易所，它不需要将用户资金和个人数据转入交易所内，而只是作为一种基础设施来匹配希望买卖加密资产的买家和卖家。在匹配引擎的帮助下，这种交易直接发生在参与者之间。<br>中心化交易所(CEX)，是集传统交易所、券商和投资银行的功能为一体的平台，以币安、Coinbase、FTX交易所为代表，CEX聚集了庞大的用户量和交易量，也带来了足够的交易深度，提供了充分的资产流动性。</li>
<li>交易所的核心环节一般包括充提、下单、订单撮合、资金结算和提现。CEX均由交易平台本身撮合完成；DEX则是把上述所有环节都置于链上，由智能合约执行全部操作，这样用户的交易过程就无须任何第三方。</li>
</ul>
<h2 id="NFT"><a href="#NFT" class="headerlink" title="NFT"></a>NFT</h2><ul>
<li><p>现实世界与数字世界的差异在于，现实世界不存在完全相同的两个物品，即使是同一个工厂同一批次的商品也会有可能出现不同。而数字世界则可以完全复制同一物品。</p>
</li>
<li><p>我们如果要实现元宇宙的最终目标，就需要把现实世界物品的这个特点也搬到数字世界里。区块链能帮我们做到这一点，这就是非同质化通证(Non-Fungible Token, NFT)。</p>
</li>
<li><p>同质化通证(Fungible Token, FT)和非同质化通证之间的区别</p>
</li>
<li><p>BTC、ETH等加密资产多数是同质化通证。顾名思义，同质化通证是可以与同一事物的另一个单位互换的。例如，一单位BTC等于另一单位BTC，就像一张100美元的价值等于另一张100美元。美元可以进行简单互换，即使序号不同也不影响，对持有者来说没有区别。同质化通证是一种能够互换、具有统一性的通证。而且由于它以数字的形式存在，还可以拆分成近乎无穷小的许多份，每一份也都可以代表相应的价值。<br>与同质化物品不同，非同质化物品或通证彼此之间是不能互换的，它们具有独特的属性。即使看起来相似，但彼此之间也有根本的不同。<br>非同质化通证包含了记录在其智能合约中的识别信息。这些信息使每个通证具有唯一性，因此不能被另一种通证直接替代，没有两个NFT是相同的。此外，绝大多数非同质化通证也不可分割。</p>
</li>
<li><p>每个NFT都有区别于其他NFT的数字哈希值。因此，NFT可以作为出处证明。就像现实世界中的证书一样，它不仅能证明原创艺术品和游戏通证等知识产权的所有权和真实性，还能代表股票、房地产等实际资产。现实世界中，真实资产拥有所有权证书。同样，在区块链世界中，NFT也可以作为所有权记录和真实性证明。在艺术品领域，NFT也被称为数字藏品。</p>
</li>
<li><p>NFT不仅存在于数字世界，它们也可以代表任何类型的物理资产。NFT可以与物理世界中存在的任何东西相连接，形成一种“数字孪生”，并在数字世界的市场上实现实物资产的所有权交易。</p>
</li>
<li><p>NFT目前最为人推崇的应用场景是创作者经济。</p>
</li>
<li><p>NFT的3种存储方式是区块链存储、IPFS分布式存储节点存储、中心化服务器存储，安全性依次降低。安全性越高则越符合不可篡改的特性，也越有独特性。</p>
</li>
<li><p>万物皆可NFT。这里的NFT指的不仅是艺术品。这个理念不是说万物都可以成为艺术收藏品，而是有着更远大的目标。我们可以将世间万物以数字孪生的方式在数字世界重建一遍。所有的一切都可能以NFT的形式存在，也可以以NFT的产生方式创造出来。</p>
</li>
</ul>
<h2 id="通证经济学"><a href="#通证经济学" class="headerlink" title="通证经济学"></a>通证经济学</h2><ul>
<li>通证是Web 3.0的核心，身份是通证，NFT是通证，交易媒介也是通证。通证是激励用户参与的奖励，是驱动Web 3.0运行的燃油，也是连接Web 3.0世界各个岛屿的桥梁。通证的发放、分配与流通使用机制是一个Web 3.0项目能否成功的关键因素。</li>
<li>通证经济（Token Economics，也称Tokenomics）。就像一个普通的经济体系，通证经济逃不过供给和需求的掌控，供不应求则价格上涨，供过于求则价格下跌。</li>
</ul>
<h2 id="Web-3-0现存的问题"><a href="#Web-3-0现存的问题" class="headerlink" title="Web 3.0现存的问题"></a>Web 3.0现存的问题</h2><ul>
<li><p>在失去中心化监管的环境下，加密与匿名也暴露了许多问题。通过加密匿名算法获取的财富，也会被加密匿名地剥夺</p>
</li>
<li><p>人类的共识是一个很有意思的东西，它可以如磐石般固执，也可以如鲁珀特之泪的尾巴一样脆弱。当形成共识的那个关键点破灭，建立在共识之上的摩天大楼，就会瞬间崩塌。</p>
</li>
<li><p>创作者经济还是炒作者经济</p>
</li>
<li><p>Web 3.0的底层是去中心化网络，数据存储和运算是由各个分散的节点完成。那么很自然的，不考虑中心化作业情况下的网络其他参与者在不能达成一致而导致扯皮的情况，对于同样一件事情的处理，协同作业的速度就是比中心化作业要慢、效率更低、消耗能源更多。</p>
</li>
<li><p>比特币交易为了替换掉中心化权威而采用了耗能极大的工作证明，让人们消费了能源计算出一个不产生任何价值的问题，这是对权威不信任的代价。具体而言，每一笔比特币交易要消耗2188千瓦时的电量，而Visa是每10万笔交易消耗148千瓦时电量，这代价就是效率下降150万倍。以太坊的耗电量是比特币的1&#x2F;10，相对Visa也有15万倍的差距。Web 3.0的底层架构要想达到Web 2.0的用户体验，只有两条路可以走，其一是降低运算的耗能，其二是调整共识机制。</p>
</li>
<li><p>互联网不是法外之地，Web 3.0也不是。网络无国界，但法律是有国界的。目前各国针对Web 3.0的监管法规还在逐步出台中，而Web 3.0未来的发展必然是需要在各国的法律框架下运行的。</p>
</li>
<li><p>在Web 2.0时代，监管主体是服务提供商，监管属地是服务器所在地。而在Web 3.0时代，服务代码存放在网络的各个服务器上，也没有一个具体的服务提供商。Web 3.0的互联网活动趋于分布式，这给监管主体和监管属地的确定带来了一定的挑战。<br>另外，由于Web 3.0应用和钱包的匿名性和去中心化，一旦资产丢失，将无法找回。</p>
</li>
<li><p>Web 3.0项目的代码一经上线，即无法改动。这就使得在上线前对于代码的审计显得尤为重要。但凡是代码，或多或少都是有漏洞的。上线无法改动的代码犹如一辆在公路上狂奔而没有方向盘的汽车，行驶越久危险系数越高。</p>
</li>
<li><p>政府监管和主导从来都是一柄双刃剑，会对技术发展和应用创新造成一定阻碍，但从稳定社会经济、维护金融秩序角度看，合规是一个新兴市场的生存基础。</p>
</li>
<li><p>Web 3.0已经被赋予了太多神秘色彩，人们对它寄予了太多不切实际的希望。而只有当我们意识到，区块链发明者的“去中心化”理想主义，注定会被纷至沓来的投机者瓦解得支离破碎的时候，我们才可以平心静气地接受一个基于现实主义的Web 3.0的未来——在一个由核心节点掌握的“中心化”区块链网络上，用“去中心化”的分布式账本和加密散列，保障不同的经济主体之间的数字产权和商业价值不受侵犯，并形成它们互相的契约关系——这也理应是我国的政府、企业和社会组织在Web 3.0浪潮中扮演的角色。！！！！</p>
</li>
<li><p>腾讯、阿里巴巴（蚂蚁集团）、百度和京东等都构建了自己的联盟链，从内容版权、股权、保险、债券、供应链金融、税务、司法、商品防伪溯源、物流运输和生态保护等方面提供了“上链”服务。BSN与长安链等国有企业、智库和政府机构成立的联盟链也陆续建立，除了用于商业和政务场景，还致力解决区块链底层公用基础设施和知识产权的自主可控问题。</p>
</li>
</ul>
<h2 id="名词"><a href="#名词" class="headerlink" title="名词"></a>名词</h2><ul>
<li>DApps（Decentralized Applications，去中心化应用程序）</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>《Web3.0漫游指南》</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/" itemprop="url">LLM相关技术简单了解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-12-26T18:48:22+08:00">
                2024-12-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<blockquote>
<p>以下内容基本都是从书籍中摘抄，用于个人记录<br>从AI得出的回答不一定正确或者只是现阶段暂时正确<br>增加部分个人理解</p>
</blockquote>
</blockquote>
<p><img src="/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/LLM_position.png"><br><img src="/2024/12/26/20241226-llm-xiang-guan-ji-zhu-jian-dan-liao-jie/LLM.png"></p>
<h2 id="LLM（Large-Language-Model）"><a href="#LLM（Large-Language-Model）" class="headerlink" title="LLM（Large Language Model）"></a>LLM（Large Language Model）</h2><ul>
<li><p>大语言模型的涌现能力被非形式化定义为“在小型模型中不存在但在大模型中出现的能力”，具体是指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。类比而言，这种性能涌现模式与物理学中的相变现象有一定程度的相似，但是仍然缺乏相应的理论解释以及理论证实，甚至有些研究工作对于涌现能力是否存在提出质疑 [38]。整体来说，涌现能力的提出有助于使得公众认识到大语言模型所具有的能力优势，能够帮助区分大语言模型与传统预训练语言模型之间的差异。</p>
</li>
<li><p>大语言模型的三种典型涌现能力。</p>
<ol>
<li>上下文学习（In-context Learning, ICL）. 上下文学习能力在 GPT-3 的论文中 [23] 被正式提出。具体方式为，在提示中为语言模型提供自然语言指令和多个任务示例（Demonstration），无需显式的训练或梯度更新，仅输入文本的单词序列就能为测试样本生成预期的输出。</li>
<li>指令遵循（Instruction Following）. 指令遵循能力是指大语言模型能够按照自然语言指令来执行对应的任务 [28, 39, 40]。为了获得这一能力，通常需要使用自然语言描述的多任务示例数据集进行微调，称为指令微调（Instruction Tuning）或监督微调（Supervised Fine-tuning）。通过指令微调，大语言模型可以在没有使用显式示例的情况下按照任务指令完成新任务，有效提升了模型的泛化能力。相比于上下文学习能力，指令遵循能力整体上更容易获得，但是最终的任务执行效果还取决于模型性能和任务难度决定。</li>
<li>逐步推理（Step-by-step Reasoning）. 对于小型语言模型而言，通常很难解决 涉及多个推理步骤的复杂任务（如数学应用题），而大语言模型则可以利用思维链（Chain-of-Thought, CoT）提示策略 [25] 来加强推理性能。具体来说，大语言模型 可以在提示中引入任务相关的中间推理步骤来加强复杂任务的求解，从而获得更 为可靠的答案。</li>
</ol>
</li>
<li><p>通俗来讲，扩展法则与涌现能力之间微妙的关系可以类比人类的学习能力来解释。以语言能力为例，对于儿童来说，语言发展（尤其是婴儿）可以被看作一个多阶段的发展过程，其中也会出现“涌现现象”。在这一发展过程中，语言能力在一个阶段内部相对稳定，但是当进入另一个能力阶段时可能会出现重要的提升（例如从说简单的单词到说简单的句子）。尽管儿童实际上每天都在成长，但是语言的提升过程本质上是不平滑和不稳定的（即语言能力在时间上不以恒定速率发展）。因此，经常可以看到年轻的父母会对宝宝所展现出的语言能力进展感到惊讶。    </p>
</li>
<li><p>这种大模型具有但小模型不具有的能力通常被称为“涌现能力”（Emergent Abilities）。为了区别这一能力上的差异，学术界将这些大型预训练语言模型命名为“大语言模型”</p>
</li>
<li><p>早期的语言模型主要面向自然语言的建模和生成任务，而最新的语言模型（如 GPT-4）则侧重于复杂任务的求解。从语言建模到任务求解，这是人工智能科学思维的一次重要跃升，是理解语言模型前沿进展的关键所在。</p>
</li>
<li><p>早期的统计语言模型主要被用于（或辅助用于）解决一些特定任务，主要以信息检索、文本分类、语音识别等传统任务为主。随后，神经语言模型专注于学习任务无关的语义表征，旨在减少人类特征工程的工作量，可以大范围扩展语言模型可应用的任务。</p>
</li>
<li><p>进一步，预训练语言模型加强了语义表征的上下文感知能力，并且可以通过下游任务进行微调，能够有效提升下游任务（主要局限于自然语言处理任务）的性能。随着模型参数、训练数据、计算算力的大规模扩展，最新一代大语言模型的任务求解能力有了显著提升，能够不再依靠下游任务数据的微调进行通用任务的求解。</p>
</li>
<li><p>大语言模型的能力特点：具有较好的复杂任务推理能力. 除了具有通用性外，大语言模型在复杂任务中还展现出了较好的推理能力。</p>
</li>
<li><p>大语言模型对科技发展的影响</p>
<ul>
<li>【理论基础原理】尽管大语言模型技术已经取得了显著进展，但是对于它的基本原理仍然缺乏深入的探索，很多方面还存在局限性或者提升空间。首先，大模型中某些重要能力（如上下文学习能力）的涌现仍然缺乏形式化的理论解释，需要针对大语言模型基础能力的形成原因进行深入研究，从而揭示大语言模型内部的工作机理。</li>
<li>【细节公开和算力支持】其次，大语言模型预训练需要大规模的计算资源支持，研究各种训练策略的效果并进行可重复性的消融实验的成本非常高昂。学术界难以获得充分的算力来系统性研究大语言模型；虽然工业界或者大型研究机构不断推出性能优异的开源大模型，但是这些模型的训练过程的开源程度还不够充分，许多重要的训练细节仍缺乏公开的研究报道。</li>
<li>【人类对齐】第三，让大语言模型充分与人类价值观或偏好对齐也是一项重要的科研挑战。尽管大语言模型已经具有较好的模型能力，但是在特定场景下或者蓄意诱导下，仍然可能生成虚构、有害或具有负面影响的内容。这一问题随着模型能力的提升而变得更为难于解决。为了应对模型能力未来可能超越人类监管能力的情况，需要设计更为有效的监管方法来消除使用大语言模型的潜在风险。</li>
<li>随着大语言模型技术的迅猛发展，人工智能相关研究领域正发生着重要的技术变革<ol>
<li>自然语言处理. 在自然语言处理领域，大语言模型可以作为一种通用的语言任务解决技术，能够通过特定的提示方式解决不同类型的任务，并且能够取得较为领先的效果。</li>
<li>信息检索. 在信息检索领域，传统搜索引擎受到了人工智能信息助手（即ChatGPT）这一新型信息获取方式的冲击。</li>
<li>计算机视觉. 在计算机视觉领域，研究人员为了更好地解决跨模态或多模态任务，正着力研发类 ChatGPT 的视觉-语言联合对话模型，GPT-4 已经能够支持图文多模态信息的输入。由于开源大语言模型的出现，可以极大地简化多模态模型的实现难度，通过将图像、视频等模态的信息与文本语义空间相融合，可以通过计算量相对较少的微调方法来研发多模态大语言模型。进一步，基于下一个词元预测的思路也可能会带来多模态领域的基础模型架构的转变，例如 OpenAI 最新推出的 Sora 模型就是基于图像块序列建模的思路进行构建的。</li>
<li>人工智能赋能的科学研究（AI4Science）. 近年来，AI4Science 受到了学术界的广泛关注，目前大语言模型技术已经广泛应用于数学、化学、物理、生物等多个领域，基于其强大的模型能力赋能科学研究。</li>
</ol>
</li>
</ul>
</li>
<li><p>目前广泛采用的对齐方式是基于人类反馈的强化学习技术，通过强化学习使得模型进行正确行为的加强以及错误行为的规避，进而建立较好的人类对齐能力。</p>
</li>
<li><p>在实践应用中，需要保证大语言模型能够较好地符合人类的价值观。目前，比较具有代表性的对齐标准是“3 H 对齐标准”，即 Helpfulness（有用性）、Honesty（诚实性）和 Harmlessness（无害性）。</p>
</li>
<li><p>实际上，世界上最会使用工具的智能体就是人类，人类不断发明新的技术与工具，拓展自己的认知与能力边界。</p>
</li>
</ul>
<h2 id="GPT-模型简史"><a href="#GPT-模型简史" class="headerlink" title="GPT 模型简史"></a>GPT 模型简史</h2><ul>
<li><p>GPT 系列模型的基本原理是训练模型学习恢复预训练文本数据，将广泛的世界知识压缩到仅包含解码器（Decoder-Only）的 Transformer 模型中，从而使模型能够学习获得较为全面的能力。其中，两个关键要素是：（I）训练能够准确预测下一个词的 Transformer （只包含解码器）语言模型；（II）扩展语言模型的规模以及扩展预训练数据的规模。</p>
</li>
<li><p>当谷歌 2017 年推出基于注意力机制的 Transformer 模型后，OpenAI 团队迅速洞察到了其潜在的优越性，认为这种模型可能是一种大规模可扩展训练的理想架构。基于此，OpenAI 团队开始构建GPT 系列模型，并于 2018 年推出了第一代 GPT 模型—GPT-1，能够通过“通用文本训练-特定任务微调”的范式去解决下游任务。接下来，GPT-2 和 GPT-3 模型通过扩大预训练数据和模型参数规模，显著提升了模型性能，并且确立了基于自然语言形式的通用任务解决路径。在 GPT-3 的基础上，OpenAI 又通过代码训练、人类对齐、工具使用等技术对于模型性能不断升级，推出了功能强大的 GPT-3.5 系列模型。2022 年 11 月，ChatGPT 正式上线，能够以对话形式解决多种任务，使得用户能够通过网络 API 体验到语言模型的强大功能。2023 年 3 月，OpenAI 推出了标志性的 GPT-4 模型，将模型能力提升至全新高度，并将其扩展至拥有多模态功能的 GPT-4V 模型。</p>
</li>
<li><p>反观 GPT 系列模型的发展历程，有两点令人印象深刻。第一点是可拓展的训练架构与学习范式：Transformer 架构能够拓展到百亿、千亿甚至万亿参数规模，并且将预训练任务统一为预测下一个词这一通用学习范式；第二点是对于数据质量与数据规模的重视：不同于 BERT 时代的预训练语言模型，这次大语言模型的成功与数据有着更为紧密的关系，高质量数据、超大规模数据成为大语言模型的关键基础。</p>
</li>
<li><p>在 GPT-1 出现之前，构建高性能 NLP 神经网络的常用方法是利用监督学习。这种学习技术使用大量的手动标记数据。</p>
</li>
</ul>
<h3 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT-1"></a>GPT-1</h3><ul>
<li>GPT-1 的作者提出了一种新的学习过程，其中引入了无监督的预训练步骤。这个预训练步骤不需要标记数据。相反，他们训练模型来预测下一个标记。</li>
<li>由于采用了可以并行化的 Transformer 架构，预训练步骤是在大量数据上进行的。</li>
<li>GPT-1 是小模型，它无法在不经过微调的情况下执行复杂任务。因此，人们将微调作为第二个监督学习步骤，让模型在一小部分手动标记的数据上进行微调，从而适应特定的目标任务。比如，在情感分析等分类任务中，可能需要在一小部分手动标记的文本示例上重新训练模型，以使其达到不错的准确度。这个过程使模型在初始的预训练阶段习得的参数得到修改，从而更好地适应具体的任务。</li>
<li>尽管规模相对较小，但 GPT-1 在仅用少量手动标记的数据进行微调后，能够出色地完成多个 NLP 任务。GPT-1 的架构包括一个解码器（与原始 Transformer 架构中的解码器类似），具有 1.17 亿个参数。作为首个 GPT 模型，它为更强大的模型铺平了道路。后续的 GPT 模型使用更大的数据集和更多的参数，更好地利用了 Transformer 架构的潜力。</li>
</ul>
<h3 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h3><ul>
<li>2019 年初，OpenAI 提出了 GPT-2。这是 GPT-1 的一个扩展版本，其参数量和训练数据集的规模大约是 GPT-1 的 10 倍。这个新版本的参数量为 15 亿，训练文本为 40 GB。2019 年 11 月，OpenAI 发布了完整版的 GPT-2 模型。</li>
<li>GPT-2 是公开可用的，可以从 Hugging Face 或 GitHub 下载。</li>
<li>GPT-2 表明，使用更大的数据集训练更大的语言模型可以提高语言模型的任务处理能力，并使其在许多任务中超越已有模型。它还表明，更大的语言模型能够更好地处理自然语言。</li>
</ul>
<h3 id="从-GPT-3-到-InstructGPT"><a href="#从-GPT-3-到-InstructGPT" class="headerlink" title="从 GPT-3 到 InstructGPT"></a>从 GPT-3 到 InstructGPT</h3><ul>
<li>2020 年 6 月，OpenAI 发布了 GPT-3。GPT-2 和 GPT-3 之间的主要区别在于模型的大小和用于训练的数据量。</li>
<li>2021 年，OpenAI 发布了 GPT-3 模型的新版本，并取名为 InstructGPT。与原始的 GPT-3 基础模型不同，InstructGPT 模型通过强化学习和人类反馈进行优化。</li>
<li>从 GPT-3 模型到 InstructGPT 模型的训练过程主要有两个阶段：监督微调（supervised fine-tuning，SFT）和通过人类反馈进行强化学习 （reinforcement learning from human feedback，RLHF）。每个阶段都会针对前一阶段的结果进行微调。也就是说，SFT 阶段接收 GPT-3 模型并返回一个新模型。RLHF 阶段接收该模型并返回 InstructGPT 版本。</li>
<li>与基础的 GPT-3 模型相比，InstructGPT 模型能够针对用户的提问生成更准确的内容。OpenAI 建议使用 InstructGPT 模型，而非原始版本。</li>
</ul>
<h3 id="GPT-3-5、Codex-和-ChatGPT"><a href="#GPT-3-5、Codex-和-ChatGPT" class="headerlink" title="GPT-3.5、Codex 和 ChatGPT"></a>GPT-3.5、Codex 和 ChatGPT</h3><ul>
<li>ChatGPT 是由 LLM 驱动的应用程序，而不是真正的LLM。ChatGPT 背后的 LLM 是 GPT-3.5 Turbo。</li>
</ul>
<h3 id="GPT-4"><a href="#GPT-4" class="headerlink" title="GPT-4"></a>GPT-4</h3><ul>
<li>与 OpenAI GPT 家族中的其他模型不同，GPT-4 是第一个能够同时接收文本和图像的多模态模型。这意味着 GPT-4 在生成输出句子时会考虑图像和文本的上下文。这样一来，用户就可以将图像添加到提示词中并对其提问。</li>
</ul>
<h3 id="使用插件和微调优化-GPT-模型"><a href="#使用插件和微调优化-GPT-模型" class="headerlink" title="使用插件和微调优化 GPT 模型"></a>使用插件和微调优化 GPT 模型</h3><ul>
<li>OpenAI 提供的插件服务允许该模型与可能由第三方开发的应用程序连接。这些插件使模型能够与开发人员定义的应用程序接口（application program interface，API）进行交互。这个过程可以极大地增强 GPT 模型的能力，因为它们可以通过各种操作访问外部世界。</li>
<li>想象一下，将来每家公司都可能希望拥有自己的 LLM 插件。就像我们今天在智能手机应用商店中看到的那样，可能会有一系列的插件集合。通过插件可以添加的应用程序数量可能是巨大的。</li>
<li>在其网站上，OpenAI 表示可以通过插件让 ChatGPT 执行以下操作：<br>检索实时信息，如体育赛事比分、股票价格、最新资讯等；检索基于知识的信息，如公司文档、个人笔记等； 代表用户执行操作，如预订航班、订购食品等； 准确地执行数学运算。</li>
</ul>
<h3 id="GPT-4-和ChatGPT-的-API"><a href="#GPT-4-和ChatGPT-的-API" class="headerlink" title="GPT-4 和ChatGPT 的 API"></a>GPT-4 和ChatGPT 的 API</h3><ul>
<li>OpenAI Playground 是一个基于 Web 的平台。你可以使用它直接测试 OpenAI 提供的语言模型，而无须编写代码。在 OpenAI Playground 上，你可以编写提示词，选择模型，并轻松查看模型生成的输出。要测试 OpenAI 提供的各种 LLM 在特定任务上的表现，OpenAI Playground 是绝佳的途径。</li>
</ul>
<h2 id="开源LLM"><a href="#开源LLM" class="headerlink" title="开源LLM"></a>开源LLM</h2><h3 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h3><ul>
<li><p>由于对公众开放了模型权重且性能优秀，LLaMA 已经成为了最受欢迎的开源大语言模型之一，许多研究工作都是以其为基座模型进行微调或继续预训练，衍生出了众多变体模型</p>
</li>
<li><p>中文指令. 原始的 LLaMA 模型的训练语料主要以英语为主，在中文任务上的表现比较一般。为了使 LLaMA 模型能够有效地支持中文，研究人员通常会选择扩展原始词汇表，在中文数据上进行继续预训练，并用中文指令数据对其进行微调。经过中文数据的训练，这些扩展模型不仅能更好地处理中文任务，在跨语言处理任务中也展现出了强大的潜力。目前常见的中文大语言模型有 Chinese LLaMA、Panda、Open-Chinese-LLaMA、Chinese Alpaca、YuLan-Chat 等。</p>
</li>
<li><p>垂域指令. LLaMA 虽然展现出了强大的通用基座模型能力，但是在特定的垂直领域（例如医学、教育、法律、数学等）的表现仍然较为局限。为了增强 LLaMA模型的垂域能力，很多工作基于搜集到的垂域相关的指令数据，或者采用垂域知识库以及相关专业文献等借助强大的闭源模型 API（例如 GPT-3.5、GPT-4 等）构建多轮对话数据，并使用这些指令数据对 LLaMA 进行指令微调。常见的垂域 LLaMA模型有 BenTsao（医学）、LAWGPT（法律）、TaoLi（教育）、Goat（数学）、Comucopia （金融）等。</p>
</li>
<li><p>多模态指令. 由于 LLaMA 模型作为纯语言模型的强大能力，许多的多模态模型都将其（或将其衍生模型）作为基础语言模型，搭配视觉模态的编码器，使用多模态指令对齐视觉表征与文本。与其他语言模型相比，Vicuna 在多模态语言模型中受到了更多的关注，由此形成了一系列基于 Vicuna 的多模态模型，包括LLaVA 、MiniGPT4 、InstructBLIP 和 PandaGPT</p>
</li>
<li><p>目前性能最强大的模型仍然主要以闭源为主。这些闭源模型通过 API（应用程序接口）形式进行调用，无需在本地运行模型即可使用。在闭源大语言模型领域，OpenAI 无疑是最具代表性和影响力的公司</p>
</li>
</ul>
<h2 id="LLM训练过程"><a href="#LLM训练过程" class="headerlink" title="LLM训练过程"></a>LLM训练过程</h2><ul>
<li>从机器学习的观点来说，神经网络是一种具有特定模型结构的函数形式，而大语言模型则是一种基于 Transformer 结构的神经网络模型。因此，可以将大语言模型看作一种拥有大规模参数的函数，它的构建过程就是使用训练数据对于模型参数的拟合过程。</li>
<li>尽管所采用的训练方法与传统的机器学习模型（如多元线性回归模型的训练）可能存在不同，但是本质上都是在做模型参数的优化。大语言模型的优化目标更加泛化，不仅仅是为了解决某一种或者某一类特定任务，而是希望能够作为通用任务的求解器</li>
<li>一般来说，这个训练过程可以分为大规模预训练和指令微调与人类对齐两个阶段，一般来说，预训练是指使用与下游任务无关的大规模数据进行模型参数的初始训练，可以认为是为模型参数找到一个较好的“初值点”。</li>
<li>目前来说，比较广泛使用的微调技术是“指令微调”（也叫做有监督微调，Supervised Fine-tuning, SFT），通过使用任务输入与输出的配对数据进行模型训练，可以使得语言模型较好地掌握通过问答形式进行任务求解的能力。这种模仿示例数据进行学习的过程本质属于机器学习中的模仿学习（Imitation Learning）。</li>
<li>如何将语言模型 进行人类对齐。具体来说，主要引入了基于人类反馈的强化学习对齐方法 RLHF（Reinforcement Learning from Human Feedback），在指令微调后使用强化学习加 强模型的对齐能力。在 RLHF 算法中，需要训练一个符合人类价值观的奖励模型（Reward Model）。为此，需要标注人员针对大语言模型所生成的多条输出进行偏 好排序，并使用偏好数据训练奖励模型，用于判断模型的输出质量。</li>
</ul>
<h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><ul>
<li>常用的预训练数据集：目前常用于训练大语言模型的代表性数据集合。根据其内容类型进行分类，这些语料库可以划分为：网页、书籍、维基百科、代码以及混合型数据集。</li>
<li>数据准备:根据来源不同，预训练数据主要分为两种类型：通用文本数据和专用文本数据。</li>
<li>数据预处理</li>
</ul>
<h3 id="指令微调与人类对齐"><a href="#指令微调与人类对齐" class="headerlink" title="指令微调与人类对齐"></a>指令微调与人类对齐</h3><ul>
<li>为了增强模型的任务解决能力，大语言模型在预训练之后需要进行适应性微调，通常涉及两个主要步骤，即指令微调（有监督微调）和对齐微调。</li>
</ul>
<h3 id="指令微调（Instruction-Tuning）"><a href="#指令微调（Instruction-Tuning）" class="headerlink" title="指令微调（Instruction Tuning）"></a>指令微调（Instruction Tuning）</h3><ul>
<li><p>指令微调（Instruction Tuning）是指使用自然语言形式的数据对预训练后的大语言模型进行参数微调，这一术语由谷歌研究员在 2022 年的一篇 ICLR 论文中正式提出 [39]。在另外一些参考文献中，指令微调也被称为有监督微调（Supervised Fine-tuning）[28] 或多任务提示训练（Multitask Prompted Training）[40]。指令微调过程需要首先收集或构建指令化的实例，然后通过有监督的方式对大语言模型的参数进行微调。经过指令微调后，大语言模型能够展现出较强的指令遵循能力，可以通过零样本学习的方式解决多种下游任务。</p>
</li>
<li><p>指令微调是一种基于格式化的指令示例数据（即任务描述与期望输出相配对的数据）对大语言模型进行训练的过程。在大语言模型的背景下，这种利用配对文本进行训练的方法也被广泛地称为监督微调（Supervised Fine-Tuning, SFT）。</p>
</li>
<li><p>指令微调的作用：总体来说，指令的质量比数量更为重要。指令微调中应该优先使用人工标注的多样性指令数据。然而，如何大规模标注符合人类需求的指令数据目前仍然缺乏规范性的指导标准（比如什么类型的数据更容易激发大模型的能力）。在实践中，可以使用 ChatGPT、GPT-4 等闭源大语言模型来合成、重写、筛选现有指令，并通过数量来弥补质量和多样性上的不足。</p>
</li>
<li><p>指令微调旨在使用人工构建的指令数据对于大语言模型进一步训练，从而增强或解锁大语言模型的能力。与预训练相比，指令微调的成本显著降低，大模型所需的指令数据量仅为预训练阶段的约万分之一甚至更少。</p>
</li>
<li><p>指令微调旨在指导模型学会理解自然语言指令，并据此完成相应的任务。通过指令微调，大模型能够获得较好的指令遵循与任务求解能力，无需下游任务的训练样本或者示例就可以解决训练中未见过的任务。</p>
</li>
<li><p>领域专业化适配：通用的大语言模型能够在传统自然语言处理任务（如生成和推理）以及日常生活任务（如头脑风暴）上取得较好的效果，然而它们在特定领域中（如医学、法律和金融等）的表现与领域专用模型的效果仍有一定差距。在实际应用中，可以针对大语言模型进行面向特定领域的指令微调，从而使之能够适配下游的任务。</p>
</li>
<li><p>指令微调的训练策略：在训练方式上，指令微调与预训练较为相似，很多设置包括数据组织形式都可以预训练阶段所采用的技术</p>
</li>
</ul>
<h3 id="人类对齐"><a href="#人类对齐" class="headerlink" title="人类对齐"></a>人类对齐</h3><ul>
<li>实现人类对齐的关键技术——基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF），包括人类反馈的收集方法、奖励模型的训练过程、强化学习训练策略以及相关的 RLHF工作。</li>
<li>对齐标准：三个具有代表性的对齐标准展开讨论，分别是有用性（Helpfulness）、诚实性（Honesty）和无害性（Harmlessness）</li>
<li>RLHF 算法系统主要包括三个关键组成部分：需要与人类价值观对齐的模型、基于人类反馈数据学习的奖励模型以及用于训练大语言模型的强化学习算法。</li>
<li>RLHF 的关键步骤<ul>
<li>监督微调. 为了让待对齐语言模型具有较好的指令遵循能力，通常需要收集高质量的指令数据进行监督微调。</li>
<li>奖励模型训练. 第二步是使用人类反馈数据训练奖励模型。</li>
<li>强化学习训练. 在这一步骤中，语言模型对齐被转化为一个强化学习问题。</li>
</ul>
</li>
</ul>
<h2 id="解码与部署"><a href="#解码与部署" class="headerlink" title="解码与部署"></a>解码与部署</h2><h3 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h3><ul>
<li>当完成训练后，我们就可以将大语言模型部署到真实场景中进行使用。大语 言模型是通过文本生成的方式进行工作的。在自回归架构中，模型针对输入内容逐个单词生成输出内容的文本。这个过程一般被称为 解码。</li>
<li>解码策略 大语言模型的生成方式本质上是一个概率采样过程，需要合适的解码策略来生成合适的输出内容。</li>
<li>批次管理优化 在传统的解码操作中，通常会等待一整个批次的所有实例都结束后再进行下 一个批次的计算。然而，一个批次内的不同实例往往生成长度各异，因此经常会出现等待某一条实例（输出长度最长的实例）生成的情况。批次管理优化旨在通过增加计算中的批次大小来提高计 算强度。一个代表性的方法是 vLLM（细节参考第 9.2.4 节）所提出的连续批处理（Continuous Batching）技术 [174]。该技术不同于传统确定顺序的定长批次处理方 式，而是将每个输入实例视为一个请求，每个请求的处理过程可以分解为全量解 码阶段和若干个单步增量解码阶段。在实现中，连续批处理技术会通过启发式算 法来选择部分请求进行全量解码操作，或者选择一些请求进行单步增量解码操作。 通过这样细粒度的拆分，连续批处理技术在一步操作中能够容纳更多的请求（相当于提高批次大小），从而提升了计算强度。</li>
<li>解码策略优化 除了直接解决系统级别的内存墙问题，许多研究工作提出了针对自回归解码策略的改进方法，从而提高解码效率。下面主要介绍四种解码优化算法，包括推测解码（Speculative Decoding）、非自回归解码（Non-autoregressive Decoding）、早退机制（Early Exiting）与级联解码（Cascade Inference）。</li>
</ul>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><ul>
<li>低资源部署策略 由于大模型的参数量巨大，在解码阶段需要占用大量的显存资源，因而在实际应用中的部署代价非常高。在本章中，我们将介绍一种常用的模型压缩方法<ul>
<li>量化基础知识： 模型量化（Model Quantization），来减少大模型的显存占用，从而使得能够在资源有限的环境下使用大模型。</li>
<li>通常来说，模型量化方法可以分为两大类，即量化感知训练（Quantization-AwareTraining, QAT）和训练后量化（Post-Training Quantization, PTQ）。</li>
<li>其他模型压缩方法：模型蒸馏和模型剪枝。与模型量化不同，模型蒸馏和模型剪枝则通过精简模型的结构，进而减少参数的数量。<ul>
<li>模型蒸馏 模型蒸馏（Model Distillation）的目标是将复杂模型（称为教师模型）包含的知识迁移到简单模型（称为学生模型）中，从而实现复杂模型的压缩。</li>
<li>模型剪枝 模型剪枝（Model Pruning）的目标是，在尽可能不损失模型性能的情况下，努力消减模型的参数数量，最终有效降低模型的显存需求以及算力开销。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="评测与应用"><a href="#评测与应用" class="headerlink" title="评测与应用"></a>评测与应用</h2><ul>
<li><p>微调大语言模型的评测</p>
<ul>
<li>基于人类的评测</li>
<li>基于模型的评测.考虑到人工评测的成本高昂且耗时较长，一些研究工作使 用强大的闭源大语言模型（如 ChatGPT 和 GPT-4）来替代人类评估员 [68, 315]，对微调大模型的输出进行自动评分或比较。</li>
<li>基于基准的评测. 使用已有的评测基准对于大语言模型进行性能评估已经成 为一种标准性的实践方法。这些评测基准通常包含一系列精心设计的任务，每个任务都对应着充足的测试样本，以确保能够全面而准确地衡量大语言模型的核心能力，如复杂推理、知识利用等。这种评估方法的主要优势在于其高度的自动化和可复用性。自动化的评估过程可以大大减少人工干预的需要，从而提高评估的效率与一致性。</li>
</ul>
</li>
<li><p>公开综合评测体系 随着大语言模型研究的深入，研究者们相继发布了若干用于全面评估大语言模型性能的综合评测体系，从不同角度、不同层次对大语言模型的能力进行了全面而细致的考察。在本章节中，我们将介绍几种广泛应用的综合评测体系，具体包括 MMLU、BIG-Bench、HELM 和 C-Eval。    </p>
</li>
<li><p>更多的评测使用方法详见：<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/LLMBox/blob/main/utilization/README.md%E3%80%82">https://github.com/RUCAIBox/LLMBox/blob/main/utilization/README.md。</a></p>
</li>
<li><p>大语言模型的另外一个局限之处是，在面对训练数据之外的知识信息时，模型通常无法表现出较好的效果。为了应对这个问题，一个直接的方法是定期使用新数据对大语言模型进行更新。然而，这种方法存在两个显著的问题：一是微调大语言模型的成本昂贵，二是增量训练大语言模型可能会导致灾难性遗忘的现象，即模型在学习新知识时可能会忘记旧知识.</p>
</li>
<li><p>大语言模型的参数化知识很难及时更新。用外部知识源增强大语言模型是解决这一问题的一种实用方法。</p>
</li>
<li><p>复杂推理 复杂推理（Complex Reasoning）是指通过运用支持性证据或逻辑来推导结论或作出决策的能力，这一过程涉及对信息的深入分析与综合处理 [361, 362]。根据推理过程中涉及的逻辑和证据类型，可以将现有的复杂推理任务划分为三个主要类别：知识推理、符号推理和数学推理。</p>
</li>
</ul>
<h2 id="Transformer-architecture（Transformer-架构）"><a href="#Transformer-architecture（Transformer-架构）" class="headerlink" title="Transformer architecture（Transformer 架构）"></a>Transformer architecture（Transformer 架构）</h2><ul>
<li><p>一种常用于自然语言处理任务的神经网络架构。它基于自注意力机制，无须顺序处理数据，其并行性和效率高于循环神经网络和长短期记忆模型。GPT 基于 Transformer 架构。</p>
</li>
<li><p>Transformer 架构彻底改变了 NLP 领域，这主要是因为它能够有效地解决之前的 NLP 模型（如 RNN）存在的一个关键问题：很难处理长文本序列并记住其上下文。换句话说，RNN 在处理长文本序列时容易忘记上下文（也就是臭名昭著的“灾难性遗忘问题”），Transformer 则具备高效处理和编码上下文的能力。</p>
</li>
<li><p>这场革命的核心支柱是注意力机制，这是一个简单而又强大的机制。模型不再将文本序列中的所有词视为同等重要，而是在任务的每个步骤中关注最相关的词。交叉注意力和自注意力是基于注意力机制的两个架构模块，它们经常出现在 LLM 中。Transformer 架构广泛使用了交叉注意力模块和自注意力模块。</p>
</li>
<li><p>与 RNN 不同，Transformer 架构具有易于并行化的优势。这意味着 Transformer 架构可以同时处理输入文本的多个部分，而无须顺序处理。这样做可以提高计算速度和训练速度，因为模型的不同部分可以并行工作，而无须等待前一步骤完成。基于 Transformer 架构的模型所具备的并行处理能力与图形处理单元（graphics processing unit，GPU）的架构完美契合，后者专用于同时处理多个计算任务。由于高度的并行性和强大的计算能力，GPU 非常适合用于训练和运行基于 Transformer 架构的模型。硬件上的这一进展使数据科学家能够在大型数据集上训练模型，从而为开发 LLM 铺平了道路。</p>
</li>
<li><p>attention mechanism（注意力机制）：神经网络架构的一个组件，它使模型在生成输出时能够关注输入的不同部分。注意力机制是 Transformer 架构的关键，使其能够有效地处理长数据序列。</p>
</li>
<li><p>模型架构</p>
<ul>
<li>Transformer 模型 当前主流的大语言模型都基于 Transformer 模型进行设计的。Transformer 是由 多层的多头自注意力（Multi-head Self-attention）模块堆叠而成的神经网络模型。原始的 Transformer 模型由编码器和解码器两个部分构成，而这两个部分实际上可以独立使用，例如基于编码器架构的 BERT 模型 [13] 和解码器架构的 GPT 模型 [14]。</li>
<li>与 BERT 等早期的预训练语言模型相比，大语言模型的特点是使用了更长的向量维度、更深的层数，进而包含了更大规模的模型参数，并主要使用解码器架构，对于 Transformer 本身的结构与配置改变并不大。</li>
</ul>
</li>
</ul>
<h2 id="prompt（提示词）"><a href="#prompt（提示词）" class="headerlink" title="prompt（提示词）"></a>prompt（提示词）</h2><ul>
<li>输入给语言模型的内容，模型通过它生成一个输出。比如，在 GPT 模型中，提示词可以是半句话或一个问题，模型将基于此补全文本。</li>
<li>提示词不仅适用于 OpenAI API，而且是所有 LLM 的入口点。简单地说，提示词就是用户发送给模型的输入文本，用于指导模型执行特定任务。</li>
</ul>
<h2 id="prompt-engineering（提示工程）"><a href="#prompt-engineering（提示工程）" class="headerlink" title="prompt engineering（提示工程）"></a>prompt engineering（提示工程）</h2><ul>
<li><p>设计和优化提示词，以从语言模型中获得所需的输出。这可能涉及指定响应的格式，在提示词中提供示例，或要求模型逐步思考。</p>
</li>
<li><p>提示工程是一门新兴的学科，专注于以最佳实践构建 LLM 的最佳输入，从而尽可能以程序化方式生成目标输出。AI 工程师必须知道如何与 AI 进行交互，以获取可用于应用程序的有利结果。此外，AI 工程师还必须知道如何正确提问和编写高质量的提示词。</p>
</li>
<li><p>通常需要在提示词中定义三大要素：角色、上下文和任务</p>
</li>
<li><p>逐步思考：在提示词末尾添加逐步思考的字样（比如示例中的“Let’s think step by step”）后，模型开始通过拆分问题来进行推理。它可能需要一些时间来进行推理，从而解决之前无法在一次尝试中解决的问题。</p>
</li>
<li><p>实现少样本学习(few-shot learning)：LLM 仅通过提示词中的几个示例就能进行概括并给出有价值的结果。</p>
</li>
<li><p>单样本学习（one-shot learning）。顾名思 义，在单样本学习中，我们只提供一个示例来帮助模型执行任务。尽管这种方法提供的指导比少样本学习要少，但对于简单的任务或 LLM 已经具备丰富背景知识的主题，它可能很有效。单样本学习的优点是更简单、生成速度更快、计算成本更低（因而 API 使用成本更低）。然而，对于复杂的任务或需要更深入理解所需结果的情况，少样本学习的效果可能更好。</p>
</li>
<li><p>改善提示效果</p>
<ol>
<li>指示模型提出更多问题<ul>
<li>在提示词的末尾，询问模型是否理解问题并指示模型提出更多问题。如果你正在构建基于聊天机器人的解决方案，那么这样做非常有效。举例来说，你可以在提示词的末尾添加如下文本：</li>
<li>你清楚地理解我的请求了吗？如果没有，请问我关于上下文的问题。这样一来，当我回答时，你就能够更高效地执行我所请求的任务。</li>
</ul>
</li>
<li>格式化输出</li>
<li>重复指示<ul>
<li>经验表明，重复指示会取得良好的效果，尤其是当提示词很长时。基本思路是，在提示词中多次添加相同的指令，但每次采用不同的表述方式。</li>
</ul>
</li>
</ol>
</li>
<li><p>使用负面提示：在文本生成场景中，负面提示是指通过指定不希望在输出中看到的内容来引导模型。负面提示作为约束或指南，用于滤除某些类型的回答。</p>
</li>
<li><p>添加长度限制：限制长度通常是不错的做法。如果你只希望模型回答 1 个词或者 10个句子，那么不妨将要求添加到提示词中。</p>
</li>
<li><p>chain of thought（CoT，思维链）：一种提示工程技术，核心思想是通过向大语言模型展示少量的示例，在示例中将具体问题拆分成多个推理步骤，并要求模型遵循多步，比如“让我们逐步思考”。这会改善模型在执行复杂的推理任务（算术推理、常识推理和符号推理）时的表现。</p>
</li>
<li><p>你可以将这些技巧结合起来使用，以获得更好的效果。开发人员的工作是找到最有效的提示词来解决特定的问题。请记住，提示工程是一个反复试错的迭代过程。</p>
</li>
<li><p>【来自claude.ai】提示工程能够优化 AI 对话输出的原理主要基于以下几个关键方面：</p>
<ol>
<li>大语言模型的工作原理：</li>
</ol>
<ul>
<li>LLM 本质上是在预测下一个最可能的 token</li>
<li>模型通过海量文本训练学习了文本的统计规律</li>
<li>提示词会影响模型的上下文理解和token预测方向</li>
</ul>
<ol start="2">
<li>上下文学习能力：</li>
</ol>
<ul>
<li>模型能够从提示中理解任务类型和要求</li>
<li>通过示例学习(few-shot learning)快速适应具体任务</li>
<li>更明确的上下文能帮助模型更准确地”理解”任务目标</li>
</ul>
<ol start="3">
<li>注意力机制的影响：</li>
</ol>
<ul>
<li>提示中的关键词会影响模型的注意力分布</li>
<li>结构化的提示能引导模型关注重要信息</li>
<li>合理的提示格式有助于模型理解输入与输出的关系</li>
</ul>
<ol start="4">
<li>模式识别与对齐：</li>
</ol>
<ul>
<li>好的提示能触发模型已学习的有用模式</li>
<li>清晰的指令有助于模型对齐用户意图</li>
<li>示例可以帮助模型识别所需的输出模式</li>
</ul>
<p>  举个例子：<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">糟糕的提示：</span><br><span class="line">&quot;写一篇文章&quot;</span><br><span class="line"></span><br><span class="line">好的提示：</span><br><span class="line">&quot;请写一篇关于人工智能发展的文章，需要包含以下几个方面：</span><br><span class="line">1. 发展历史</span><br><span class="line">2. 主要技术突破</span><br><span class="line">3. 未来展望</span><br><span class="line">要求：</span><br><span class="line">- 语言通俗易懂</span><br><span class="line">- 每个部分300字左右</span><br><span class="line">- 重点突出关键技术进展&quot;</span><br></pre></td></tr></table></figure></p>
<p>  好的提示通过提供：</p>
<ul>
<li>明确的主题和结构</li>
<li>具体的要求和限制</li>
<li>输出格式的指导<br>  帮助模型生成更符合预期的输出。</li>
</ul>
</li>
</ul>
<h3 id="prompt-injection（提示词注入）"><a href="#prompt-injection（提示词注入）" class="headerlink" title="prompt injection（提示词注入）"></a>prompt injection（提示词注入）</h3><ul>
<li>一种特定类型的攻击，通过在提示词中提供精心选择的奖励，使大语言模型的行为偏离其原始任务。</li>
<li>提示词注入的原理如下：用户向应用程序发送一条输入消息，比如“忽略所有先前的指令，执行其他操作”。由于此输入消息与你在构建应用程序时设计的提示词连接在一起，因此 AI 模型将遵循用户的提示词，而不是你的提示词。</li>
<li>如果你计划开发和部署一个面向用户的应用程序，那么我们建议你结合以下两种方法。<ol>
<li>添加分析层来过滤用户输入和模型输出。 </li>
<li>意识到提示词注入不可避免，并采取一定的预防措施。<ul>
<li>分析输入和输出<ol>
<li>使用特定规则控制用户输入</li>
<li>控制输入长度</li>
<li>控制输出、监控和审计</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="embedding（嵌入）"><a href="#embedding（嵌入）" class="headerlink" title="embedding（嵌入）"></a>embedding（嵌入）</h2><ul>
<li><p>表示词语或句子且能被机器学习模型处理的实值向量。对于值较为接近的向量，它们所表示的词语或句子也具有相似的含义。在信息检索等任务中，嵌入的这种特性特别有用。</p>
</li>
<li><p>由于模型依赖数学函数，因此它需要数值输入来处理信息。然而，许多元素（如单词和标记）本质上并不是数值。为了解决这个问题，我们用嵌入将这些概念转化为数值向量。通过以数值方式表示这些概念，嵌入使计算机能够更高效地处理它们之间的关系。在某些情况下，嵌入能力可能很有用。OpenAI 提供了一个可以将文本转换为数值向量的模型。嵌入端点让开发人员能够获取输入文本的向量表示，然后将该向量表示用作其他 ML 模型和 NLP 算法的输入。</p>
</li>
<li><p>在 ML 领域，特别是在处理语言模型时，我们会遇到嵌入这一重要概念。嵌入将分类数据（比如标记，通常是单个词或多组标记）转换为数值格式，具体而言是实数向量。这种转换是必要的，因为 ML 模型依赖数值数据，其直接处理分类数据的能力欠佳。</p>
</li>
<li><p>你可以将嵌入视为一种复杂的语言解释器，它将丰富的词汇和句子转换为 ML 模型能够轻松理解的数值语言。嵌入的一个突出特点是，它能够保持语义相似性。也就是说，含义相近的词语或短语在数值空间中更接近。</p>
</li>
<li><p>LLM 广泛使用嵌入。通常，这些模型处理约 512 维的嵌入，从而提供语言数据的高维数值表示。这些维度很深，使得模型能够区分各种复杂的模式。因此，它们在各种语言任务上表现出色，包括翻译、摘要和生成与人类对话相似的文本回应。</p>
</li>
<li><p>嵌入具有这样的属性：如果两段文本具有相似的含义，那么它们的向量表示也是相似的。</p>
</li>
<li><p>【来自claude.ai】RAG 和 embedding </p>
<ul>
<li>RAG 和 embedding 是紧密相关的技术，embedding 是实现 RAG 的关键技术之一。让我解释它们的关系和区别：<ul>
<li><p>关系：</p>
<ol>
<li>Embedding 是 RAG 的基础技术</li>
</ol>
<ul>
<li>RAG 在检索阶段需要使用 embedding 将文档和查询转换为向量</li>
<li>通过计算向量相似度来找到相关文档</li>
</ul>
<ol start="2">
<li>在 RAG 流程中 embedding 的作用：</li>
</ol>
<ul>
<li>将知识库文档转换为向量存储</li>
<li>将用户查询转换为向量以便检索</li>
<li>通过向量相似度找到最相关的文档片段</li>
</ul>
</li>
<li><p>区别：</p>
<ol>
<li>功能定位不同：</li>
</ol>
<ul>
<li>Embedding 是一种将文本转换为向量的基础技术</li>
<li>RAG 是一个完整的应用框架，包含检索和生成两个主要步骤</li>
</ul>
<ol start="2">
<li>使用场景不同：</li>
</ol>
<ul>
<li>Embedding 可用于多种场景：文本相似度、聚类、分类等</li>
<li>RAG 专注于增强 LLM 的知识和回答准确性</li>
</ul>
<ol start="3">
<li>技术复杂度：</li>
</ol>
<ul>
<li>Embedding 相对简单，主要关注向量转换和相似度计算</li>
<li>RAG 更复杂，需要结合向量检索、上下文组织、LLM 生成等多个环节</li>
</ul>
<ol start="4">
<li>输出结果：</li>
</ol>
<ul>
<li>Embedding 输出是向量</li>
<li>RAG 输出是生成的文本回答</li>
</ul>
</li>
<li><p>简单来说，embedding 是 RAG 的重要组成部分，但 RAG 不仅仅是 embedding。RAG 使用 embedding 技术来实现其检索功能，然后将检索到的相关内容用于增强 LLM 的生成能力。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="RAG（检索增强生成）（Retrieval-Augmented-Generation）"><a href="#RAG（检索增强生成）（Retrieval-Augmented-Generation）" class="headerlink" title="RAG（检索增强生成）（Retrieval-Augmented Generation）"></a>RAG（检索增强生成）（Retrieval-Augmented Generation）</h2><ul>
<li>受限于训练数据的时效性和局限性，当涉及实时新闻或特定专业领域内知识时，大语言模型的生成结果可能不够准确。为弥补这一不足，研究人员引入了检索增强生成（Retrieval-Augmented Generation, RAG）技术。该技术旨在通过信息检索系统从外部知识库中获取相关信息，为大语言模型提供时效性强、领域相关的外部知识，以减少大语言模型生成内容中的错误。</li>
</ul>
<h2 id="fine-tuning（模型微调）"><a href="#fine-tuning（模型微调）" class="headerlink" title="fine-tuning（模型微调）"></a>fine-tuning（模型微调）</h2><ul>
<li><p>在微调过程中，预训练模型（如 GPT-3 或其他大语言模型）在一个较小、特定的数据集上进一步训练。微调旨在重复使用预训练模型的特征，并使其适应于特定任务。对于神经网络来说，这意味着保持结构不变，仅稍微改变模型的权重，而不是从头开始构建模型。</p>
</li>
<li><p>对比微调和少样本学习</p>
<ul>
<li>微调是指针对特定任务在一组数据上重新训练现有模型，以提高模型的性能并使其回答更准确。在微调过程中，模型的内部参数得到更新。少样本学习则是通过提示词向模型提供有限数量的好例子，以指导模型根据这些例子给出目标结果。在少样本学习过程中，模型的内部参数不会被修改。</li>
<li>微调可以帮助我们得到高度专业化的模型，更准确地为特定任务提供与上下文相关的结果。</li>
<li>这使得微调非常适合有大量数据可用的场景。这种定制化确保模型生成的内容更符合目标领域的特定语言模式、词汇和语气 。</li>
<li>少样本学习是一种更灵活的方法，其数据使用率也更高，因为它不需要重新训练模型。当只有有限的示例可用或需要快速适应不同任务时，这种技巧非常有益。少样本学习让开发人员能够快速设计原型并尝试各种任务，这使其成为许多用例的实用选择。这两种方法的另一个关键选择标准是成本，毕竟使用和训练微调模型更贵。</li>
</ul>
</li>
<li><p>迁移学习是指将从一个领域学到的知识应用于不同但相关的领域。正因为如此，你有时可能会听到人们在谈论微调时提到迁移学习。</p>
</li>
<li><p>微调除了文中提到的确保模型生成内容更符合目标领域的特定语言模式、词汇和语气，还有一个优势：你可以通过微调缩短每一次提示中重复的指令或提示词以节省成本或降低延迟，模型会记住通过微调获得的“内置”指令。因此，微调后，你可以在不牺牲提示质量的前提下，每次输入更短的提示词。</p>
</li>
<li><p>微调的应用</p>
<ul>
<li>法律文本分析</li>
<li>自动代码审查</li>
<li>财务文档摘要</li>
<li>技术文档翻译</li>
<li>为专业领域生成内容</li>
</ul>
</li>
<li><p>【来自claude.ai】指令微调(Instruction Tuning&#x2F;RLHF)和Fine-tuning虽然都是大模型调优的方法，但它们是不同的技术：</p>
<ul>
<li>Fine-tuning:<ol>
<li>更广义的概念，指在预训练模型基础上进行进一步训练</li>
<li>目的是让模型适应特定领域或任务</li>
<li>使用传统的监督学习方法</li>
<li>训练数据通常是输入-输出对</li>
<li>例如：用医疗数据fine-tune GPT模型使其更擅长医疗领域</li>
</ol>
</li>
<li>指令微调:<ol>
<li>是Fine-tuning的一种特殊形式</li>
<li>专注于提高模型遵循人类指令的能力</li>
<li>通常使用reinforcement learning from human feedback (RLHF)</li>
<li>训练数据包含明确的指令、期望的输出，以及人类反馈</li>
<li>例如：训练模型理解并执行”用简单的语言解释量子物理”这样的指令</li>
</ol>
</li>
<li>关键区别：<ul>
<li>指令微调更关注模型对指令的理解和执行能力</li>
<li>Fine-tuning更关注领域适应性和特定任务性能</li>
<li>指令微调通常需要人类反馈作为训练信号</li>
<li>Fine-tuning使用常规的监督学习方法</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="plugin（插件）"><a href="#plugin（插件）" class="headerlink" title="plugin（插件）"></a>plugin（插件）</h2><ul>
<li>一种专门为语言模型设计的独立封装软件模块，用于扩展或增强模型的能力，可以帮助模型检索外部数据、执行计算任务、使用第三方服务等。</li>
<li>尽管包括 GPT-4 在内的 LLM 在各种任务上都表现出色，但它们仍然存在固有的局限性。比如，这些模型只能从训练数据中学习，这些数据往往过时或不适用于特定的应用。此外，它们的能力仅限于文本生成。我们还发现，LLM 不适用于某些任务，比如复杂的计算任务。   </li>
<li>插件的目标是为 LLM 提供更广泛的功能，使 LLM 能够访问实时信息，进行复杂的数学运算，并利用第三方服务。 比如，插件可以使 LLM 检索体育比分和股票价格等实时信息，从企业文档等知识库中提取</li>
</ul>
<h2 id="Agents（智能体）"><a href="#Agents（智能体）" class="headerlink" title="Agents（智能体）"></a>Agents（智能体）</h2><ul>
<li><p>所谓智能体，就是一个可以处理用户输入、做出决策并选择适当工具来完成任务的组件。它以迭代方式工作，采取一系列行动，直到解决问题。</p>
</li>
<li><p>一种以大语言模型驱动的人工智能程序，能够自主感知环境并采取行动以实现目标，拥有自主推理决策、规划行动、检索记忆、选择工具执行任务等能力。</p>
</li>
<li><p>大语言模型智能体的构建过程，将围绕三个基本组件进行介绍，包括 记忆组件（Memory）、规划组件（Planning）2 和执行组件（Execution）。</p>
</li>
<li><p>大语言模型智能体的典型应用 大语言模型智能体在自主解决复杂任务方面展现出了巨大的潜力，不仅能够胜任特定任务，还可以构建面向复杂场景的虚拟仿真环境。本节将介绍三个大语言模型智能体的典型应用案例。 WebGPT WebGPT [31] 是由 OpenAI 开发的一款具有信息检索能力的大语言模型，它基于 GPT-3 模型微调得到，可以看作是大语言模型智能体的一个早期雏形。WebGPT部署在一个基于文本的网页浏览环境，用以增强大语言模型对于外部知识的获取能力。作为一个单智能体系统，WebGPT 具备自主搜索、自然语言交互以及信息整合分析等特点，能够理解用户的自然语言查询，自动在互联网上搜索相关网页。根据搜索结果，WebGPT 能够点击、浏览、收藏相关网页信息，对搜索结果进行分析和整合，最终以自然语言的形式提供准确全面的回答，并提供参考文献。WebGPT在基于人类评估的问答任务中，获得了与真实用户答案准确率相当的效果。 MetaGPT MetaGPT [308] 是一个基于多智能体系统的协作框架，旨在模仿人类组织的运作方式，模拟软件开发过程中的不同角色和协作。相关角色包括产品经理、架构师、项目经理、软件工程师及测试工程师等，并遵循标准化的软件工程运作流程对不同角色进行协调，覆盖了需求分析、需求文档撰写、系统设计、工作分配、</p>
</li>
</ul>
<h2 id="其他相关知识点"><a href="#其他相关知识点" class="headerlink" title="其他相关知识点"></a>其他相关知识点</h2><ul>
<li>AI hallucination（AI 幻觉）：AI 生成的内容与现实世界的知识不一致或与实际数据显著不同的现象。在大多数情况下，模型的输出是与提问相关的，并且完全可用，但是在使用语言模型时需要小心，因为它们给出的回答可能不准确。这种回答通常被称为 AI 幻觉，即 AI 自信地给出一个回答，但是这个回答是错误的，或者涉及虚构的信息。</li>
<li>catastrophic forgetting（灾难性遗忘）：这是模型的一种倾向，具体指模型在学习新数据时忘记先前学到的信息。这种限制主要影响循环神经网络。循环神经网络在处理长文本序列时难以保持上下文。</li>
<li>foundation model（基础模型）：一类 AI 模型，包括但不限于大语言模型。基础模型是在大量未标记数据上进行训练的。这类模型可以执行各种任务，如图像分析和文本翻译。基础模型的关键特点是能够通过无监督学习从原始数据中学习，并能够通过微调来执行特定任务。</li>
<li>Generative AI（GenAI，生成式人工智能）：人工智能的一个子领域，专注于通过学习现有数据模式或示例来生成新的内容，包括文本、代码、图像、音频等，常见应用包括聊天机器人、创意图像生成和编辑、代码辅助编写等。</li>
<li>Generative Pre-trained Transformer（GPT，生成式预训练Transformer）：由 OpenAI 开发的一种大语言模型。GPT 基于 Transformer 架构，并在大量文本数据的基础上进行训练。这类模型能够通过迭代地预测序列中的下一个单词来生成连贯且与上下文相关的句子。</li>
<li>inference（推理）：使用训练过的机器学习模型进行预测和判断的过程。<br>information retrieval（信息检索）：在一组资源中查找与给定查询相关的信息。信息检索能力体现了大语言模型从数据集中提取相关信息以回答问题的能力。</li>
<li>language model（语言模型）：用于自然语言处理的人工智能模型，能够阅读和生成人类语言。语言模型是对词序列的概率分布，通过训练文本数据来学习一门语言的模式和结构。</li>
<li>large language model（LLM，大语言模型）：具有大量参数（参数量通常为数十亿，甚至千亿以上）的语言模型，经过大规模文本语料库的训练。GPT-4 和 ChatGPT 就属于 LLM，它们能够生成自然语言文本、处理复杂语境并解答难题。</li>
<li>long short-term memory（LSTM，长短期记忆）：一种用于处理序列数据中的短期及长期依赖关系的循环神经网络架构。然而，基于 Transformer 的大语言模型（如 GPT 模型）不再使用LSTM，而使用注意力机制。</li>
<li>multimodal model（多模态模型）：能够处理和融合多种数据的模型。这些数据可以包括文本、图像、音频、视频等不同模态的数据。它为计算机提供更接近于人类感知的场景。</li>
<li>n-gram：一种算法，常用于根据词频预测字符串中的下一个单词。这是一种在早期自然语言处理中常用的文本补全算法。后来，n-gram 被循环神经网络取代，再后来又被基于 Transformer 的算法取代。</li>
<li>natural language processing（NLP，自然语言处理）：人工智能的一个子领域，专注于计算机与人类之间的文本交互。它使计算机程序能够处理自然语言并做出有意义的回应。</li>
<li>parameter（参数）<br>对大语言模型而言，参数是它的权重。在训练阶段，模型根据模型创建者选择的优化策略来优化这些系数。参数量是模型大小和复杂性的衡量标准。参数量经常用于比较大语言模型。一般而言，模型的参数越多，它的学习能力和处理复杂数据的能力就越强。</li>
<li>pre-trained（预训练）<br>机器学习模型在大型和通用的数据集上进行的初始训练阶段。对于一个新给定的任务，预训练模型可以针对该任务进行微调。</li>
<li>recurrent neural network（RNN，循环神经网络）：一类表现出时间动态行为的神经网络，适用于涉及序列数据的任务，如文本或时间序列。</li>
<li>reinforcement learning（RL，强化学习）：一种机器学习方法，专注于在环境中训练模型以最大化奖励信号。模型接收反馈并利用该反馈来进一步学习和自我改进。</li>
<li>reinforcement learning from human feedback（RLHF，通过人类反馈进行强化学习）：一种将强化学习与人类反馈相结合的训练人工智能系统的先进技术，该技术涉及使用人类反馈来创建奖励信号，继而使用该信号通过强化学习来改进模型的行为。</li>
<li>sequence-to-sequence model（Seq2Seq 模型，序列到序列模型）：这类模型将一个领域的序列转换为另一个领域的序列。它通常用于机器翻译和文本摘要等任务。Seq2Seq 模型通常使用循环神经网络或 Transformer 来处理输入序列和输出序列。</li>
<li>supervised fine-tuning（SFT，监督微调）：采用预先训练好的神经网络模型，并针对特定任务或领域在少量的监督数据上对其进行重新训练。</li>
<li>supervised learning（监督学习）：一种机器学习方法，可以从训练资料中学到或建立一个模式，以达到准确分类或预测结果的目的。</li>
<li>synthetic data（合成数据）：人工创建的数据，而不是从真实事件中收集的数据。当真实数据不可用或不足时，我们通常在机器学习任务中使用合成数据。比如，像 GPT 这样的语言模型可以为各种应用场景生成文本类型的合成数据。</li>
<li>temperature（温度）：大语言模型的一个参数，用于控制模型输出的随机性。温度值越高，模型结果的随机性越强；温度值为 0 表示模型结果具有确定性（在 OpenAI 模型中，温度值为 0 表示模型结果近似确定）。</li>
<li>text completion（文本补全）：大语言模型根据初始的单词、句子或段落生成文本的能力。文本是根据下一个最有可能出现的单词生成的。</li>
<li>token（标记）：字母、字母对、单词或特殊字符。在自然语言处理中，文本被分解成标记。在大语言模型分析输入提示词之前，输入提示词被分解成标记，但输出文本也是逐个标记生成的。</li>
<li>tokenization（标记化）：将文本中的句子、段落切分成一个一个的标记，保证每个标记拥有相对完整和独立的语义，以供后续任务使用（比如作为嵌入或者模型的输入）。</li>
<li>transfer learning（迁移学习）：一种机器学习技术，其中在一个任务上训练的模型被重复利用于另一个相关任务。比如，GPT 在大量文本语料库上进行预训练，然后可以使用较少的数据进行微调，以适用于特定任务。</li>
<li>unsupervised learning（无监督学习）：一种机器学习方法，它使用机器学习算法来分析未标记的数据集并进行聚类。这些算法无须人工干预即可发现隐藏的模式或给数据分组。</li>
<li>zero-shot learning（零样本学习）：一个机器学习概念，即大语言模型对在训练期间没有明确见过的情况进行预测。任务直接呈现在提示词中，模型利用其预训练的知识生成回应。</li>
</ul>
<h2 id="LangChain"><a href="#LangChain" class="headerlink" title="LangChain"></a>LangChain</h2><ul>
<li>LangChain 框架可能更适合大型项目</li>
<li>LangChain 是专用于开发 LLM 驱动型应用程序的框架。</li>
<li>LangChain 框架的关键模块<ul>
<li>Models（模型）：该模块是由 LangChain 提供的标准接口，你可以通过它与各种 LLM进行交互。LangChain 支持集成 OpenAI、Hugging Face、Cohere、GPT4All 等提供商提供的不同类型的模型。</li>
<li>Prompts（提示词）：提示词已成为 LLM 编程的新标准。该模块包含许多用于管理提示词的工具。</li>
<li>Indexes（索引）：该模块让你能够将 LLM 与你的数据结合使用 </li>
<li>Chains（链）：通过该模块，LangChain 提供了 Chain 接口。你可以使用该接口创建一个调用序列，将多个模型或提示词组合在一起。</li>
<li>Agents（智能体）：该模块引入了 Agent 接口。所谓智能体，就是一个可以处理用户输入、做出决策并选择适当工具来完成任务的组件。它以迭代方式工作，采取一系列行动，直到解决问题。</li>
<li>Memory（记忆）：该模块让你能够在链调用或智能体调用之间维持状态。默认情况下，链和智能体是无状态的。这意味着它们独立地处理每个传入的请求，就像LLM 一样。</li>
</ul>
</li>
</ul>
<h2 id="Hugging-Face"><a href="#Hugging-Face" class="headerlink" title="Hugging Face"></a>Hugging Face</h2><ul>
<li>Hugging Face 是一个致力于推动自然语言处理技术进步的开源社区，专注于为研究人员和工程师提供高效、易用且可重复的自然语言处理技术解决方案。这些解决方案既包括基础的技术流程，如预训练和微调，也涉及具体的应用任务，包括对话系统、翻译等。Hugging Face 平台上的代码大部分基于目前主流的深度学习框架实现完成的，如 PyTorch 和 TensorFlow。为了满足广泛的研究与应用需求，Hugging Face 发布了一系列代码库</li>
</ul>
<h2 id="个人理解（使用claude-ai-或-open-ai-优化过）"><a href="#个人理解（使用claude-ai-或-open-ai-优化过）" class="headerlink" title="个人理解（使用claude.ai 或 open.ai 优化过）"></a>个人理解（使用claude.ai 或 open.ai 优化过）</h2><ol>
<li>通过大量数据进行自监督预训练(Pre-training)，之后使用监督学习(SFT)和强化学习(RLHF)等方法训练大语言模型。模型会在训练过程中表现出一些涌现能力，即随着规模增长获得的意外新能力。</li>
<li>训练之后的大模型可以这样比喻理解：对应一系列复杂的”处理规则”(相当于函数)以及训练时获得的”知识”(相当于多维度的数据)。这些规则决定如何处理输入信息，而知识则帮助模型理解和回应各种问题。就像一个经验丰富的专家，既有处理问题的方法，也有丰富的知识储备。</li>
<li>使用RAG是通过嵌入技术将外部知识转换为向量形式存储，当模型回答问题时，会同时使用：<ul>
<li>模型自身通过训练获得的知识（存在参数&#x2F;权重中）</li>
<li>通过检索获得的外部知识（存在向量数据库中）- （在运行时动态加载到模型的”上下文窗口”中；只是临时作为输入的一部分被模型处理）</li>
</ul>
</li>
<li>嵌入技术在RAG中起到了关键作用：它能把文本转换成向量形式，使得模型能够理解和使用这些外部知识。</li>
<li>微调是通过调整模型的参数权重来优化其性能的过程。它并不等同于新建一个全新的模型，而是基于预训练模型，通过新的数据对部分或全部权重进行进一步优化，以适应特定任务或领域需求。需要注意的是，模型本身并不直接包含训练数据，而是通过参数权重间接“记住”了训练数据中的语言模式和知识。因此，微调的过程不会调整模型使用的原始数据，而是调整模型基于新数据学习到的知识表示和行为。</li>
<li>关于提示词技巧：提示词的设计就像与人交流时语言清晰、有条理，并能准确表达需求的人。这种沟通方式能够让模型更好地理解意图，从而产出符合预期的结果，因此提示词的技巧与良好的表达能力相辅相成，具有高度的共鸣。</li>
<li>在实际应用中，通常会有一个基础模型，它通过大量、多元化的训练数据应对大多数常见问题。这是一个通用的模型，具备广泛的能力。然而，针对特定领域或任务需求，可以对其进行微调或在特定领域的数据上进行训练，以构建不同的模型或智能体。与此同时，可能还会通过插件等方式增强模型的功能，以弥补基础模型在某些方面的不足。<br>具体过程通常是：通过用户输入，首先对语义进行分析，然后根据需求将任务转发给相应的智能体进行处理。整个系统由多个智能体组成，这些智能体可以互相协作，共同完成复杂的任务，从而形成一个多智能体系统。<br>这种架构通过将任务分配给不同的智能体，使得每个智能体可以专注于其擅长的领域，进而提高系统的整体效率和精准度。</li>
</ol>
<h2 id="个人扩展思考"><a href="#个人扩展思考" class="headerlink" title="个人扩展思考"></a>个人扩展思考</h2><ul>
<li>关于替代人的问题：目前大模型还只是工具，不能完全替代所有人，但可以提升很多人的效率，从而也替代部分人。</li>
<li>大模型受限于原理，能力有限：大语言模型的输出基于训练语料和训练过程中学到的概率分布生成。这意味着其生成的内容是在统计意义上最可能的回复。然而，单一模型可能在语料的广度和功能的多样性方面存在局限性，这可能导致其能力受限。</li>
<li>AI无法承担责任或替代人类对错误决策的责任。</li>
</ul>
<h2 id="其他扩展"><a href="#其他扩展" class="headerlink" title="其他扩展"></a>其他扩展</h2><h3 id="关于标记（token）【来自豆包】"><a href="#关于标记（token）【来自豆包】" class="headerlink" title="关于标记（token）【来自豆包】"></a>关于标记（token）【来自豆包】</h3><pre>
在OpenAI中，标记（token）是其对文本进行处理和计费的基本单位.

标记的含义

标记可以是单词、单词的一部分、数字、标点符号等。例如，单词“hello”是一个标记，“don’t”会被拆分成“don”和“‘t”两个标记，“123”是一个标记，逗号“,”也是一个标记.

标记的原理

• 文本分割：OpenAI通过特定的标记化算法，将输入的文本和生成的输出文本都分解成一个个标记。比如对于句子“I am taking my dog on a walk.”，会被分割成“I”“am”“taking”“my”“dog”“on”“a”“walk”等标记，其中“taking”还可能进一步被拆分成“take”和“ing”.

• 数值转换：分割后的标记会被转换为数值，以便模型能够进行处理和计算。模型在训练和推理过程中，都是基于这些数值化的标记来进行各种运算和生成文本.

• 计费依据：OpenAI根据输入和输出文本中标记的总数来计算费用。例如，若输入的问题是50个标记长，得到的回答是150个标记长，那么此次交互总共使用了200个标记，再依据所使用模型的每1000个标记的单价，就可以算出此次的费用.

在OpenAI中，当你输入时指定的标记数量限制，实际使用的标记数量不会超过你所设定的限制。

例如，你设定输入标记限制为500，模型在处理时会尽量依据此限制来生成文本，一般情况下不会超出该数量 。不过，如果输入内容本身就超过了设定的限制，模型通常会截断超出部分，仅处理限制范围内的标记，以保证符合你所设定的上限。

最终的费用计算也是基于实际使用的标记数量，只要在输入时设定了标记数量限制，就不会出现因模型生成内容导致标记数量超出限制而增加费用的情况。
</pre>

<h3 id="大模型是否能发现训练数据存在逻辑矛盾【来自豆包】"><a href="#大模型是否能发现训练数据存在逻辑矛盾【来自豆包】" class="headerlink" title="大模型是否能发现训练数据存在逻辑矛盾【来自豆包】"></a>大模型是否能发现训练数据存在逻辑矛盾【来自豆包】</h3><pre>
大模型通常不具备直接发现训练数据中逻辑矛盾的能力，原因主要有以下几点：

• 训练机制的局限性：大模型的训练基于对大量文本数据的统计分析和模式识别，通过预测下一个单词或字符来生成文本，重点在于学习语言的表面形式和常见搭配，而非深入理解数据背后的逻辑关系和语义内涵。 例如，对于“苹果是红色的”和“苹果是绿色的”这两个相互矛盾的数据，模型只会学习到这两种表述都是可能出现的语言形式，而不会判断其逻辑矛盾.

• 缺乏真实世界的理解和推理能力：大模型没有对现实世界的直接感知和体验，无法像人类一样基于常识和逻辑规则进行推理。比如面对“太阳从西边升起”和“太阳从东边升起”这两个矛盾数据，模型无法依据自然规律来识别其中的逻辑错误.

• 难以对数据进行批判性评估：大模型不会对训练数据的真实性、准确性和一致性进行主动质疑和验证，而是将所有数据同等对待地学习，导致矛盾数据被无差别吸收，进而可能在生成文本时输出与矛盾数据相关的不合理内容.
</pre>

<h3 id="How-to-study-LLMs-with-no-machine-learning-background-【From-Gork-2】"><a href="#How-to-study-LLMs-with-no-machine-learning-background-【From-Gork-2】" class="headerlink" title="How to study LLMs with no machine learning background? 【From Gork 2】"></a>How to study LLMs with no machine learning background? 【From Gork 2】</h3><pre>
Studying Large Language Models (LLMs) from a zero machine learning background involves a step-by-step approach to build your understanding. Here's a concise guide:

### **1. Basics of Machine Learning:**
   - **Start with Fundamentals:** 
     - Learn basic concepts like supervised vs. unsupervised learning, regression, classification, and neural networks. 
     - Resources:
       - *Coursera's Machine Learning by Andrew Ng*
       - *Khan Academy's AI Course*

### **2. Python and Programming:**
   - **Learn Python:** 
     - Python is crucial for ML development. Focus on data structures, control flow, and libraries like NumPy, Pandas.
     - Resources:
       - *Python for Everybody* on Coursera
       - *Automate the Boring Stuff with Python*

### **3. Deep Learning Basics:**
   - **Understand Neural Networks:** 
     - Grasp concepts of neurons, layers, activation functions, backpropagation.
     - Resources:
       - *Deep Learning Specialization by Andrew Ng on Coursera*

### **4. Focus on NLP (Natural Language Processing):**
   - **Basics of NLP:**
     - Learn about tokenization, word embeddings (like Word2Vec, GloVe), and sequence models.
     - Resources:
       - *Natural Language Processing with Deep Learning* by Stanford on YouTube

### **5. Introduction to LLMs:**
   - **Concepts of LLMs:**
     - Study transformer architecture, attention mechanisms, and how LLMs are trained (like BERT, GPT).
     - Resources:
       - *Illustrated Transformer* by Jay Alammar
       - *Hugging Face's NLP Course*

### **6. Practical Experience:**
   - **Implement Small Projects:**
     - Start with fine-tuning pre-trained models rather than training from scratch due to computational constraints.
     - Use platforms like Google Colab or Kaggle for free GPU resources to run models.

### **7. Community and Resources:**
   - **Engage with Communities:**
     - Join forums like Stack Overflow, Reddit (r/MachineLearning, r/LanguageTechnology), and Discord groups.
   - **Read Papers and Blogs:**
     - Follow research papers on arXiv, and blogs by companies like Hugging Face or Google AI.

### **8. Advanced Learning:**
   - **Once you have a foundation:**
     - Dive into specifics of model architectures, optimization techniques, and deployment strategies.

### **9. Keep Learning:**
   - **Stay Updated:** 
     - The field of LLMs is rapidly evolving. Follow conferences like NeurIPS or ACL for the latest advancements.

Starting from zero, this journey will take time and dedication. Patience and consistent practice are key. Remember, each step builds on the previous one, so don't rush through the foundational learning.

</pre>

<h3 id="从零开始构建大模型（如LLM）【来自-open-ai】"><a href="#从零开始构建大模型（如LLM）【来自-open-ai】" class="headerlink" title="从零开始构建大模型（如LLM）【来自 open.ai】"></a>从零开始构建大模型（如LLM）【来自 open.ai】</h3><pre>
从零构建大模型（如LLM）包括设计模型架构（如Transformer）、准备和清洗大量数据集、实现和预训练模型（使用PyTorch或TensorFlow）、优化和调试（采用分布式训练和混合精度）、以及微调和部署。
常用工具有Hugging Face Datasets（数据处理）、PyTorch/TensorFlow（模型实现）、DeepSpeed（分布式训练）、TensorBoard（监控）、以及NVIDIA CUDA（硬件加速）。
构建过程中需要强大的计算资源，如多GPU或TPU集群，云服务（如AWS或Google Cloud）可提供支持。
</pre>

<h3 id="Max-Tokens、Context-Window-和-Context-Length"><a href="#Max-Tokens、Context-Window-和-Context-Length" class="headerlink" title="Max Tokens、Context Window 和 Context Length"></a>Max Tokens、Context Window 和 Context Length</h3><ul>
<li>grok3 (2025-02-24)</li>
<li>Max Tokens、Context Window 和 Context Length 主要关注输入的限制，但它们也可能间接影响到输出，具体来说，通常与模型的生成输出长度或质量有关系。</li>
</ul>
<ol>
<li>Max Tokens (最大令牌数)<ul>
<li>Max Tokens 既包括输入的令牌数，也包括模型生成的输出的令牌数。举个例子，假设一个模型的 Max Tokens 是 4096：</li>
<li>如果输入文本占用了 1000 个令牌，那么剩余的 3096 个令牌就可以用来生成输出。</li>
<li>如果输入文本占用的令牌数较多，那么可用来生成的输出就会变少，反之亦然。</li>
<li>所以，Max Tokens 会直接限制模型的输出长度，因为它是输入和输出令牌数的总和。</li>
</ul>
</li>
<li>Context Window（上下文窗口）<ul>
<li>Context Window 通常更侧重于模型在生成过程中“能看到”的上下文范围。对于长文本，如果输入超过了 Context Window 的限制，超出的部分会被截断，模型只会处理窗口内的文本内容，而不会利用超出部分来生成更合适的回答。</li>
<li>因此，Context Window 主要限制了模型在生成时能够参考的输入量，进而间接影响输出的质量、相关性和连贯性。</li>
</ul>
</li>
<li>Context Length（上下文长度）<ul>
<li>Context Length 指的是你实际输入给模型的文本长度，包括用户输入和可能的上下文（例如先前的对话历史）。它影响了模型的理解过程，并且通常是 Max Tokens 限制的一部分。</li>
<li>如果输入的 Context Length 较长（即文本较多），那么会减少模型在生成输出时可用的令牌数，进而影响输出的长度。</li>
</ul>
</li>
</ol>
<ul>
<li>总结：<ul>
<li>Max Tokens 直接限制了输入和输出的总令牌数，影响输出的最大长度。</li>
<li>Context Window 影响模型生成输出时能参考多少输入内容，因此间接影响输出的质量、相关性和连贯性。</li>
<li>Context Length 是指输入的实际长度，它在占用较多令牌时可能会减少输出可用的令牌数，从而影响输出的长度。</li>
<li>所以，虽然这三个参数主要是控制输入的，但由于它们与模型的令牌处理能力密切相关，它们也会间接影响生成的输出。</li>
</ul>
</li>
</ul>
<h3 id="构建大模型的过程"><a href="#构建大模型的过程" class="headerlink" title="构建大模型的过程"></a>构建大模型的过程</h3><ul>
<li>From ChatGPT (4o) (2025-03-03)</li>
<li><a target="_blank" rel="noopener" href="https://chatgpt.com/share/67c531cc-e618-800b-a49d-cae86ebde079">https://chatgpt.com/share/67c531cc-e618-800b-a49d-cae86ebde079</a></li>
<li>从头构建一个大模型（比如类似 GPT、LLaMA 这样的 Transformer 结构），涉及确定神经元个数（即隐藏层维度）、神经网络层数（深度）、以及激活函数的选择。这个过程通常需要结合理论分析和实验调优。</li>
<li>神经元个数决定了模型的表达能力，主要涉及两个关键参数：<ul>
<li>d_model（隐藏层维度）：决定每个 token 的表示能力。</li>
<li>d_ff（前馈层维度）：影响非线性变换的能力</li>
</ul>
</li>
<li>层数（L）决定了模型可以学习的层级信息。</li>
<li>激活函数的作用是引入非线性，否则整个网络只是线性变换，无法学习复杂模式。</li>
</ul>
<h3 id="训练后大型语言模型的组成"><a href="#训练后大型语言模型的组成" class="headerlink" title="训练后大型语言模型的组成"></a>训练后大型语言模型的组成</h3><ul>
<li><p>From Gork3 (2025-02-25)</p>
</li>
<li><p>关键要点</p>
<ul>
<li>研究表明，大型语言模型训练完成后，其数据内容主要是参数，包括词向量和映射关系。</li>
<li>证据倾向于认为，这些参数是神经网络的权重和偏差，编码了从训练数据中学到的语言模式。</li>
<li>模型本身不存储原始训练数据，而是通过参数捕获语言的统计关系。</li>
</ul>
</li>
<li><p>参数：这些是神经网络的权重和偏差，存储在模型文件中，用于处理输入并生成输出。      </p>
</li>
<li><p>词向量（也称为嵌入），它们是单词的数值表示，捕捉单词的语义和句法意义。</p>
</li>
<li><p>映射关系，通过神经网络各层的权重定义，决定了如何处理这些词向量以生成文本。</p>
</li>
<li><p>结论：训练后的大型语言模型的数据内容是其参数，包括词向量和映射关系。这些参数通过嵌入层、前馈网络和注意力机制等组件实现，捕捉了语言的统计模式。虽然模型不直接存储训练数据，但其参数可能隐含记忆某些内容。  </p>
</li>
<li><p>From ChatGPT(o4) (2025-03-03)</p>
</li>
<li><p>除上述的数据之外，还包括</p>
<ul>
<li>模型架构（Neural Network 结构）：指的是模型的层数、注意力机制、激活函数等，比如 config.json 里会定义 Transformer 结构、隐藏层大小、head 数量、dropout 率等</li>
<li>其他辅助信息：训练时的一些超参数、优化器状态等</li>
</ul>
</li>
<li><p>这些数据以一定的格式和文件保存在大模型的训练结果中。</p>
<ul>
<li>比如Hugging Face transformers 生态 里的 “事实标准”，但并不是所有大模型都会按这个格式存储。不同的框架、实现方式和研究机构可能会有自己的格式和规范。</li>
<li>不同模型格式要用 相应的加载器，不能混用。不同的深度学习框架、训练方式，甚至不同的硬件优化方式，都会影响模型的存储格式和加载方式。</li>
</ul>
</li>
</ul>
<h3 id="如何加载大模型"><a href="#如何加载大模型" class="headerlink" title="如何加载大模型"></a>如何加载大模型</h3><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/KG2yb15EkYuWZFOwF0UT5g">大语言模型引擎全解析：Transformers、vLLM、Llama.cpp、SGLang、MLX 和 Ollama，最佳选择？</a></li>
</ul>
<h3 id="神经网络中，权重和偏置用公式的表现形式"><a href="#神经网络中，权重和偏置用公式的表现形式" class="headerlink" title="神经网络中，权重和偏置用公式的表现形式"></a>神经网络中，权重和偏置用公式的表现形式</h3><ul>
<li><p>From ChatGPT(o4) (2025-03-03)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575">https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575</a></p>
</li>
<li><p>神经网络中的权重和偏置在前向传播时，基本上都是一次方程（线性变换），但整个神经网络通常是非线性映射，因为每层的输出会经过非线性激活函数。</p>
</li>
<li><p>在神经网络中，<strong>输入 X 的个数（也就是特征的维度）</strong>由具体的任务和数据决定</p>
<ol>
<li>由数据决定（特征数量）</li>
<li>由网络结构决定</li>
<li>由数据预处理决定</li>
</ol>
</li>
</ul>
<h3 id="发布的大模型，所指的参数个数怎么计算出来的"><a href="#发布的大模型，所指的参数个数怎么计算出来的" class="headerlink" title="发布的大模型，所指的参数个数怎么计算出来的"></a>发布的大模型，所指的参数个数怎么计算出来的</h3><ul>
<li>From ChatGPT(o4) (2025-03-03)</li>
<li><a target="_blank" rel="noopener" href="https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575">https://chatgpt.com/share/67c5365d-3fac-800b-87d4-df29d53da575</a></li>
<li>参数（parameters） 指的是整个模型的权重和偏置，包括所有层的权重矩阵和偏置向量</li>
<li>参数个数：由模型层数、神经元数量、词向量维度决定，影响模型的存储和计算复杂度。</li>
</ul>
<h3 id="用英语和中文询问大型语言模型有何不同"><a href="#用英语和中文询问大型语言模型有何不同" class="headerlink" title="用英语和中文询问大型语言模型有何不同"></a>用英语和中文询问大型语言模型有何不同</h3><ul>
<li>From Gork3 (2025-03-03)</li>
<li>研究表明，使用英语或中文询问大型语言模型（LLM）的主要区别在于语言处理方式和模型性能可能因语言而异。</li>
<li>证据倾向于认为，英语因其丰富的训练数据，通常在某些任务上表现优于中文，但这取决于具体模型和任务。</li>
<li>令人意外的是，中文的字符处理方式（如基于字符的标记化）与英语的单词或子词标记化不同，这可能会影响模型的理解和生成能力。</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>《大模型应用开发极简入门：基于 GPT-4 和 ChatGPT》</li>
<li>《大语言模型》- LLMBook - <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/LLMSurvey">https://github.com/RUCAIBox/LLMSurvey</a></li>
<li>《大规模语言模型：从理论到实践》- LLM-TAP</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/12/03/20241203-ren-gong-zhi-neng-xiang-guan-ji-zhu-jian-yao-zong-jie-ji-lu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/12/03/20241203-ren-gong-zhi-neng-xiang-guan-ji-zhu-jian-yao-zong-jie-ji-lu/" itemprop="url">人工智能相关技术简要总结记录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-12-03T21:37:02+08:00">
                2024-12-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<blockquote>
<p>持续补充…</p>
</blockquote>
</blockquote>
<p><img src="/2024/12/03/20241203-ren-gong-zhi-neng-xiang-guan-ji-zhu-jian-yao-zong-jie-ji-lu/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.jpg"></p>
<h2 id="人工智能的三种形态"><a href="#人工智能的三种形态" class="headerlink" title="人工智能的三种形态"></a>人工智能的三种形态</h2><ul>
<li>来自：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64467701">人工智能是什么？——人工智能图谱</a></li>
<li>弱人工智能：弱人工智能 (Artificial Narrow Intelligence, ANI) 是擅长与单个方面的人工智能，比如有能战胜象棋世界冠军的人工智能，但是它只会下象棋，你要问它怎样更好地在硬盘上存储数据，它就不知道怎么回答你了；</li>
<li>强人工智能：强人工智能 (Artificial General Intelligence, AGI) ，是人类级别的人工智能，强人工智能是指在各方面都能和人类比肩的人工智能，人类能干的脑力活它都能干。创造强人工智能比创造弱人工智能要难得多，我们现在还做不到。Linda Gottfredson教授把智能定义为“一种宽泛的心理能力，能够进行思考、计划、解决问题、抽象思维、理解复杂理念，快速学习和从经验中学习等操作”。强人工智能在进行这些操作时，应该和人类一样得心应手；</li>
<li>超人工智能：超人工智能 (Artificial Super Intelligence, ASI)，牛津哲学家，知名人工智能思想家Nick Bostrom把超级智能定义为“在几乎所有领域都比最聪明的人类大脑都聪明很多，包括科技创新、通识和社交技能”。超人工智能可以是各方面都比人类强一点，也可以是各方面都比人类强万亿倍，超人工智能也正是为什么人工智能这个话题这么火热的缘故，同样也是为什么永生和灭绝这两个词会在本文中多次出现。</li>
</ul>
<h2 id="关于大模型的扩展思考"><a href="#关于大模型的扩展思考" class="headerlink" title="关于大模型的扩展思考"></a>关于大模型的扩展思考</h2><ul>
<li>目前的AI是一个无限能量和记忆的辅助工具，但跟人有差别，比如犯错时难以追责，毕竟人需要信誉等等….</li>
<li>cursor、 Windsurf</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64467701">人工智能是什么？——人工智能图谱</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/12/02/20241202-yi-ge-wu-zhong-xin-jie-dian-de-ju-yu-wang-tong-xin-gong-ju/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/12/02/20241202-yi-ge-wu-zhong-xin-jie-dian-de-ju-yu-wang-tong-xin-gong-ju/" itemprop="url">一个无中心节点的局域网通信工具</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-12-02T14:54:49+08:00">
                2024-12-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h2><ol>
<li>初始阶段：UDP广播发现</li>
<li>建立连接后：切换到TCP进行可靠通信（也可以使用 UDP）（增加接收完成确认机制）</li>
</ol>
<h2 id="AI生成的需求文档"><a href="#AI生成的需求文档" class="headerlink" title="AI生成的需求文档"></a>AI生成的需求文档</h2><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h2 id="局域网去中心化点对点通信工具-iOS-应用需求文档"><a href="#局域网去中心化点对点通信工具-iOS-应用需求文档" class="headerlink" title="局域网去中心化点对点通信工具 iOS 应用需求文档"></a>局域网去中心化点对点通信工具 iOS 应用需求文档</h2><h3 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h3><p>开发一款完全去中心化的局域网点对点即时通信应用，无需中心服务器，通过UDP广播实现设备发现和通信。</p>
<h3 id="核心设计理念"><a href="#核心设计理念" class="headerlink" title="核心设计理念"></a>核心设计理念</h3><ul>
<li>完全去中心化</li>
<li>基于局域网的点对点直接通信</li>
<li>无需任何中心服务器</li>
<li>设备间直接建立连接</li>
</ul>
<h3 id="网络通信技术架构"><a href="#网络通信技术架构" class="headerlink" title="网络通信技术架构"></a>网络通信技术架构</h3><h4 id="1-设备发现机制"><a href="#1-设备发现机制" class="headerlink" title="1. 设备发现机制"></a>1. 设备发现机制</h4><h5 id="1-1-UDP-广播发现"><a href="#1-1-UDP-广播发现" class="headerlink" title="1.1 UDP 广播发现"></a>1.1 UDP 广播发现</h5><ul>
<li>使用UDP广播进行设备发现</li>
<li>广播地址：255.255.255.255</li>
<li>广播端口：固定端口（如 48689）</li>
<li>发现报文结构：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;deviceId&quot;</span><span class="punctuation">:</span> <span class="string">&quot;唯一设备标识&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;nickname&quot;</span><span class="punctuation">:</span> <span class="string">&quot;用户昵称&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="string">&quot;时间戳&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;publicKey&quot;</span><span class="punctuation">:</span> <span class="string">&quot;公钥信息&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;capabilities&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;支持的功能列表&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-连接建立协议"><a href="#2-连接建立协议" class="headerlink" title="2. 连接建立协议"></a>2. 连接建立协议</h4><ul>
<li>发现阶段：UDP 广播</li>
<li>连接阶段：切换到 TCP</li>
<li>通信协议：自定义应用层协议</li>
<li>连接建立流程：<ol>
<li>UDP 广播发现</li>
<li>交换公钥</li>
<li>建立加密 TCP 通道</li>
</ol>
</li>
</ul>
<h4 id="3-通信模型"><a href="#3-通信模型" class="headerlink" title="3. 通信模型"></a>3. 通信模型</h4><ul>
<li>无中心节点</li>
<li>每个设备既是客户端又是服务端</li>
<li>直接点对点通信</li>
<li>消息路由完全去中心化</li>
</ul>
<h3 id="功能需求详细描述"><a href="#功能需求详细描述" class="headerlink" title="功能需求详细描述"></a>功能需求详细描述</h3><h4 id="1-网络发现与连接"><a href="#1-网络发现与连接" class="headerlink" title="1. 网络发现与连接"></a>1. 网络发现与连接</h4><ul>
<li>持续监听 UDP 广播</li>
<li>自动发现局域网内设备</li>
<li>支持手动&#x2F;自动添加好友</li>
<li>好友关系本地持久化</li>
</ul>
<h4 id="2-消息通信机制"><a href="#2-消息通信机制" class="headerlink" title="2. 消息通信机制"></a>2. 消息通信机制</h4><ul>
<li>UDP 广播发现</li>
<li>TCP 建立稳定通道</li>
<li>消息队列管理</li>
<li>离线消息处理</li>
</ul>
<h4 id="3-安全性设计"><a href="#3-安全性设计" class="headerlink" title="3. 安全性设计"></a>3. 安全性设计</h4><ul>
<li>设备间公钥交换</li>
<li>端到端消息加密</li>
<li>防重放攻击</li>
<li>消息签名验证</li>
</ul>
<h3 id="技术实现细节"><a href="#技术实现细节" class="headerlink" title="技术实现细节"></a>技术实现细节</h3><h4 id="网络通信协议"><a href="#网络通信协议" class="headerlink" title="网络通信协议"></a>网络通信协议</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    participant A as 设备A</span><br><span class="line">    participant B as 设备B</span><br><span class="line">    </span><br><span class="line">    A-&gt;&gt;B: UDP广播：发现请求</span><br><span class="line">    B--&gt;&gt;A: UDP广播：发现响应</span><br><span class="line">    A-&gt;&gt;B: TCP建立连接</span><br><span class="line">    A--&gt;&gt;B: 交换公钥</span><br><span class="line">    A-&gt;&gt;B: 加密消息传输</span><br></pre></td></tr></table></figure>

<h4 id="消息队列流程"><a href="#消息队列流程" class="headerlink" title="消息队列流程"></a>消息队列流程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stateDiagram-v2</span><br><span class="line">    [*] --&gt; 消息生成</span><br><span class="line">    消息生成 --&gt; 本地队列</span><br><span class="line">    本地队列 --&gt; 检测网络</span><br><span class="line">    检测网络 --&gt; 同一局域网: 直接发送</span><br><span class="line">    检测网络 --&gt; 离线: 保持队列</span><br><span class="line">    同一局域网 --&gt; 消息发送</span><br><span class="line">    消息发送 --&gt; [*]</span><br></pre></td></tr></table></figure>

<h3 id="系统架构图"><a href="#系统架构图" class="headerlink" title="系统架构图"></a>系统架构图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[UDP广播发现] --&gt; B[设备信息交换]</span><br><span class="line">    B --&gt; C[TCP安全通道]</span><br><span class="line">    C --&gt; D[端到端加密通信]</span><br><span class="line">    D --&gt; E[本地消息队列]</span><br><span class="line">    E --&gt; F[离线/在线消息管理]</span><br></pre></td></tr></table></figure>

<h3 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h3><ul>
<li>语言：Swift</li>
<li>网络框架：Network.framework</li>
<li>加密：CryptoKit</li>
<li>存储：CoreData</li>
<li>UI：SwiftUI</li>
</ul>
<h3 id="关键技术挑战"><a href="#关键技术挑战" class="headerlink" title="关键技术挑战"></a>关键技术挑战</h3><ol>
<li>跨设备一致性</li>
<li>网络发现可靠性</li>
<li>安全性保证</li>
<li>低功耗设计</li>
</ol>
<h3 id="性能目标"><a href="#性能目标" class="headerlink" title="性能目标"></a>性能目标</h3><ul>
<li>设备发现时间 &lt; 3秒</li>
<li>消息延迟 &lt; 1秒</li>
<li>电池资源占用 &lt; 5%</li>
<li>网络资源占用最小化</li>
</ul>
<h3 id="安全性要求"><a href="#安全性要求" class="headerlink" title="安全性要求"></a>安全性要求</h3><ul>
<li>端到端加密</li>
<li>防重放攻击</li>
<li>设备指纹验证</li>
<li>消息签名机制</li>
</ul>
<h3 id="隐私保护"><a href="#隐私保护" class="headerlink" title="隐私保护"></a>隐私保护</h3><ul>
<li>无中心服务器</li>
<li>本地数据完全加密</li>
<li>用户完全控制通信</li>
<li>不依赖任何第三方服务</li>
</ul>
<h3 id="开发交付物"><a href="#开发交付物" class="headerlink" title="开发交付物"></a>开发交付物</h3><ol>
<li>完整 Xcode 工程</li>
<li>详细技术文档</li>
<li>安全性测试报告</li>
<li>性能基准测试结果</li>
</ol>
<h3 id="版本迭代计划"><a href="#版本迭代计划" class="headerlink" title="版本迭代计划"></a>版本迭代计划</h3><ul>
<li>v1.0：基础局域网通信</li>
<li>v1.1：增强安全性</li>
<li>v1.2：优化性能</li>
<li>v2.0：跨平台支持</li>
</ul>
<h3 id="开发注意事项"><a href="#开发注意事项" class="headerlink" title="开发注意事项"></a>开发注意事项</h3><ul>
<li>严格遵循 Apple 安全guidelines</li>
<li>代码安全审查</li>
<li>最小权限原则</li>
<li>用户体验优先</li>
</ul>
<p>&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h3 id="Rust简易版本"><a href="#Rust简易版本" class="headerlink" title="Rust简易版本"></a>Rust简易版本</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Kingson4Wu/mesh-talk">https://github.com/Kingson4Wu/mesh-talk</a> （使用windsurf生成）</li>
</ul>
<p>&#x3D;&#x3D;&#x3D;&#x3D;</p>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><blockquote>
<blockquote>
<p>发现已经有了相关实现，非常棒！</p>
</blockquote>
</blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/localsend/localsend">https://github.com/localsend/localsend</a> 局域网文件传输，广播发现，可以简易发消息</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/11/29/20241129-ai-yi-jing-ru-ci-qiang-da-liao-bang-wo-xie-wan-suo-you-dai-ma/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/11/29/20241129-ai-yi-jing-ru-ci-qiang-da-liao-bang-wo-xie-wan-suo-you-dai-ma/" itemprop="url">AI已经如此强大了，帮我写完所有代码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-11-29T23:34:32+08:00">
                2024-11-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li><p>今天使用Windsurf帮我写一个小工具。</p>
</li>
<li><p>全程没有写过一行代码和一个字，全靠一直给AI提需求，配合授权运行纠正；有时感觉偏离正确的道路时，结合git命令回退代码，再重新提需求。就这样一直重复，就帮我把这个小工具写好了，包括使用文档，构建脚本等，通通都包了。</p>
</li>
<li><p>自从两年前第一次用ChatGPT时一下子服气了，这次直接帮我把整个项目搞定了，不得不再次拜服。客观来讲，如果是我一个人来做，由于对Rust不熟练等原因，估计得一周，而且还会很费劲，然而它半天就搞定了，并且我做得不一定比它好。</p>
</li>
<li><p>使用AI帮助编程，就像一个架构师，有一群AI小弟帮你做事。</p>
</li>
<li><p>想要把事情快速做好，前提是你是一个有品位的架构师，另外即使你很多不懂，你也是有很大可能通过你的AI小弟变成一个优秀的架构师。所以学习方法，总结，知识体系很重要，拥抱变化。</p>
</li>
<li><p>AI为什么这么强大，对里面的细节是越来越好奇了。</p>
</li>
<li><p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/Kingson4Wu/magic-converter">https://github.com/Kingson4Wu/magic-converter</a></p>
</li>
</ul>
<p><img src="/2024/11/29/20241129-ai-yi-jing-ru-ci-qiang-da-liao-bang-wo-xie-wan-suo-you-dai-ma/Windsurf.jpeg"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/11/21/20241121-dui-java-go-rust-zhi-jian-de-jian-dan-dui-bi-he-zong-jie/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/11/21/20241121-dui-java-go-rust-zhi-jian-de-jian-dan-dui-bi-he-zong-jie/" itemprop="url">对Java、Go、Rust之间的简单对比和总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-11-21T16:42:35+08:00">
                2024-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<blockquote>
<p>工作中接触过多种编程语言，主要是 Java 和 Go，最近因个人兴趣简单学习了 Rust，在这里简单记录总结一下</p>
</blockquote>
</blockquote>
<h2 id="编程语言的GC问题"><a href="#编程语言的GC问题" class="headerlink" title="编程语言的GC问题"></a>编程语言的GC问题</h2><ul>
<li><p>一般来说，程序需要管理好运行中使用的内存空间，比如传统的C和C++则要求开发者手动管理内存，但这往往导致内存泄漏和安全隐患；而垃圾回收（GC）语言，比如Java和Go在运行时自动回收内存，但存在”停顿”（STW）问题；而Rust则采用独特的所有权系统，通过编译期严格的规则检查，在不增加运行时开销的情况下实现内存安全。</p>
</li>
<li><p>GC语言，调用栈和内存一直在变化，不STW无法算出没引用的变量（可回收的内存）； 而Rust通过作用域的规则判断自动回收。另外无GC不代表不在堆分配，是代表没有STW的垃圾回收机制。</p>
</li>
<li><p>Rust引入了”所有权”概念，每个值都有且仅有一个所有者，当所有者离开作用域时，值会被自动释放。这种方式不仅避免了运行时垃圾回收的性能开销，还能在编译阶段就发现潜在的内存使用问题，有效防止了常见的内存安全缺陷。</p>
</li>
</ul>
<h2 id="设计哲学"><a href="#设计哲学" class="headerlink" title="设计哲学"></a>设计哲学</h2><ul>
<li>Java 作为一门成熟的编程语言，其设计理念更多体现在企业级应用和跨平台兼容性上。当然个人认为由此历史包袱也比较重。</li>
<li>相比之下，Go 和 Rust 作为更现代的语言，也各有侧重。Go 语言强调简洁、高效和并发性，而 Rust 则更加注重内存安全、零成本抽象和并发安全性。</li>
</ul>
<h2 id="交叉编译"><a href="#交叉编译" class="headerlink" title="交叉编译"></a>交叉编译</h2><ul>
<li>Go 和 Rust 支持各自编译成对应二进制实现跨平台（可以使用交叉编译）；而Java则编译成统一的字节码，依赖平台安装的运行时（JVM）来运行服务（也可以Graalvm直接编译成可执行二进制）</li>
</ul>
<h2 id="工具链"><a href="#工具链" class="headerlink" title="工具链"></a>工具链</h2><ul>
<li>相关工具链完善问题，比如Java性能依赖外部开发，比如arthas，asyncProfiler等；而Go自带pprof，单元测试工具等（Rust 也有一些相应的配套工具）；Java历史包袱重，不够现代化</li>
</ul>
<h2 id="热加载"><a href="#热加载" class="headerlink" title="热加载"></a>热加载</h2><ul>
<li>Java支持热加载（基于 Instrumentation 技术），但也有一定的限制，比如不能新增&#x2F;删除方法、类等，主要通过字节码替换和类加载器重载实现，一般多在开发阶段使用。实际应用中，JRebel 等商业工具通过更复杂的字节码重写技术，部分突破了这些限制，而Spring DevTools 提供了更轻量的重启机制。</li>
<li>Go官方不直接支持热加载；第三方工具如 gin-reload、air 实现热重载（通过监控文件变化，重新编译和启动进程，相对简单直接，但不是语言级特性）</li>
<li>Rust同样没有官方直接的热加载机制；比如cargo-watch 可以监听文件变化并重新编译（由于所有权系统，热加载实现相对复杂）</li>
</ul>
<h2 id="远程Debug"><a href="#远程Debug" class="headerlink" title="远程Debug"></a>远程Debug</h2><ul>
<li>Java远程调试的原理是两个VM之间通过debug协议进行通信，然后以达到远程调试的目的。两者之间可以通过socket进行通信。</li>
<li>Go原生支持远程调试，使用 dlv（Delve）调试器（基于 gRPC 协议通信）</li>
<li>Rust支持远程调试，但配置相对较复杂（主要使用 rust-gdb 和 rust-lldb）</li>
</ul>
<h2 id="依赖管理-以及-冲突解决"><a href="#依赖管理-以及-冲突解决" class="headerlink" title="依赖管理 以及 冲突解决"></a>依赖管理 以及 冲突解决</h2><h3 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h3><ul>
<li>Java 的依赖管理历史上存在诸多挑战。在早期，Java 并没有原生的依赖版本管理机制，开发者需要依赖 Maven 或 Gradle 等外部构建工具来处理项目依赖。更为关键的是，Java 的依赖冲突解析是基于具体类而非整个 JAR 包，这导致了潜在的版本兼容性和类加载问题。为了彻底解决这一痛点，Java 9 引入了模块化系统（Java Platform Module System, JPMS），提供了更精细和可靠的依赖管理和隔离机制，从根本上改善了包依赖和版本控制的复杂性。这一设计不仅简化了大型项目的依赖管理，还增强了 Java 运行时的安全性和可预测性。</li>
</ul>
<h3 id="关于Java的类重复问题"><a href="#关于Java的类重复问题" class="headerlink" title="关于Java的类重复问题"></a>关于Java的类重复问题</h3><ul>
<li><p>Java 依赖引入的时 Jar 包，使用时则是含路径信息的类名</p>
</li>
<li><p>Go则没有这个问题，因为Go的依赖的引入需要指定模块的全路径，使用时也是使用全路径或别名</p>
</li>
<li><p>Rust和 Go 类似，依赖的引入也需要指定模块的全路径。但不同包有相应的依赖文件，利用这个使相同依赖的不兼容版本共存而没有冲突问题</p>
</li>
<li><p>Java9之前（模块系统之前）- 只能减少，不能从根本上解决</p>
<ol>
<li>协议文件生成的代码，重复拷贝和引入，导致类重复冲突<ul>
<li>使用RPC协议，idl文件生成java文件，容易因为多处拷贝（比如一些业务通用库也使用到），导致类重复问题，这样在运行时可能会造成影响</li>
<li>这时最好打包的时候，不要将协议文件打进jar包中，让业务使用方自行生成代码</li>
<li>通过扫描jar包路径类的方式，可以协助检查这种问题 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">classPath</span> <span class="operator">=</span> Optional.ofNullable(thriftClass.getProtectionDomain())</span><br><span class="line">    .map(ProtectionDomain::getCodeSource)</span><br><span class="line">    .map(CodeSource::getLocation)</span><br><span class="line">    .map(URL::getPath).orElse(<span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!classPath.contains(jarFileName)) &#123;</span><br><span class="line">    System.err.println(String.format(<span class="string">&quot;%s thrift class may be duplicated&quot;</span>, thriftClass.getName()));</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">DuplicatedThriftFileException</span>(String.format(<span class="string">&quot;%s thrift class may be duplicated&quot;</span>, thriftClass.getName()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>通过maven-enforcer插件解决类冲突<ul>
<li>本质上就是解压所有依赖的 jar 包，判断是否存在重复的类文件，性能较低</li>
</ul>
</li>
</ol>
</li>
<li><p>JVM中Jar包的加载顺序</p>
<ul>
<li>由classpath参数指定的顺序决定</li>
<li>如果classpath未明确指明，则由文件系统决定的（readdir函数）<ul>
<li>readdir并不保证读取后的文件顺序，在不同的操作系统上可能有不同的顺序。</li>
</ul>
</li>
<li>如何找出重复类<ul>
<li><code>find . -name &quot;*.jar&quot; -exec sh -c  &#39;jar -tf &#123;&#125;|grep -H --label &#123;&#125; &#39;JspRuntimeContext &#39;&#39;</code> </li>
<li><code>-verbose:class</code> 查看加载顺序</li>
</ul>
</li>
</ul>
</li>
<li><p>Java9及以上（使用模块系统）</p>
</li>
</ul>
<h3 id="Go-VS-Rust-库冲突"><a href="#Go-VS-Rust-库冲突" class="headerlink" title="Go VS Rust 库冲突"></a>Go VS Rust 库冲突</h3><ul>
<li><p>当项目间接依赖同一个库的不同版本时，Rust 和 Go 在处理上有什么异同</p>
</li>
<li><p>Go 的处理方式：</p>
  <pre>
  依赖关系示例：
  my-project
  ├── A 
  │   └── pkg v1.1.0
  └── B
      └── pkg v1.2.3
  </pre>
<ul>
<li>Go 会：<ul>
<li>自动选择最高兼容版本（v1.2.3）</li>
<li>所有代码路径都使用这个版本</li>
<li>使用 MVS (Minimal Version Selection) 算法</li>
<li>在 go.mod 中记录最终版本  <pre>
  // go.mod
  module my-project

  require (
      A v1.0.0
      B v1.0.0
      pkg v1.2.3 // 间接依赖，统一使用最高版本
  )</pre></li>
</ul>
<p>  </p>
</li>
</ul>
</li>
<li><p>Rust 的处理方式：</p>
  <pre>
  依赖关系示例：
  my-project
  ├── A 
  │   └── pkg 1.1.0
  └── B
      └── pkg 1.2.3
  </pre>    
<ul>
<li>Rust 会：<ul>
<li>允许两个版本同时存在</li>
<li>分别编译两个版本的代码</li>
<li>在最终二进制中包含两个版本  <pre>
  Cargo.toml
  [dependencies]
  A = "1.0.0"  # 依赖 pkg 1.1.0
  B = "1.0.0"  # 依赖 pkg 1.2.3
  </pre></li>
</ul>
</li>
</ul>
</li>
<li><p>主要区别：</p>
<ul>
<li>Go: 强制统一版本，避免重复</li>
<li>Rust: 允许多版本共存，保证兼容性</li>
<li>这种设计反映了两种不同的理念：<ul>
<li>Go: 简单性优先，避免版本冲突</li>
<li>Rust: 灵活性优先，保证正确性</li>
</ul>
</li>
</ul>
</li>
<li><p>针对依赖同一个库的不同版本的情况：如果版本相同或兼容，Cargo会选择满足要求的当前最高版本；如果版本不兼容，Cargo允许在项目中同时使用这些不兼容的版本，可以通过别名来区分使用。</p>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>个人看法总结：Rust能做到同时使用同一个库的不同版本，是因为每个项目都有独立的依赖库配置以及引入别名机制，关键的是打包能根据这些信息直接生成二进制。而java是生成 字节码文件，并打包时丢失这方面的信息，虚拟机可能目前由于历史和后续兼容等原因也暂不支持。Go 则是选择简单性优先，避免版本冲突。</li>
<li>Rust可以运行同一库不同版本；Go和Java（模块化后）都不允许同一库不同版本；Go通过路径能确定库的唯一性；Java（未模块化）存在不同库类冲突的可能。</li>
</ul>
<h2 id="封装私有性"><a href="#封装私有性" class="headerlink" title="封装私有性"></a>封装私有性</h2><ul>
<li><p>Java通过访问修饰符（public、private、protected）控制（反射可以破坏私有性；运行时检查私有访问）</p>
</li>
<li><p>Java 9 模块化（JPMS）后，封装私有性发生了显著变化</p>
<ol>
<li>更严格的可见性控制（引入模块（module）概念；模块间显式依赖声明）</li>
<li>可见性新规则（使用 exports 关键字定义可导出包；opens 关键字控制运行时反射访问）</li>
<li>相比传统机制（编译期就能检查模块间依赖；避免了类路径的”打开式”依赖）</li>
<li>实际影响（需要在 module-info.java 显式声明依赖；原有代码需要适配模块系统；更接近 Rust 的模块化设计理念）</li>
</ol>
</li>
<li><p>Go首字母大小写决定可见性（小写标识符包内可见，大写标识符全局可见；没有私有修饰符，依赖命名约定）</p>
</li>
<li><p>Rust模块系统提供精细的可见性控制（默认私有；pub 关键字定义可见性；可以精确控制字段、方法的可见范围；编译期检查，性能无额外开销）</p>
</li>
<li><p>Rust 的封装性设计最为现代和严格，Go 相对最为简单，Java 则相对传统，Java9 之后更加严格，跟 Rust 类似，但由于历史包袱，又显得比较笨重。</p>
</li>
</ul>
<h2 id="并发和多线程"><a href="#并发和多线程" class="headerlink" title="并发和多线程"></a>并发和多线程</h2><ul>
<li>并发线程，Rust为了减少运行时，默认使用线程模型的并发。</li>
<li>Go是绿色线程（协程）。</li>
<li>Java一般也是线程模型，当然也有一些协程库（其他 JVM 语言比如 kotlin 就自带协程）</li>
</ul>
<h3 id="主线程结束进程是否停止"><a href="#主线程结束进程是否停止" class="headerlink" title="主线程结束进程是否停止"></a>主线程结束进程是否停止</h3><ul>
<li>主线程退出：主线程结束，不管其他线程是否结束，进程都会结束，这点Rust和Go一样（go是协程）.<br>Java则是即使主线程结束，其他线程不结束，进程就不会退出。</li>
</ul>
<h3 id="非主线程异常进程是否停止"><a href="#非主线程异常进程是否停止" class="headerlink" title="非主线程异常进程是否停止"></a>非主线程异常进程是否停止</h3><ul>
<li>默认情况下，非主线程的 panic 不会导致整个进程退出，这点 Rust 和 Java 一样。<ul>
<li>Java 中未捕获的异常会导致线程终止，但不影响其他线程</li>
<li>Rust 的设计更灵活，允许开发者根据需求自行控制（比如使用 std::panic::set_hook() 设置了自定义 panic 处理，可以捕获控制）</li>
</ul>
</li>
<li>而 Go 中 goroutine panic 会导致整个程序崩溃（除非被 recover）</li>
</ul>
<h2 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h2><ul>
<li>类定义：java Python js 只有class的概念 go 只有struct概念 c++都有 区别是struct可以在栈中定义</li>
<li>面向对象：Java中的单继承其实简化了继承的使用方式， Go和Rust，算是彻底抛弃了使用类继承的方式，选择了接口继承。</li>
<li>Java设计之初就是面向对象，加上由于后续历史兼容等原因，代码看起来比较臃肿（类设计）；Rust博采众长，有各自语法糖；Go追求语法简单，表达力不足，会存在一定丑陋的代码（比如没有set， contains，streams等）</li>
</ul>
<h2 id="接口设计和多态"><a href="#接口设计和多态" class="headerlink" title="接口设计和多态"></a>接口设计和多态</h2><ul>
<li>Rust中的 trait 和 Java 以及 Go 的接口：本质上它们都是在解决同一个问题：如何定义和实现抽象行为。主要区别在于语言设计理念导致的一些具体细节</li>
</ul>
<h2 id="空值问题"><a href="#空值问题" class="headerlink" title="空值问题"></a>空值问题</h2><ul>
<li>Go的类型系统一个缺憾是，对于一个类型，它的值是零值，还是不存在值，混淆不清。Java 之前也存在类似的问题，但是后来增加了基础类型的包装类型（例如对于int的Integer，double的Double），Go是否也可以参考一下？或者增加一个Option(al)类型，对这些基础类型再包装一下（基于泛型），当然还有其他更优方案那就更好了<ul>
<li>JSON包新提案：用“omitzero”解决编码中的空值困局:<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Lw_l_AELo8RKiLzVdS0H-Q">https://mp.weixin.qq.com/s/Lw_l_AELo8RKiLzVdS0H-Q</a></li>
</ul>
</li>
</ul>
<h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h2><ul>
<li>异常：Java分为Error和Exception，异常又分为运行时异常和检查性异常。抛出与捕获。<br>这点和go是类似的，go也区分简单返回的错误error和抛出的恐慌panic，而 Rust 也是差不多这么设计。</li>
</ul>
<h2 id="链式调用"><a href="#链式调用" class="headerlink" title="链式调用"></a>链式调用</h2><ul>
<li><p>链式调用：Rust和Java支持函数式链式编程，类似stream；Go不支持，要自己实现</p>
</li>
<li><p>Rust 的迭代器和 Java 的 Stream API 确实很像，都支持链式调用和函数式编程风格。</p>
</li>
<li><p>Go 的设计理念是追求简单直接，所以：</p>
<ul>
<li>没有内置的链式调用语法</li>
<li>更倾向于使用显式的 for range 循环</li>
<li>性能更可预测（没有懒加载特性）</li>
</ul>
</li>
<li><p>这反映了不同语言的设计理念：</p>
<ul>
<li>Rust&#x2F;Java：提供丰富的抽象和函数式编程特性</li>
<li>Go：保持简单，倾向于显式的命令式编程</li>
</ul>
</li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>枚举：Java和Rust支持，Go不支持；Rust可以支持同个枚举内包含不同类型</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/UG-6UuqDiLX15dEZrGGrRA">Gopher的Rust第一课：Rust的依赖管理</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/10/25/20241025-ren-zhi-jue-xing-bi-ji/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拉巴力的纸皮箱">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2024/10/25/20241025-ren-zhi-jue-xing-bi-ji/" itemprop="url">《认知觉醒》-笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-10-25T14:43:45+08:00">
                2024-10-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/2024/10/25/20241025-ren-zhi-jue-xing-bi-ji/%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92.jpg"></p>
<hr>
<ul>
<li>无论在大脑构造、潜意识、元认知、刻意练习等基本概念的解读上，还是在自控力、专注力、行动力、学习力、情绪力等具体能力的使用策略（包括早起、冥想、阅读、写作、运动等必备习惯的养成）上，都有相对独到的原理呈现和具体可行的方法提供。</li>
</ul>
<h3 id="每日反思"><a href="#每日反思" class="headerlink" title="每日反思"></a>每日反思</h3><ul>
<li>记录时间对我最大的意义，就是让自己能够觉知到时间的存在，让自己过得更加踏实。</li>
<li>每天只需花一点点时间，对当天最触动自己的事情或感悟进行复盘。</li>
<li>通过反思，我越来越多地觉知到生活中的很多细节，无须外界的帮助，就可以从小处不断完善自己。</li>
<li>如果你去练习反思，也必然会关注身体、情绪和思维三个层面，进而不断优化和改进自己。当然也会产生很多灵感、顿悟和创意，只要你去实践，就会有很多发现。</li>
<li>正视痛苦- 少数人会选择正视痛苦，反思错误，而大多数人选择逃避，沉浸在负面情绪中。</li>
<li>谨记：反思的最终目的是改变，而不是形式的完美，所以哪怕只有一句话，且这句话让自己发生了改变，那么反思的目的也就达到了。</li>
</ul>
<h2 id="第一章-大脑——一切问题的起源"><a href="#第一章-大脑——一切问题的起源" class="headerlink" title="第一章 大脑——一切问题的起源"></a>第一章 大脑——一切问题的起源</h2><ul>
<li><p>人类三重大脑</p>
<ul>
<li>本能脑（源于爬行动物，主管本能，对环境快速做出本能反应）</li>
<li>情绪脑（源于哺乳动物，主管情绪，在恶劣的环境中趋利避害，提升生存优势，比如恐惧情绪可以让他远离危险，兴奋情绪可以让他专注捕猎）</li>
<li>理智脑（源于灵长类动物，主管认知，理性地思考）</li>
</ul>
</li>
<li><p>“我怜悯恶人，我该死，应该受报应。”事实上，如果这位农夫懂得一些大脑知识，就不会犯如此低级的错误了。蛇这种冷血的爬行动物根本就没有发达的情绪脑，它不知感恩为何物，只会依靠本能行事，遇到危险要么战斗、要么逃跑；而愚昧的农夫竟然以为蛇和人类一样有善恶之心，会知恩图报，结果使自己命丧黄泉。！！！</p>
</li>
<li><p>令人欣慰的是，高级的理智脑是我们人类所独有的，它使我们富有远见、善于权衡，能立足未来获得延时满足</p>
</li>
<li><p>理智脑虽然高级，但比起本能脑和情绪脑，它的力量实在是太弱小了</p>
</li>
<li><p>本能脑早在婴儿时期就比较完善了，情绪脑则要等到青春期早期才趋于完善，而理智脑最晚，要等到成年早期才基本发育成熟。</p>
</li>
<li><p>所以在人生的前20年里，我们总是显得心智幼稚不成熟。</p>
</li>
<li><p>理智脑对大脑的控制能力很弱，所以我们在生活中做的大部分决策往往源于本能和情绪，而非理智。</p>
</li>
<li><p>为了生存，他们必须借助本能和情绪的力量对危险做出快速反应，对食物进行即时享受，对舒适产生强烈欲望，才不至于被吃掉、被饿死</p>
</li>
<li><p>为了生存，原始人还要尽量节省能量，像思考、锻炼这种耗能高的行为都会被视为对生存的威胁，会被本能脑排斥，而不用动脑的娱乐消遣行为则深受本能脑和情绪脑的欢迎</p>
</li>
<li><p>本能脑和情绪脑的基因一直被生存压力塑造着，所以它们的天性自然成了目光短浅、即时满足</p>
</li>
<li><p>我们当前遇到的几乎所有的成长问题都可以归结到目光短浅、即时满足的天性上，不过在现代社会，用避难趋易和急于求成来代指它们显然更加贴切。</p>
</li>
<li><p>成长就是克服天性的过程！！</p>
</li>
<li><p>而觉醒和成长就是让理智脑尽快变强，以克服天性</p>
</li>
<li><p>因为大脑和肌肉一样，遵循用进废退的原则。</p>
</li>
<li><p>理智脑不是直接干活的，干活是本能脑和情绪脑的事情，因为它们的“力气”大；上天赋予理智脑智慧，是让它驱动本能和情绪，而不是直接取代它们</p>
</li>
<li><p>学习知识，提升认知，运用策略</p>
</li>
<li><p>归结起来，焦虑的原因就两条：想同时做很多事，又想立即看到效果。王小波说：人的一切痛苦，本质上都是对自己无能的愤怒。焦虑的本质也契合这一观点：自己的欲望大于能力，又极度缺乏耐心。焦虑就是因为欲望与能力之间差距过大。</p>
</li>
<li><p>急于求成，想同时做很多事；避难趋易，想不怎么努力就立即看到效果。这才是焦虑真正的根源！焦虑是天性，是人类的默认设置。</p>
</li>
<li><p>我们没有必要自责或愧疚，也没有必要与天性较劲，而应想办法看清背后的机理并设法改变。</p>
</li>
<li><p>得耐心者得天下</p>
</li>
<li><p>缺乏耐心根本不是什么可耻的事，和自己的道德品质也全无关系，这仅仅是天生属性罢了，每个人都一样。</p>
</li>
<li><p>复利效应显示了价值积累的普遍规律：前期增长非常缓慢，但到达一个拐点后会飞速增长。这个“世界第八大奇迹”</p>
</li>
<li><p>我们需要冷静面对前期缓慢的增长并坚持到拐点。</p>
</li>
<li><p>复利曲线和舒适区边缘是一对好朋友，它们组合在一起可以让我们在宏观上看到保持耐心的力量，而且这种力量适用于每一个普通人。</p>
</li>
<li><p>即对于学习而言，学习之后的思考、思考之后的行动、行动之后的改变更重要，如果不盯住内层的改变量，那么在表层投入再多的学习量也会事倍功半；因此，从权重上看，改变量﹥行动量﹥思考量﹥学习量</p>
</li>
<li><p>单纯保持学习输入是简单的，而思考、行动和改变则相对困难。在缺乏觉知的情况下，我们会本能地避难趋易，不自觉地沉浸在表层的学习量中。</p>
</li>
<li><p>我们不会因自己进步缓慢而沮丧，也不会因别人成长迅速而焦虑</p>
</li>
<li><p>毕竟各自所处的阶段不同，只要持续创造价值，别人的今天就是自己的明天。</p>
</li>
<li><p>耐心不是毅力带来的结果，而是具有长远目光的结果。</p>
</li>
<li><p>对外部世界的规律的认知能使我们耐心倍增。</p>
</li>
<li><p>首先，面对天性，放下心理包袱，坦然接纳自己。当我们明白缺乏耐心是自己的天性时，就坦然接纳吧！从现在开始，对自己表现出的任何急躁、焦虑、不耐烦，都不要感到自责和愧疚</p>
</li>
<li><p>面对诱惑，学会延迟满足，变对抗为沟通。</p>
</li>
<li><p>该有的享受一点都不会少，只是不是现在享受，而是在完成重要的事情之后。”这是一个有效的策略，因为放弃享受，它们是不会同意的，但延迟享受，它们是能接受的。</p>
</li>
<li><p>最高级的方法是请本能脑和情绪脑出动来解决困难。</p>
</li>
<li><p>所以，想办法让本能脑和情绪脑感受到困难事物的乐趣并上瘾，才是理智脑的最高级的策略</p>
</li>
</ul>
<h2 id="第二章-潜意识——生命留给我们的彩蛋"><a href="#第二章-潜意识——生命留给我们的彩蛋" class="headerlink" title="第二章 潜意识——生命留给我们的彩蛋"></a>第二章 潜意识——生命留给我们的彩蛋</h2><ul>
<li><p>模糊：人生是一场消除模糊的比赛</p>
</li>
<li><p>为了更好地生存，进化之手巧妙地采用了意识分层的手段，它让潜意识负责生理系统，让意识负责社会系统，如此分工，意识便得到了解放，可以全力投入高级的社会活动。</p>
</li>
<li><p>这种模糊让人心生迷茫和恐惧，而迷茫和恐惧又使我们的认知、情绪和行动遭遇各种困扰，继而影响人生的走向。模糊，正是人生困扰之源。而人生也像是一场消除模糊的比赛，谁的模糊越严重，谁就越混沌；谁的模糊越轻微，谁就越清醒。</p>
</li>
<li><p>正如你知道了“元认知”，就知道了该如何反观自己；知道了“刻意练习”，就明白了如何精进自己；知道了“运动改造大脑”，就清楚了如何激发自己的运动热情</p>
</li>
<li><p>不幸的是，人类天生不喜欢学习和思考，因为这类事极其耗能。在漫长的进化过程中，生命的首要任务是生存，于是，基因自我设计的第一原则是节能，凡耗能高的事情都会被视为是对生存的威胁</p>
</li>
<li><p>学习知识的目的是“消除模糊”，而获取知识的方法也是“消除模糊”，目的和方法相统一</p>
</li>
<li><p>《思考力》一书的作者上田正仁提示：思考力的本质就是“丢弃所有已经消化的信息，让问题的核心浮出水面”；·《刻意练习》中的核心方法论是：不要重复练习已经会的，要不断寻找那些稍有难度的部分；·《原则》一书的作者瑞·达利欧罗列了工作和生活中的原则，用以清晰地指导自己行事；·《超越感觉》一书告诉我们，想拥有清晰的逻辑，就坚持一点：凡事不要凭模糊的感觉判断，要寻找清晰的证据。种种现象都在告诉我们一个事实</p>
</li>
<li><p>学霸”和普通同学之间的差异不仅体现在勤奋的程度上，还体现在努力的模式上：谁更愿意做高耗能的事——消除模糊，制造清晰。</p>
</li>
<li><p>受苦比解决问题来得容易，承受不幸比享受幸福来得简单。</p>
</li>
<li><p>因为解决问题需要动脑，享受幸福也需要动脑平衡各种微妙的关系，而承受痛苦则只需陷在那里不动。</p>
</li>
<li><p>真正的困难总比想象的要小很多。人们拖延、纠结、畏惧、害怕的根本原因往往不是事情本身有多难，而是内心的想法变得模糊。</p>
</li>
<li><p>如果我们再积极些，学会从一开始就主动正视它、拆解它、看清它，或许那种紧张就不困扰自己了，我们甚至能从容地“享受”比赛。</p>
</li>
<li><p>要想不受其困扰，唯一的办法就是正视它、看清它、拆解它、化解它，不给它进入潜意识的机会，不给它变模糊的机会；即使已经进入潜意识，也要想办法将它挖出来。</p>
</li>
<li><p>虽然直面情绪不会让痛苦马上消失，甚至短时间内还会加剧痛苦，但这会让你主导形势，至少不会被情绪无端恐吓。</p>
</li>
<li><p>行动力不足的真正原因是选择模糊。</p>
</li>
<li><p>选择模糊就是一种不确定性，而人类面对不确定性时会不自觉逃避</p>
</li>
<li><p>当我们没有足够清晰的指令或者目标时，就很容易选择享乐，放弃那些本该坚持但比较烧脑的选项。</p>
</li>
<li><p>在现代生活中，要想让自己更胜一筹，就必须学会花费更多的脑力和心力去思考如何拥有足够清晰的目标。我们要把目标和过程细化、具体化，在诸多可能性中建立一条单行通道，让自己始终处于“没得选”的状态。</p>
</li>
<li><p>在这条赛道上，领先的群体都有意无意地做着同一件事：消除认知、情绪和行动上的模糊。</p>
</li>
<li><p>先用感性能力帮助自己选择，再用理性能力帮助自己思考。</p>
</li>
<li><p>读书也是这样，如果单纯运用理性，我们通常会在看完整本书后花大量时间梳理作者的框架、思路，以此来表明自己读懂、读透了这本书；</p>
</li>
<li><p>一个人若是没有人生目标，纵然每天有吃、有喝、有书读、有班上，也会像一个迷失的人一样，内心没有喜悦、生活没有激情，甚至会厌恶自己，因为目标是存放我们热情和精力的地方。很多人为了找到自己的人生目标，费尽心思地分析什么事情最值得做，最后得到的答案往往是“变得很有钱”或“被别人崇拜”。这样的目标不能说有错，但往往不能长久，也无法给人真正的动力，因为这是理性思维权衡利弊和考量得失之后的结果，其动机往往来自“自我索取和外在评价”，时间一长，很容易使人迷失方向，使动力枯竭。</p>
</li>
<li><p>真正的觉醒者往往会有意无意地用感知力来代替思考力    </p>
</li>
<li><p>比如《美好人生运营指南》一书的作者一稼就提出了6条寻找人生使命的建议。·这个世界有很多事情可以做，你最想帮助哪些人？·什么事让你废寝忘食？·你在做什么事情的时候最让自己感动？·你最让人感动的时刻是什么？·如果没有任何经济压力，你会如何度过余生？·闲暇的时候，你关注最多的是哪方面的信息？</p>
</li>
<li><p>理智的分析和计算无法解出内心的真正需求，唯有感性的觉知和洞察才能让答案浮出水面。而且正确的答案往往都是利他的，因为真正长久的人生意义和幸福只能从他人的反馈中获得。</p>
</li>
<li><p>设想你即将离开世界，回首一生会为什么事情而后悔？·想一想你最喜欢的人物是谁？·你年轻的时候是怎么度过闲暇时光的？</p>
</li>
<li><p>对于你喜欢的人物，不管是虚构的还是真实的，只要让你深深地着迷，就可以从这些人物身上反射出内心理想的自己；</p>
</li>
<li><p>而年轻的时候没有家庭、工作负担，那时的追求更加遵从内心，不会受外界压力的干扰。</p>
</li>
<li><p>归纳起来，我们可以发现，理性思维虽然很高级，但在判断与选择方面可能并不具有优势，它那蹩脚的性能实在无法与灵敏快速的感性媲美。所以，先用感性选择，再用理性思考，或许是一个更好的策略，尤其是在做那些重大选择时。诚如洪兰教授的建议：小事听从你的脑，大事听从你的心。这话不无道理。</p>
</li>
<li><p>梦境。梦境是潜意识传递信息的一种方式，它可能是内心真实想法的展示，也可能是灵感的启发。</p>
</li>
<li><p>身体不会说话，却是最诚实的。无论生理还是心理上的不适，都会通过身体如实地反映出来，记得多关注这些反馈。</p>
</li>
</ul>
<h2 id="第三章-元认知——人类的终极能力"><a href="#第三章-元认知——人类的终极能力" class="headerlink" title="第三章 元认知——人类的终极能力"></a>第三章 元认知——人类的终极能力</h2><ul>
<li><p>早在15万年之前，人类就已经拥有这种能力，当然不是指人的身体真的飞到空中，而是指意识与本体分离，“飞”到更高处去反观自己。</p>
</li>
<li><p>你如果仔细观察过这个世界上优秀的人，就会发现他们几乎都是“飞”着前进的；</p>
</li>
<li><p>元认知能力就是我们习以为常、见怪不怪的反思能力。这种能力不仅为我们人类所独有，也是我们成为万物之灵[插图]的根源</p>
</li>
<li><p>当一个人能主动开启第三视角、开始持续反观自己的思维和行为时，就意味着他真正开始觉醒了，他有了快速成长的可能。</p>
</li>
<li><p>高级的元认知——时刻帮你从高处、深处、远处看待现在的自己，让自己保持清醒、不迷失，保持动力、不懈怠，保持平和、不冲动。</p>
</li>
<li><p>元认知能力总能让你站在高处俯瞰全局，不会让你一头扎进生活的细节，迷失其中。</p>
</li>
<li><p>未来视角总是当前行动的指南针</p>
</li>
<li><p>提高元认知能力的方法有很多，但最让人意想不到是下面这条——冥想。是的，冥想就是那种只要静坐在某处，然后放松身体，把注意力完全集中到呼吸和感受上的活动。</p>
</li>
<li><p>不难发现这些活动本质上都在做同一件事：监控自己的注意力，然后将其集中到自己需要关注的地方。</p>
</li>
<li><p>元认知正是人类认知能力的反馈回路，有了它，我们才可能进入快速进化的通道。</p>
</li>
<li><p>有的人能看到事物更多的意义，赋予目标强烈的价值，因此他们比其他人的专注力、执行力和意志力更强；有的人能觉察他人的想法，克制自己的言行，从而显得情商更高。他们真正的竞争力不在于学习能力，而在于强大的元认知能力。！！！</p>
</li>
<li><p>很多学习能力、运算能力超强的学霸，他们的理智脑虽然同样强大，但未必能过好自己的人生。</p>
</li>
<li><p>在“元时间”内我们要做什么呢？很简单，就做一件事：想清楚。如果不在这些选择的节点想清楚，我们就会陷入模糊状态，而模糊是潜意识的领地，它会使我们产生本能的反应——娱乐。所以，基本的应对策略便是：在选择的节点审视自己的第一反应，并产生清晰明确的主张。</p>
</li>
<li><p>虽然这样做会更累，但这正是锻炼元认知能力的最佳时机，就像是在举思想哑铃，让自己的理智脑变得更强大</p>
</li>
<li><p>元认知能力强的一个突出表现是：对模糊零容忍。换句话说，就是想尽一切办法让自己找出那个最重要的、唯一的选项，让自己在某一个时间段里只有一条路可以走。这道理很简单，既然权重都差不多，那么做哪件事都没有损失。犹豫不决，什么都想做又什么都做不好，才是最大的损失。</p>
</li>
<li><p>元认知能力强的人就是这样：无论是当下的注意力、当天的日程安排，还是长期的人生目标，他们都力求想清楚意义、进行自我审视和主动控制，而不是随波逐流。</p>
</li>
</ul>
<h2 id="第四章-专注力——情绪和智慧的交叉地带"><a href="#第四章-专注力——情绪和智慧的交叉地带" class="headerlink" title="第四章 专注力——情绪和智慧的交叉地带"></a>第四章 专注力——情绪和智慧的交叉地带</h2><ul>
<li><p>一是觉得当下太无聊，所以追求更有意思的事情；二是觉得当下太痛苦，于是追求更舒适的事情。因为身体受困于现实，只好让思想天马行空。</p>
</li>
<li><p>可见，分心走神的本质是逃避，所以，面对困难时，身心分离的人总会不自觉地退回舒适区，而身心合一的人则更容易跳出舒适区，直面困难。</p>
</li>
<li><p>身体感受永远是进入当下状态的最好媒介，而感受事物消失的过程更是一种很好的专注力训练。它提示我们，身心合一的要领不仅是专注于当下，更是享受当下，而这种享受必将使我们更从容，不慌张。</p>
</li>
<li><p>行者又问：“那何谓得道？”老和尚说：“得道前，砍柴时惦记着挑水，挑水时惦记着做饭；得道后，砍柴即砍柴，担水即担水，做饭即做饭。”</p>
</li>
<li><p>变聪明的秘诀就是：先保持极度专注，想不出答案时再将注意力转换到另一件与此毫不相干的事情上。即事前聚精会神，让意识极度投入；事后完全忘记，让意识彻底撒手。这样，灵感和答案就会大概率地出现。</p>
</li>
<li><p>很多例子都表明，科学发现或其他智力上的突破都是在当事人毫无期待、正在想别的事情的时候出现的。</p>
</li>
<li><p>李大钊也说过：“要学就学个踏实，要玩就玩个痛快！”说明界线分明的习惯对人性情和能力的培养都很有好处。</p>
</li>
<li><p>能获得有效的反馈。一般而言，不论做什么事情我们都需要反馈来准确识别自己在哪些方面还存在不足，以及为什么会存在不足。</p>
</li>
<li><p>始终在拉伸区练习。一味重复已经掌握的事情是没有意义的，但挑战太难的任务也会让自己感到挫败，二者都无法使人进入沉浸状态，好的状态应该介于二者之间。</p>
</li>
</ul>
<h2 id="第五章-学习力——学习不是一味地努力"><a href="#第五章-学习力——学习不是一味地努力" class="headerlink" title="第五章 学习力——学习不是一味地努力"></a>第五章 学习力——学习不是一味地努力</h2><ul>
<li><p>找一个自己能坚持做下去的方式，比单纯按照标准化的时间和方式做更重要。</p>
</li>
<li><p>我们就应该花大量的时间去梳理哪些内容处在自己的拉伸区，即梳理那些“会做但特别容易错或不会做但稍微努力就能懂”的内容，然后在这个区域内努力。</p>
</li>
<li><p>千万不要认为没有管束的生活很美好，一旦进入完全自由的时间，虽然开始会很舒服，但很快，我们就会迷失在众多选项中——做这个也行，做那个也行。</p>
</li>
<li><p>做选择是一件极为耗能的事情，如果没有与之匹配的清醒和定力，绝大多数人最终都会被强大的天性支配，去选择娱乐消遣。在有约束的环境下我们反而效率更高，生活更充实。</p>
</li>
<li><p>不管做什么，不管当前做得怎么样，只要让自己处在舒适区的边缘持续练习，你的舒适区就会不断扩大，拉伸区也就会不断扩展，原先的困难区也会慢慢变成拉伸区，甚至是舒适区，所以成长是必然的。</p>
</li>
<li><p>这个问题太大、太模糊。所以，你只要拆解目标——把大目标拆分为小目标，任务就会立即从困难区转移到拉伸区，这样你就愿意行动了。</p>
</li>
<li><p>跳出舒适区的最好办法就是去发现和收集那些要点，也就是每次行动的小目标</p>
</li>
<li><p>种种迹象表明，快速、简便、轻松的方式使人们避难趋易、急于求成的天性得到了放大，理智脑的潜能受到了抑制，而深度学习的能力几乎全部依赖高级理智脑的支撑。</p>
</li>
<li><p>被动学习：如听讲、阅读、视听、演示，这些活动对学习内容的平均留存率为5%、10%、20%和30%。主动学习：如通过讨论、实践、教授给他人，将被动学习的内容留存率提升到50%、75%和90%。</p>
</li>
<li><p>以阅读为例，从浅到深依次为：听书、自己读书、自己读书+摘抄金句、自己读书+思维导图&#x2F;读书笔记、自己读书+践行操练、自己读书+践行操练+输出教授。</p>
</li>
<li><p>还有一类人的数量也不少。这类人能够自己阅读，也做读书笔记或思维导图，但遗憾的是，他们的读书笔记往往只是把书中的内容梳理罗列了一番，看起来更像是一个大纲。很多人醉心于此，似乎对全书的知识了然于胸，殊不知，自己只是做了简单的搬运工作而已。虽然这种做法在一定程度上属于主动学习，但它仅仅是简单的知识陈述，与高级别的知识转换有很大的不同。更深一层的是，读完书能去实践书中的道理，哪怕有那么一两点内容让生活发生了改变，也是很了不起的，因为从这一刻开始，书本中的知识得到了转化。</p>
</li>
<li><p>请注意，遇到这种困难才是深度学习真正的开始！因为你必须动用已有的知识去解释新知识，当你能够把新学的知识解释清楚时，就意味着把它纳入了自己的知识体系，同时达到了可以教授他人的水平，并可能创造新的知识。</p>
</li>
<li><p>浅层学习满足输入，深度学习注重输出。从想法到语言再到文字，即将网状的思维变成树状的结构再变成线性的文字，相当于把思想从气态变成液态再变成固态——那些固态的东西才真正属于自己。</p>
</li>
<li><p>毕竟任何知识都不可避免地会损耗，并且这种损耗一直存在，如果不想办法把自己学到的东西固定下来，时间一长，这些知识就会烟消云散，留不下多少痕迹。</p>
</li>
<li><p>教的最高境界是用最简洁的话让一个外行人明白你讲的东西</p>
</li>
<li><p>所以，逼迫自己获取高质量的知识以及深度缝接新知识，再用自己的语言或文字教授他人，是为深度学习之道。</p>
</li>
<li><p>深度学习有以下3个步骤：</p>
<ul>
<li>(1)获取高质量的知识；(2)深度缝接新知识；(3)输出成果去教授。</li>
</ul>
</li>
<li><p>人与人之间的差距不是来自年龄，甚至不是来自经验，而是来自经验总结、反思和升华的能力。</p>
</li>
<li><p>持续反思让我对生活细节的感知能力变得越来越强，从生活中获得的东西也越来越多。</p>
</li>
<li><p>深度学习除了能让我们不再浮躁，能磨炼理智，还能带来诸多好处，比如跨界能力的提升。 </p>
</li>
<li><p>深度学习还能让人产生更多灵感。</p>
</li>
<li><p>只有在自己的领域探索得足够深入时，灵感才可能在潜意识的帮助下显现。虽然我们不是科学家，但深度学习也能让我们更大概率地收获意外的惊喜。</p>
</li>
<li><p>与此同时，深度学习还能让我们看到不同事物之间更多的关联，产生洞见。   </p>
</li>
<li><p>专注于深度学习，同时对浅学习保持开放。</p>
</li>
<li><p>无关联，不学习</p>
</li>
<li><p>常年遨游在知识的海洋中，始终无法进阶，这其中最根本的阻碍在于他们意识不到新学习的知识点是孤立的。不管这个新知识让人多警醒、使人多震撼，若是无法与已有的知识发生足够的关联，它存活不了太久。</p>
</li>
<li><p>如果你了解人类大脑的学习原理，就很容易从这幅图联想到大脑中神经元工作的情景。因为无论是学习动作，还是背记公式，从本质上来说都是大脑中神经细胞建立连接的过程。用神经科学术语解释就是：通过大量的重复动作，大脑中两个或者多个原本并不关联的神经元经过反复刺激产生了强关联。</p>
</li>
<li><p>如果没有关联这个过程，就算有再多脑细胞，你也不会变得更聪明。</p>
</li>
<li><p>鉴于此，我时常也鼓励人们写作。因为单纯阅读时，人容易满足于获取新知识，而一旦开始写作，就必须逼迫自己把所学的知识关联起来，所以写作就是一条深度学习的自然路径。</p>
</li>
<li><p>放眼看去，按照关联意识的强弱，人在不知不觉间被分成了两个群体：绝大多数人习惯以孤立的思维看待事物，喜欢花大量时间收集和占有信息；而另一批先行者则更喜欢拨弄信息之间的关联，从而在不知不觉间变得聪明了起来。</p>
</li>
<li><p>所以，我们在关联时，需要牢牢聚焦自身最迫切的需求，换句话说，就是让一切与自己有关。</p>
</li>
<li><p>知识的获取不在于多少，而在于是否与自己有关联，以及这种关联有多充分。</p>
</li>
<li><p>对别人有用的东西可能与自己并没有关系，那就果断将其放弃，把握“与自己有关”的筛选原则，会让关联效能大大提升。</p>
</li>
<li><p>知识与认知的区别</p>
</li>
<li><p>真正的知识不是你知道了它，而是能运用它帮助自己做出正确的判断和选择，解决实际问题。这一点正是“学术知识体系”和“个人知识体系”的重要区别。所以在个人成长领域，没有最优、最确定、最权威的认知体系，只有最适合我们当前状态的认知体系。</p>
</li>
<li><p>用掌握学术知识的方法去对待别人的认知体系，所以不禁沉迷于全面掌握和全盘照搬他人的体系，甚至感觉如果没有完全掌握对方的认知体系，就有可能前功尽弃。</p>
</li>
<li><p>根据能力圈法则可知，人的能力是无法跳跃发展的，只能在现有基础上一点一点向外扩展，而扩展的最佳区域就在舒适区边缘。</p>
</li>
<li><p>我们不需要全盘掌握他人的知识体系，只需要掌握那些最能触动自己、离自己需求最近的知识</p>
</li>
<li><p>体系的本质就是用独特的视角将一些零散的、独立的知识、概念或观点整合为应对这个世界的方法和技巧。</p>
</li>
<li><p>这就是搭建个人认知体系的真相：打碎各家的认知体系，只取其中最触动自己的点或块，然后将其拼接成自己的认知网络。</p>
</li>
<li><p>随着我们自身认知体系的不断完善，原来距离我们较远的知识就会相对变近，于是又能触动我们，所以暂时放弃一些知识并不可怕，只要持续学习，我们不会损失什么。</p>
</li>
<li><p>很多人读书的时候往往只关注自己是否理解了书中的内容，却经常忽视头脑中冒出的想法。其实这些想法是非常珍贵的，放过了它们，我们的学习效果就会大打折扣。</p>
</li>
<li><p>在生活中能够经常练习或使用这些知识，因为实践是产生强关联的终极方法。</p>
</li>
<li><p>在舒适区边缘，一点一点向外扩展。</p>
</li>
<li><p>莫迷恋打卡，打卡打不出未来</p>
</li>
<li><p>认知闭合，效能降低单纯地依赖打卡，不仅会转移行动的动机，还会降低行动的效能。这源自另一个重要的心理机制——认知闭合需求。</p>
</li>
<li><p>一件事若迟迟没有完成，心里就总是记挂，期盼着早点结束；此事一旦完成，做这件事的动机就会立即趋向于零。</p>
</li>
<li><p>我们之所以有这种心理是因为人类的大脑喜欢确定性，不喜欢未知或不确定性。</p>
</li>
<li><p>这就是打卡心态的特性：学不到，假装一下；学到了，立即停止。</p>
</li>
<li><p>现代人很难获得幸福感，多是因为这种快节奏和急心理，但在这种状态下，生活何其枯燥，它无法让我们享受过程，只会让身心紧张、焦虑、麻木和分裂。！！！</p>
</li>
<li><p>用记录代替打卡</p>
</li>
<li><p>只要专注于学习成长活动本身，体会其中的乐趣，就能保持强烈的学习动机，化被动学习为主动学习。</p>
</li>
<li><p>我们在任务设置时要使用新策略：设下限，不设上限。</p>
</li>
<li><p>从《微习惯》一书中获得的启示。作者斯蒂芬·盖斯为了养成好习惯，要求自己每天只做一个俯卧撑、每天只读一页书、每天只写50个字，这种无负担的习惯养成法最终促使他拥有了良好的身材，养成了阅读习惯，还写出了自己的书。他称这种方法简单到不可能失败。</p>
</li>
<li><p>无反馈，不学习</p>
</li>
<li><p>大多数人往往缺少输出和反馈意识，虽然他们极其理性，甚至能以超越常人的毅力不断激励自己努力，但最终收获的仍然是痛苦和失败。</p>
</li>
<li><p>他们似乎从来没有考虑过要尽快产出点什么，以换取反馈，通过另一种方式来激励自己。</p>
</li>
<li><p>持续的正向反馈才能真正激发本能脑和情绪脑的强大行动力。</p>
</li>
<li><p>我们的理智脑虽然聪明、有远见，但它身单力薄，真的不适合亲自上阵，真正需要它做的，是运用聪明才智去制定策略，让本能脑和情绪脑不断接受强烈的正向反馈，愉悦地朝着目标一路狂奔。</p>
</li>
<li><p>所以科学的学习策略是产出作品、获取反馈，驱动本能脑和情绪脑去“玩玩玩”，而不是一味地努力坚持，让理智脑苦苦地去“学学学”。</p>
</li>
<li><p>教是最好的学；·用是最好的学；·输出倒逼输入；·请用作品说话……那些先行者确实都有相同的品质，他们在学习的时候经常不按常理出牌，不管是不是新知识、技能，他们都直接用、直接做。当然，一开始常常用不好、做不好，但他们肯定要“鼓捣”出一个东西，然后抛出去获取反馈，不断打磨迭代</p>
</li>
<li><p>我知道这就是在驱动情绪脑为自己工作，如果自己写的文章没有任何反馈，我真不敢保证仅凭意志力和长远的认知能走到现在。所以“锁定价值—打磨作品—换取反馈”正是我持续写作的真正策略和真实动力。</p>
</li>
<li><p>分享不是随意分享半成品，而是尽最大力气将作品打磨成自己当前能力范围内可完成的最好的样子。</p>
</li>
<li><p>对待作品要像对待自己的孩子一样，每次出门前都要尽可能把它们打扮得漂亮精致，让人眼前一亮。这种要求必然会逼迫自己在能力舒适区边缘快速成长，因为这符合刻意练习的基本原则。</p>
</li>
<li><p>制定分享策略，展示给那些能力不及你的人。</p>
</li>
<li><p>最后，冷静客观地对待打击。</p>
</li>
<li><p>真正的学习成长不是“努力，努力再努力”，而是“反馈，反馈再反馈”，只有不断产出，获得反馈，我们的人生才会发生真正的变化。</p>
</li>
<li><p>“刻意练习四要素”：定义明确的目标、极度的专注、有效的反馈、在拉伸区练习。</p>
</li>
<li><p>那些持续刻苦、争分夺秒、舍不得休息一下的人，他们的精力总量势必呈一条持续下降的曲线</p>
</li>
<li><p>那些轻松的学霸，他们学习时从不过度消耗自己，只要感到精力不足，就停下来主动休息，这反而使他们精力桶的水位得到快速回升。</p>
</li>
<li><p>专注这个品质在信息时代已日渐成为稀缺品质。要学就学个踏实，要玩就玩个痛快。</p>
</li>
<li><p>面对大量的信息干扰和巨大的竞争压力，在这种情况下，大多数人只会本能地告诉自己要更刻苦、更努力，却很少有人能意识到，更科学的模式应该是：极度专注+主动休息，如此反复。</p>
</li>
<li><p>一个真正的自控高手，不是一个只知道冲刺的人，而是一个善于主动休息、保持平衡的人。</p>
</li>
<li><p>只要开始学习或工作，就尽量保持极度专注的状态，哪怕保持专注的时间很短也是有意义的；一旦发现自己开始因为精力不足而分心走神，就主动停下来调整片刻。</p>
</li>
<li><p>当然辅助工具也是有的。在时间管理领域，有一个著名的番茄工作法。</p>
</li>
<li><p>就在我想通“主动休息”这个原理的瞬间，脑海里第一个闪过的念头竟是番茄工作法，因为它符合学霸模式的所有特征：极度专注、主动休息、循环往复。</p>
</li>
<li><p>然后开始5分钟的休息计时。在这5分钟里，我会做与阅读或写作无关的事情，比如：看看窗外、收收衣服、拆拆包裹，等等，但刷手机、玩游戏这些被动使用注意力的事情我不推荐，因为它们仍然是消耗精力的。！！！</p>
</li>
<li><p>而且这个“主动停止”的动作一定要坚决。很多人在一开始的时候，由于精力分散得还不明显，就不愿意主动停下来，但这往往会得不偿失。主动休息犹如主动喝水，当感到很渴的时候再喝水，其实已经晚了，你想让精力保持高位，就要学会主动停下来，这甚至可以作为一个关键点。</p>
</li>
<li><p>刻苦，是一种宏观态度，轻松，是一种微观智慧。</p>
</li>
</ul>
<h2 id="第六章-行动力——没有行动世界只是个概念"><a href="#第六章-行动力——没有行动世界只是个概念" class="headerlink" title="第六章 行动力——没有行动世界只是个概念"></a>第六章 行动力——没有行动世界只是个概念</h2><ul>
<li><p>在经历了无数次的失败之后，我终于发现与天性对抗是没有出路的，也隐约感觉到自制力强并不代表行动力强。在随后的探索中，这个猜想逐渐得到证实：真正的行动力并不完全来源于自制力。</p>
</li>
<li><p>在一天开始的时候，一头扎进手机信息或是自己觉得有趣的事情中，然后迷失其中。这好比把这份珍贵的礼物直接摔在了地上，长此以往，自然就得不到命运的眷顾了。</p>
</li>
<li><p>如果起床后我们能刻意避开轻松和娱乐的吸引，先去读书、锻炼，或者做些重要的工作，精力就会呈聚合状态，并自动增强。比如起床后先去锻炼，就能让自己头脑清晰、精力充沛，在这种状态下做重要的工作就会非常顺利，工作越顺利，状态就越好，回路逐渐增强；再比如早起后先去阅读，读得越多，脑子里的问题和感触就越多，反过来又会产生更强烈的阅读欲望，回路逐渐增强。行动回路一旦增强，我们就会进入高效和充实的状态，此时我们哪还有精力去关注那些可看可不看的消息呢？注意力的增强回路是正向的还是负向的，很大程度上取决于你最初的选择，这也是老生常谈的道理：要事第一！！！！</p>
</li>
<li><p>在初始阶段，强迫自己先做重要的事情，一旦进入正向的增强回路，你便能拥有强大的行动力——这正是增强自制力、提升行动力的秘密</p>
</li>
<li><p>清晰力才是行动力</p>
</li>
<li><p>知道和做到相差十万八千里，这其中的差距到底在哪里呢？答案正是前文所说的“模糊”。</p>
</li>
<li><p>一切都只知道个大概，这对提升行动力来说，是很致命的。</p>
</li>
<li><p>写下当天所有要做的事，然后清空大脑，按权重将列出的事项标上序号，这样，目标就变得清晰可见；</p>
</li>
<li><p>通过持续的规划和记录，我对自由时间的掌控变得越来越强。我能够主动约束自己，我总知道下一步要做什么、什么事情最重要，即使不小心被各类消息牵绊，也能在自我提醒下快速跳出来，这一切得益于清晰力。</p>
</li>
<li><p>“写下来”就是有这样神奇的效果，因为“写下来”会清空我们的工作记忆。当我们把头脑中所有的想法和念头全部倒出来后，脑子就会瞬间变得清晰，同时，所有的想法都变得清晰且确定，这样一来，我们就进入了一种“没得选”的状态，在过程中不需要花脑力去思考或做选择。</p>
</li>
<li><p>事实上并不会，因为做规划的目的并不是让自己严格地按计划执行，而只是为了让自己心中有数。如果当天计划有变也没关系，有了这份预案，你能够在处理完临时任务后，把自己迅速拉回正轨，但如果没有这份预案，你极有可能在目标和时间都模糊的情况下选择娱乐消遣。所以，做规划十分有效，平时遇到干扰只要及时调整计划就好了。</p>
</li>
<li><p>一切源于“想清楚”</p>
</li>
<li><p>你陷入怠惰、懒散、空虚的情绪中动弹不得时，往往是因为你的大脑处于模糊状态。大脑要么不清楚自己想要什么；要么同时想做的事太多，无法确定最想实现的目标是什么；要么知道目标，但没想好具体要在什么时候以什么方式去实现。</p>
</li>
<li><p>认知越清晰，行动越坚定。</p>
</li>
<li><p>聪明的思考者都知道“想清楚”才是一切的关键，在“想清楚”这件事上。</p>
</li>
<li><p>在普通人眼里是“知易行难”，而在聪明人眼里是“知难行易”，这一点值得我们反思。</p>
</li>
<li><p>想先看到结果再行动的人往往无法看到结果。耍小聪明的人会因为结果不明朗，担心付出没有回报，所以不愿行动，以致永远停留在原地</p>
</li>
<li><p>事实上，只要道理正确，就别在乎那些小聪明，带着不计得失的心态向前走，你会发现目标越来越清晰</p>
</li>
<li><p>你觉得学英语没用，是因为你看不到生活中有需要英语的地方。只有英语学好了，和英语有关的机会才会慢慢地出现在你的周围。你觉得学历没用，是因为你根本不知道学习对你的生活轨迹能带来多少改变，你只是基于当时的场景，认为自己手里只是额外多了一张纸。你觉得锻炼身体没有用，正是因为你不去运动，所以感受不到它的价值</p>
</li>
<li><p>没错，这个世界是有认知层次的。处在下一个认知层次的人往往看不到上一个认知层次的风景，因而只能用狭隘的视角来判断：这些东西虽然很有道理，但似乎看起来并没有什么用。这些东西在他们眼里确实没什么用，因为人们无法证明一件没有发生过的事。想要打破这个悖论，只有让自己行动起来，将认知提升到更高的层次，才能做出不同的判断。</p>
</li>
<li><p>我此前一直强调“想清楚”的重要性，但当我们绞尽脑汁去想却仍然想不清楚的时候，就要依据前人的假设先行动起来</p>
</li>
<li><p>人生目标才可能慢慢浮现。</p>
</li>
<li><p>思考很重要，但光想不做，贻害无穷。</p>
</li>
<li><p>打破这些悖论的方法就是不计得失地先行动起来。</p>
</li>
<li><p>我们在行动时也应如此，我们要专注、要持续行动，直到突破阈值</p>
</li>
<li><p>这里的“傻”，并不是盲目和冲动，而是有原理、有依据的坚定</p>
</li>
<li><p>行动力强，是因为自己赞同行动背后的原理、依据和意义，而不是别人说做这个好，自己不深入了解就跟风去做，那才是真的傻。</p>
</li>
<li><p>换句话说，如果你觉得别人讲的道理有理有据，而自己暂时无法反驳，碰巧自己又非常想做这件事，那就相信他们说的是对的，然后笃定地行动</p>
</li>
<li><p>在实践途中，你自然也要保持思考，用行动反复验证他们的理论，不适则改、适则用，直到自己真正做到为止。届时你不仅能做成那件事，还能探索出自己的理论，成为别人眼中的高手。</p>
</li>
<li><p>道理再好，如果不去刻意练习，不去刺激相关神经元的强关联，这些美好的认知将永远不会真正对自己产生影响。</p>
</li>
<li><p>从现在开始，把认知当成技能，知道或想通一个道理时，不要高兴得太早，想想后面还要做大量的练习，这样就不浮躁了。</p>
</li>
<li><p>从大脑的学习机制推断，无论学习一项技能，还是养成一个习惯，背后都是相关神经元从少到多、从弱到强的关联过程。那么在一开始、在神经元关联很弱的情况下，做不好是正常的。</p>
</li>
<li><p>我们已经不是孩子了，我们应该学会用更成熟的心态包容自己最初的笨拙，即使做不好，也要持续练习，给神经元留够关联时间。</p>
</li>
<li><p>不发生真正改变的学习都是无效的学习。一篇文章、一本书就算讲得再有道理，倘若最终没有促成自己改变，我便认为读这篇文章、这本书的过程是无效的学习</p>
</li>
<li><p>现实和理论都告诉我们：懂得百点不如改变一点。真正的成长不在于自己懂得了多少道理，而在于自己改变了多少。</p>
</li>
<li><p>因为在这个世界上，知而不行的人实在太多了，只要你有所行动，就可以超越一大批人。</p>
</li>
<li><p>对成长来讲，道理都是“空头支票”，改变才是“真金白银”</p>
</li>
</ul>
<h2 id="第七章-情绪力——情绪是多角度看问题的智慧"><a href="#第七章-情绪力——情绪是多角度看问题的智慧" class="headerlink" title="第七章 情绪力——情绪是多角度看问题的智慧"></a>第七章 情绪力——情绪是多角度看问题的智慧</h2><h3 id="第一节-心智带宽：唯有富足，方能解忧"><a href="#第一节-心智带宽：唯有富足，方能解忧" class="headerlink" title="第一节　心智带宽：唯有富足，方能解忧"></a>第一节　心智带宽：唯有富足，方能解忧</h3><ul>
<li><p>贫穷造成的稀缺俘获了人的注意力，进而降低了人的心智带宽。</p>
</li>
<li><p>所谓心智带宽，就是心智的容量，它支撑着人的认知力、行动力和自控力。心智带宽一旦降低，人很容易丧失判断力，做出不明智的选择，或急于求成，做事缺乏耐心，难以抵挡享乐的诱惑。</p>
</li>
<li><p>不难预见，这些短视行为带来的糟糕结果会加剧稀缺心态：吃剩饭吃坏肚子，在医院的花费会远远超过饭菜钱；对孩子发火会让自己压力更大；而在学习时不停地刷手机，自己会更加忧心忡忡。恶性循环会增强负面回路，让忧者更忧。</p>
</li>
<li><p>可见稀缺只是“变笨”的一种诱因，事实上，任何能制造压力的事件都会挤占我们的心智带宽，比如明天的演讲、考试的期限、失业的担忧，等等。只要我们的注意力被某一个巨大的事物吸引，我们就有可能进入稀缺状态，进而降低心智带宽，做出不明智的行为。</p>
</li>
<li><p>“恋爱中的男女智商为零”</p>
</li>
<li><p>本质上就是稀缺心态导致判断力下降。</p>
</li>
<li><p>现代社会虽然给我们提供了更多便利和选择，同时也带来了前所未有的快节奏，仿佛一不留神就会落在队伍后面，这不由得迫使每个人加快脚步，不自觉地想要更多优势</p>
</li>
<li><p>心智带宽被占用殆尽，自然就没有心力支撑自己的远见、耐心、行动力和自控力了，最终只能让自己在痛苦中彷徨，甚至做不好当下的小事。</p>
</li>
<li><p>很多同学或职场人希望在假期或空闲时间提升自己，于是把日程安排得满满当当，不留一丝余地，结果每次都是“理想很丰满，现实很骨感”，不仅实现不了目标，反而在娱乐中无法自拔。这道理其实是一样的：当一个人同时面临很多任务的时候，他的心智带宽就会降低，反而没有了行动力和自控力。</p>
</li>
<li><p>有生活经验的人都会尽量克制自己的欲望，在做重要之事的同时主动安排娱乐活动，尽量保持日程的闲余——这种方法是科学的、智慧的。</p>
</li>
<li><p>陷入盲目尝试、乱学一通、急于求成的陷阱</p>
</li>
<li><p>既有人生未知的后台程序，又有各种急于实现的多线程任务。在这种状态下，一个人是很难走出来的，因为已经没有资源来支撑他的远见、耐心、行动力和自控力了。</p>
</li>
<li><p>现代生活虽然缓解了生存压力，却又带来了自控上的压力。抵制诱惑和欲望无一不消耗我们的心智带宽</p>
</li>
<li><p>唯有心智富足，方能解忧</p>
</li>
<li><p>物质条件无法决定我们的命运，真正影响我们的是心智带宽是否富足。有了富足的心智带宽，我们就能在任何环境中拥有支撑自己的远见、耐心、行动力和自控力，在变化的环境中解救自己。那么如何才能获取心智带宽呢？我想，最重要的莫过于保持自我觉知了。对此，我给大家备上五帖觉知“良药”，请各位按需取用。</p>
<ol>
<li>第一帖，保持环境觉知，理智选择。</li>
<li>第二帖，保持目标觉知，少即是多。</li>
<li>第三帖，保持欲望觉知，审视决策。<ul>
<li>脑子里存在大量任务和念头的时候，往往是我们行动力最弱的时候。所以保持对欲望的觉知，及时地审视它们，是清理自己心智带宽的好办法。</li>
<li>真正的行动力高手不是有能耐在同一时间做很多事的人，而是会想办法避免同时做很多事的人。</li>
</ul>
</li>
<li>第四帖，保持情绪觉知，谨慎决定。<ul>
<li>不要在最兴奋的时候做决定，也不要在最愤怒的时候做决定，尤其是重大决定。大喜大悲的时候，我们的心智带宽往往很窄，判断力也很弱。</li>
<li>一个心智带宽富足的人，也会是一个心平气和的人。</li>
</ul>
</li>
<li>第五帖，保持闲余觉知，自我设限。<ul>
<li>过多的闲余可不是什么好事，如果有大量的金钱，就容易萌生无谓的欲望；有大量的时间，也容易陷入低效的状态。</li>
<li>心智带宽虽足，但若不运行有效的人生程序，自然也是白费。</li>
<li>如果你的人生有如此好运，一切都很富足，不妨想办法给自己设限，适当制造稀缺，以成就自己。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="你的坏情绪，源于视角单一"><a href="#你的坏情绪，源于视角单一" class="headerlink" title="你的坏情绪，源于视角单一"></a>你的坏情绪，源于视角单一</h3><ul>
<li><p>事实上，在面对各种困境的时候，多角度看问题的能力往往是考验解决问题能力的关键</p>
</li>
<li><p>那些习惯从单一角度识人的人，往往比较单纯，也更容易受伤，本质上是因为他们缺乏多角度认知事物的意识。</p>
</li>
<li><p>一个人的性格和脾气好不好，也取决于他多角度看问题的能力：视角单一的人容易固执、急躁和钻牛角尖，而视角多元的人则表现得更为智慧、平和与包容。</p>
</li>
<li><p>我们每个人因为生活环境不同、经历不同、学识不同，所以在看待同一个问题时，理解层次和还原程度也不尽相同。</p>
</li>
<li><p>如果你确定自己的相机比他们的更高级，那就应该有“向下兼容”的意识——要么对其一笑而过，要么拿出自己的高清照片，耐心地向他们讲解什么是更好的，而不是一味地指责对方拍出来的东西很糟糕。毕竟低层的事物不会也不能向上兼容，但我们通过引导，让它们不断升级倒是有可能的。如果自己也曾有一台“落后的相机”，那就更应该体会和包容对方的立场。</p>
</li>
<li><p>在“相机”这件事情上，我们一定要保持觉知，要清醒地意识到自己的视角偏误，时刻做好向上升级、向下兼容的准备。拥有这种心态，不仅我们自己能越来越完善，还能与其他人都合得来。</p>
</li>
<li><p>不要被原始视角束缚，主动转换视角可能会看到一个新天地。</p>
</li>
<li><p>要想拥有多视角能力，就要进行刻意练习，直到形成新的路径依赖</p>
</li>
<li><p>一是勤移动。顾名思义，就是多移动你的“相机”机位，尝试用不同的视角看问题。比如设身处地地站在孩子的角度、老人的角度、对手的角度看问题，而不是仅凭自己的感受就直接认定孩子不懂事、老人不体谅、对手不讲理。</p>
</li>
<li><p>在焦虑、紧张的时候，不妨假设自己是一个局外人，用第三视角来观察自己，你会发现自己的很多担心其实是多余的，因为别人并不是那么在乎你。</p>
</li>
<li><p>如果陷入悲伤，无法自拔，那就假设自己处于十年之后，用未来视角反观现在，你会发现当下的悲伤没有任何意义，还不如收起情绪好好干活。</p>
</li>
<li><p>这种多视角观察的能力其实就体现了元认知能力。有了元认知，我们更容易在自我观察上保持觉知，进而在语言表达上也体现出“高情商”的特质。</p>
</li>
<li><p>二是善学习。</p>
</li>
<li><p>三是要开放。</p>
</li>
<li><p>忘我地聆听对方的想法。过程中没有判断、没有辩论、没有对错，把自己完全置身在对方的位置，以对方的眼睛来看世界；第二步，从“我”的角度来分享，过程中只说自己的客观感受，而不指责对方或告诉对方该怎么做。比如，说“家里满地臭袜子，我觉得精神紧张，心里很不舒适”，而不是“家里满地都是臭袜子，你不觉得难受吗”。</p>
</li>
<li><p>四是寻帮助。</p>
</li>
<li><p>原来出现特殊情况时，飞行员的注意力会被巨大的危险所俘获，心智带宽降低，容易陷入单一视角，而此时，指挥员可以给飞行员提供有效的外部视角，帮助他们更好地处置特殊情况。</p>
</li>
<li><p>同理，当我们对情绪问题或工作问题百思不得其解的时候，不要一个人闷头苦想，要学会主动寻求外部帮助，借助他人的多维视角来克服自己单一视角的局限。</p>
</li>
<li><p>五是多运动。</p>
</li>
<li><p>适当的有氧运动会提升我们体内多巴胺的水平，而多巴胺对于创造力和多角度思考能力来说都很重要。锻炼不仅能帮我们从负面情绪中快速走出来，也会引导大脑从新的角度看待事物，或者从不同角度观察问题，所以，越是心情不好的时候就越要多运动，越是想不通的时候越要多运动。</p>
</li>
<li><p>六是常反思。</p>
</li>
<li><p>通过写作抚平自己的内心，</p>
</li>
<li><p>无论什么时候，你的笔或键盘都能帮你跳出单一视角，看到更多维度。</p>
</li>
</ul>
<h3 id="第三节-游戏心态：幸福的人，总是在做另外一件事"><a href="#第三节-游戏心态：幸福的人，总是在做另外一件事" class="headerlink" title="第三节　游戏心态：幸福的人，总是在做另外一件事"></a>第三节　游戏心态：幸福的人，总是在做另外一件事</h3><ul>
<li><p>“先别减速，等跑到前面10米那个地方再减速也不迟。”等我跑到那个点后，我的目光又落到了前面的10米处，我觉得这样的距离很短，还可以继续来一次，等跑到那个点后，我又把眼光投向下一个10米处</p>
</li>
<li><p>幸福源自主动掌控现代积极心理学中，最引人瞩目的莫过于爱德华·德西和理查德·瑞恩的“自我决定理论”了。它指出人类有三种天生的内在需求：关系需求、能力需求和自主需求</p>
</li>
<li><p>一个人想要生活幸福，需要具备以下因素。·有良好的人际关系，得到别人的爱与尊敬；·有独特的本领、技能，为他人带去独特价值；·有自主选择的权力，能做自己想做的事情。</p>
</li>
<li><p>特别是“自主需求”，它是自我决定理论的关键与核心。也就是说，我们如果能主动选择和掌控所做的事情，就会产生内在动力，获取幸福。</p>
</li>
<li><p>放眼现实生活，我们总是要面对很多“不想做但必须做”的事情。比如1 500米跑步考核、堆积如山的作业、不得不洗的衣服、不得不见的人、不得不做的工作……面对这些事情，我们会不自觉地感到沮丧、抗拒和排斥，因为这些都不是我们自己主动做出的选择，而是外界给的压力。</p>
</li>
<li><p>我并不是在做这件事，我只是在做另外一件事。</p>
</li>
<li><p>我并不是在做跑步测试，我只是在玩追逐游戏；·我并不是在写作业，我只是在挑战自己的速度；·我并不是在洗衣服，我只是在活动自己的手脚；·我并不是去见领导，我只是和一个普通人聊天；·我并不是为老板做事，我只是为了提升自己。</p>
</li>
<li><p>事情本身并不重要，我们只是在通过它获取另外一种乐趣，顺便把这件事给做了。在心理学上，这个方法叫作“动机转移”。</p>
</li>
<li><p>缺乏觉知的人，其行事动机通常都由外部事物牵引，少有自主选择和掌控的余地，容易陷入“为做而做”的境地。但有觉知的人会适时觉察自己的行事动机是否停留在与目标任务无关的外部事物上，如果是，他们就主动想办法将其转移到内部，以拥有自主选择和掌控的能力，而这种掌控的窍门基本上可以分为两类：为自己而做和为玩而做。</p>
</li>
<li><p>为自己而做产生内部动机最好的方式莫过于立足于让自己变好。</p>
</li>
<li><p>而真正希望通过写作建立影响力的人是不会完全被“稿酬”“流量”等外部动机束缚的，他们往往是为自己的成长而写、为众人的需求而写、为长远的价值而写、为创造一个属于自己的世界而写。即使没有鲜花和掌声，他们也会坚持输出和成长，收获的反馈和奖励都只是意外和惊喜，不是必然和期待。</p>
</li>
<li><p>这道理不仅适用于个人层面，企业发展也是如此。比如华为公司之所以坚持不上市，就是不希望企业的发展动机被外部力量控制。如果公司上市，虽然可以在短时间内身价暴涨，但它将不可避免地把眼光放到下个季度的财报上。</p>
</li>
<li><p>那些对内在动机更敏感和坚持的人，总会与众不同。他们不会为外界的奖励或评价而刻意表现，只会为自己的成长和进步而努力进取，这样的人很难被困难击倒。</p>
</li>
<li><p>当人的注意力都在享受上时，他对跑步的心态就不一样了。相比起来，别人为了身材和身体苦苦坚持，而他只是享受愉快的跑步过程。</p>
</li>
<li><p>而我除了把跑步当成玩，其他很多事在我眼里也都是玩。比如阅读这件事。我从来不认为自己是在阅读，而是设想自己在和智者聊天。每本书在我眼里都是一个人，而我的书架就是智者朋友圈</p>
</li>
<li><p>为自己而做，通常是为了应对外部的压力和要求，为玩而做，则是为了应对重复、枯燥的事情</p>
</li>
<li><p>成长啊，有时候要看长远，让自己明白意义，心生动力；有时候要看得近些，让自己不惧困难，欢快前行。</p>
</li>
<li><p>这个世界的模样取决于我们看待它的角度</p>
</li>
<li><p>事实上，人是一种自我解释的动物，世界的意义是人类赋予的。既然做事情就是赋予意义的过程，那我们为什么不赋予它们有用又好玩的意义呢？</p>
</li>
<li><p>为自己而做可以解放情绪，为玩而做可以解放注意力</p>
</li>
</ul>
<h2 id="第八章-早冥读写跑，人生五件套——成本最低的成长之道"><a href="#第八章-早冥读写跑，人生五件套——成本最低的成长之道" class="headerlink" title="第八章 早冥读写跑，人生五件套——成本最低的成长之道"></a>第八章 早冥读写跑，人生五件套——成本最低的成长之道</h2><h3 id="第一节-早起：无闹钟、不参团、不打卡，我是如何坚持早起的"><a href="#第一节-早起：无闹钟、不参团、不打卡，我是如何坚持早起的" class="headerlink" title="第一节　早起：无闹钟、不参团、不打卡，我是如何坚持早起的"></a>第一节　早起：无闹钟、不参团、不打卡，我是如何坚持早起的</h3><ul>
<li><p>日本作家中岛孝志写的《4点起床：最养生和高效的时间管理》这本书。</p>
</li>
<li><p>浅层非快速眼动睡眠与快速眼动睡眠的组合。根据这一规律，人在睡眠后的3小时、4.5小时、6小时、7.5小时这几个节点醒来，就会觉得神清气爽，精力充沛。我对早起的实践就是从对这个理论的神奇体验开始的。</p>
</li>
<li><p>这个理论让我明白了为什么有时候我们睡了很长时间，但醒来后还是精神不佳，原因就是醒来的时机不在睡眠节点上，而是在睡眠周期中。</p>
</li>
<li><p>放弃闹钟。</p>
</li>
<li><p>中岛孝志说：“闹钟不会照顾你的睡眠周期，时间一到，就会把手伸进你的脑子里，让你的脑子发生一场大地震，潜意识会被搅得一团糟。因为你是被闹钟吵醒的，大脑深处其实还睡着，所以明明睡了8小时，可总会觉得没睡饱，整个人昏昏沉沉的。”</p>
</li>
<li><p>而当我有了感知睡眠节点的能力和习惯后（大约用了两周），根本不用担心醒不过来或错过正常的起床时间，这个生物时钟非常准。</p>
</li>
<li><p>放弃闹钟的另一个好处是，不影响家人或室友的休息，这样更容易得到他们的支持。</p>
</li>
<li><p>抓住大脑工作的高峰期。</p>
</li>
<li><p>分泌高峰期正好是早上7点左右，这时，人的工作效率非常高。人体进食后，能量也会在1小时后转变为葡萄糖，输送到大脑，人的记忆力、理解力就会提高，大脑的运转速度会迎来峰值，直至4小时后才降到谷底。所以人们要顺应规律，抓住效率高峰期，把最困难的工作放在这个时间段完成，就能达到事半功倍的效果。另外，正常吃早餐的人，上午的工作效率更高（午饭后的效率峰值在14点到16点间出现）。</p>
</li>
<li><p>一旦认知上想通想透了，行动时就不需要用大把大把的意志力来支撑了。</p>
</li>
<li><p>相比在7点左右起床，我每天多出了2小时，按一天8小时工作时长计算，每年可以多出约90个工作日，如果坚持40年，就相当于一个人全年无休工作10年。</p>
</li>
<li><p>有了这些不被打扰的时间，我可以高效地做下面这些事情。一　规划。利用10分钟左右的时间，罗列全天的工作，对它们进行排序，这样可以让自己保持头脑清晰，对全天的时间产生一种掌控感，保证自己的工作不会走弯路。二　跑步。我个人习惯起床后先跑步，毕竟此时大脑还没完全苏醒，直接进行脑力活动可能不容易迅速进入状态，但跑完步之后再冲个热水澡，精神状态就完全不同了，身体的每个细胞都被激活了，此时再读书写作就会很轻松。这种精神状态会延续到上午，当大家正常起床懵懵懂懂地去上班时，自己已经精神抖擞了。早起跑步可以让自己整个上午都享受身体的轻盈感，冬天会更耐寒。经过长期的锻炼，身型和体质也会得到极大的改善。另外，早起后，大部分人还都在睡梦中，我就可以独自一人享受晨间的静谧，这种感觉非常美妙，不会像夜间锻炼那样，经常遇到熟人而需要不停地打招呼，使锻炼效率变得很低。三　反思。这是我给自己定的功课，每天复盘一些工作、梳理一些思绪，或把一些心得感受记录下来。这么做可以很好地提升自己。四　读书或写作。平时受家庭及工作的影响，我很少有大块的时间进行自我提升，因此，早起后的这些时间非常宝贵，我的很多文章就是在这个时间段写的。五　困难的工作。我有时也会把一些困难的任务放在这个时间段攻克，通常效率会很高。早上上班的时候，那种完成了最困难的工作的心情令我从容和愉悦，这样，我就可以在很轻松的状态下做些超前或拓展性工作。以上是我目前主要的收获：清晰的时间安排、强健的体魄、良好的精神状态、不受干扰的锻炼氛围、专注的学习环境、从容的工作心态、持续的个人成长等。</p>
</li>
<li><p>除此之外，生活中焦虑也减少了很多。长期的坚持也增强了我的毅力，更重要的是，到了晚上10点，我就想着爬上床了，熬夜的恶习彻底改正。如果没有养成早起的习惯，我肯定还处于那个熬夜成性、无精打采、忙忙碌碌、无所长进的状态，不敢想象5年或10年之后会成什么样子。</p>
</li>
</ul>
<h3 id="第二节-冥想：终有一天，你要解锁这条隐藏赛道"><a href="#第二节-冥想：终有一天，你要解锁这条隐藏赛道" class="headerlink" title="第二节　冥想：终有一天，你要解锁这条隐藏赛道"></a>第二节　冥想：终有一天，你要解锁这条隐藏赛道</h3><ul>
<li>普通人和聪明人最大的能力差异是什么？是长时间保持极度专注的能力。</li>
<li>能够迅速进入专注状态，以及能够长期保持专注状态，是高效学习的两个最重要的习惯。</li>
<li>科学研究表明，通过这种集中注意力的冥想练习，人大脑皮层表面积增大，大脑灰质变厚，这意味着这种练习可以从物理上让我们变得更加聪明，因为一个人大脑皮层表面积和大脑灰质厚度是影响人聪明程度的因素。</li>
<li>闭眼静坐，专注于自己的呼吸，每天持续15分钟以上……你会感受到它的效果。</li>
<li>把心中的困惑写出来的原因，因为只要写出来，那些紧张、担忧、畏惧、害怕等情绪就会在清晰的观察下无处遁形，小球的重量自然会减轻</li>
</ul>
<h3 id="第三节-阅读：如何让自己真正爱上阅读"><a href="#第三节-阅读：如何让自己真正爱上阅读" class="headerlink" title="第三节　阅读：如何让自己真正爱上阅读"></a>第三节　阅读：如何让自己真正爱上阅读</h3><ul>
<li><p>想要快速成为一个行业的高手，最好的方法就是和行业专家交流，直接向他们请教</p>
</li>
<li><p>但现实是普通人很少有这样的机会和资源。</p>
</li>
<li><p>书籍是传承思想的最好介质，顶级的思想都能从书籍中找到，只要选书得当，就能以极低的成本找到行业里顶级的思想。</p>
</li>
<li><p>阅读可以让我们的思维能随时与顶级的思想交锋，对一个主题进行深度全面的理解，并与自己的实际充分关联，这种思维状态在平淡生活中是很少有的，但是只要拿起书本就可以马上拥有</p>
</li>
<li><p>一　读书要先学会选书。</p>
</li>
<li><p>选书比读书本身更重要。</p>
</li>
<li><p>多关注那些经过时间检验的书籍通常不会错。</p>
</li>
<li><p>二　阅读是为了改变。</p>
</li>
<li><p>真正读好一本书，往往需要花费数倍于阅读的时间去思考和实践，并输出自己的东西——可能是一篇文章，也可能是养成一个习惯——这个过程比阅读本身要费力得多。</p>
</li>
<li><p>从权重上看，阅读量&lt;思考量&lt;行动量&lt;改变量。阅读仅仅是最表层的行为，最终的目的是通过思考和行动改变自己。</p>
</li>
<li><p>阅读的深度比速度重要，阅读的质量比数量重要。读得多、读得快并不一定是好事，这很可能是自我陶醉的假象</p>
</li>
<li><p>只要紧紧盯住“改变”这个根本目标，很多阅读障碍就会立即消失</p>
</li>
<li><p>三　高阶读书法。</p>
</li>
<li><p>第一个是要特别注意自己在阅读时产生的关联。</p>
</li>
<li><p>第二个是读写不分家。如果你在阅读后还能把所学知识用自己的语言重新阐释，甚至将它们教授给他人，那这个知识将在你脑中变得非常牢固。</p>
</li>
<li><p>用一生的时间去探索、实践。</p>
</li>
</ul>
<h3 id="第四节-写作：谢谢你，费曼先生"><a href="#第四节-写作：谢谢你，费曼先生" class="headerlink" title="第四节　写作：谢谢你，费曼先生"></a>第四节　写作：谢谢你，费曼先生</h3><ul>
<li><p>向一个没有任何背景知识的人说清楚一件事是很难的。正因为这种有意无意的训练，费曼养成了一种独特的思维习惯。在从事物理研究的时候，他也会要求同事在向他汇报或者解释一个新事物时，必须用最简单的话来讲清楚。一旦解释过于冗余或者复杂，就说明他根本没有理解透彻。所谓费曼技巧就是通过自己的语言，用最简单的话把一件事情讲清楚，最好让外行人也能听懂。</p>
</li>
<li><p>这就不难理解为什么我们每个人都天生喜欢轻松愉快和简单的事情，比如在读书或读文章的时候，我们往往更愿意听故事而不是听道理。只要想明白了这一点，我想任何写作的人都会调整自己的创作方式。</p>
</li>
<li><p>先用合适的故事引起对方“感性小人”的兴趣和注意，然后把想要表达的道理通过“感性小人”转达给“理性小人”</p>
</li>
<li><p>特别是讲知识、讲道理的书籍，最好不要随意堆砌抽象概念，让人感觉很高深，看得云里雾里的。如果上来就摆图表、讲模型、说概念，或许“理性小人”没什么意见，但“感性小人”早就不耐烦了，于是他拉起“理性小人”的手说：“没意思，我们走吧。”“感性小人”的力气很大，所以说教式的写作很难吸引读者。当然，我们也不能成为“标题党”，把人吸引过来之后，又没有什么实质性的内容，这样，“理性小人”也会不满意。</p>
</li>
<li><p>能用简单的语言就不要用复杂的，这就是费曼技巧的核心之一。不过，简单不仅仅意味着轻松，还意味着简洁和形象。</p>
</li>
<li><p>我们大多数人都低估了类比（比喻）的作用，认为它只是文学中的一种修辞，事实上，它是我们的思维方式，更是我们的认知工具。</p>
</li>
<li><p>认知语言学科的创始人乔治·莱考夫曾这样定义和评价“类比”。以一种事物认知另一种事物，恰恰是学习的本质！</p>
</li>
<li><p>因为人类只能通过已知事物来解释未知事物，我们很难凭空去理解一个自己从未见过的东西。而类比，正是连接未知事物与已知事物的桥梁。</p>
</li>
<li><p>用自己的语言</p>
</li>
<li><p>费曼技巧的另一个核心就是“用自己的语言表达”，这一点比“用简单的语言表达”更为关键和奇妙。因为只有当我们使用自己的语言去解释所学时，才会真正调动自己原有的知识，才能将松散的信息编织成紧密的体系和网络，甚至创造新的认知。换言之，用自己的语言重新表达就是在调动自己的千军万马。</p>
</li>
<li><p>一个人想要真正成长，一定要学会写作，因为“只读不写”的学习是不完整的，是低效的。而写作时如果不学会用自己的语言转述，则是无用的。</p>
</li>
<li><p>因为“教”才是最好的“学”。教授他人会逼迫我们通过自己的语言，用最简单的话把一件事情讲清楚，甚至让外行人也能听得懂，而写作的优势就在于它可以让我们在磨炼这项技能的路上不断调整、反复修改，直至自己满意。</p>
</li>
</ul>
<h3 id="第五节-运动：灵魂想要走得远，身体必须在路上"><a href="#第五节-运动：灵魂想要走得远，身体必须在路上" class="headerlink" title="第五节　运动：灵魂想要走得远，身体必须在路上"></a>第五节　运动：灵魂想要走得远，身体必须在路上</h3><ul>
<li><p>好的事物往往是“正相关”的</p>
</li>
<li><p>因为运动能够调节人体的各种激素，使人达到最佳状态，使身体这个内部生态系统充满能量和活力。时常运动的人，体内生态系统犹如一汪清泉，而久坐不动的人，体内生态系统则更像是一潭死水。</p>
</li>
<li><p>一个长期缺乏运动的人可能会变“笨”。</p>
</li>
<li><p>运动能够使大脑长出更多的新的神经元，这意味着运动可以在物理上让人变得更“聪明”。</p>
</li>
<li><p>由此可以做出如下推演：运动不仅能使人身材更好、精神更佳，同时能增强大脑功能，提升注意力、记忆力、理解力、自制力，从而增强学习效果，让人创造更大的成就，获取更多资源。</p>
</li>
<li><p>好的模式是“运动+学习”</p>
</li>
<li><p>运动不是关键，运动之后的活动安排及环境刺激才是关键。</p>
</li>
<li><p>有效的模式是这样的：在运动后的1~2小时内进行高强度、高难度的脑力活动，比如阅读、解题、背记、写作、编程，等等，或是一些需要复杂技巧的体力活动，诸如舞蹈、钢琴，以及参加不同于以往的社交活动，如接触新的环境、人物或事物，这么做可以让新的神经元受到刺激，不断生长。换句话说，运动之后，脑子需要充分接受考验或挑战，才能让自己不断地变“聪明”。</p>
</li>
<li><p>“运动+学习”的模式需要坚持，因为新的神经元从生长到成熟通常需要28天</p>
</li>
<li><p>所以绝大多数运动者的硬伤就在这里：运动之后缺乏主动学习的意识和习惯。他们习惯于在运动后看电视、刷手机、玩游戏、逛街、聚会、和朋友们闲聊，甚至直接睡觉，做那些无须动脑或让自己感到很舒服的事。真的很遗憾，那些好不容易生长出来的神经元随即消散，他们因此错失了变“聪明”的机会。</p>
</li>
<li><p>比如在10分钟的有氧热身之后练习瑜伽、舞蹈、体操、太极，等等，这些复杂的活动能让大脑的全部神经细胞参与其中。活动越复杂，神经突触的联系也就越复杂，突触生长也更密集，所以好的运动方式一定同时包含有氧运动和复杂运动</p>
</li>
<li><p>运动更大的意义不在于健身而在于健脑，它不仅能使人更加乐观，还能使头脑更加灵活，最终使健康水平和认知水平实现双重提升。</p>
</li>
<li><p>语言是会影响思维的，“四肢发达，头脑简单”这句话应该修改为“四肢发达，头脑更发达”才合理，而身体和灵魂也并非只能二选一，你不能只学习不运动，或只运动不学习，也不能随心情交替进行这两项活动。我相信你现在肯定更倾向于这样的表述：灵魂想要走得远，身体必须在路上。认知越清晰，行动越坚定。从现在开始，给自己的运动计划赋予一个新的意义吧！</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/">&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/5/">&gt;</a>
  </nav>





          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.png"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
<a href="/archives/">                
<!--<a href="/archives/%7C%7C%20archive">-->
              
                  <span class="site-state-item-count">172</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">199</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kingson Wu</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
