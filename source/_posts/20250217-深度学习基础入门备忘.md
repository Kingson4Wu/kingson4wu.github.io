---
title: 深度学习基础入门备忘
date: 2025-02-17 11:08:39
tags: [AI, 深度学习, Deep-Learning]
---

>> 简单粗略通读了解，先记录备忘
>> 后续考虑结合相关视频深入理解

+ 《深度学习人门：基于Python的理论与实现》：https://github.com/Kingson4Wu/Deep-Learning-from-Scratch
    1. 感知机是一种接收多个输入信号并输出一个信号的算法。它的工作原理基于权重和偏置这两个关键参数。
    2. 机器学习的任务是让计算机自动确定合适的权重和偏置参数值。
    3. 求解机器学习问题的步骤
        1. 训练（学习）
        2. 推理（神经网络的前向传播）
    4. 激活函数(activation function)：决定如何来激活输入信号的总和；激活函数是连接感知机和神经网络的桥梁。
    5. 神经网络的学习过程：通过损失函数 (loss function)和梯度法 (gradient method)来优化网络参数
        1. 学习的目标是通过梯度下降法(gradient descent method)找到使损失函数值最小的权重参数
        2. 学习率(learning rate)：决定参数更新的步长（超参数、人工设定）
    6. 随机梯度下降(stochastic gradient descent)(SGD)能在一定程度上帮助避免局部最优，通常将SGD与其他技术结合使用,以获得更好的优化效果
    7. 深度学习：加深了层的深度神经网络；通过叠加层，可以创建更深的网络结构

+ 《深度学习进阶：自然语言处理》：https://github.com/Kingson4Wu/Natural-Language-Processing
    1. 自然语言处理的目标就是让计算机理解人说的话，进而完成 对我们有帮助的事情
    2. 单词的分布式表示（分布式假设）（单词向量化）：“某个单词的含义由它周围的单词形成”；单词本身没有含义，单词含义由它 所在的上下文(语境)形成。
    3. 向量间的相似度：余弦相似度(cosine similarity)；直观地表示了“两个向量在多大程度上指向同一方向”
    4. 让计算机理解单词含义：基于推理的方法(word2vec)（基于神经网络）。
    5. 语言模型(language model)给出了单词序列发生的概率；使用概率来评估一个单词序列发生的可能性，即在多大程度上是自然的 单词序列。
        - 生成的新文本是训练数据中没有的新生成的文本。因为语言模型并不是背诵了训练数据，而是学习了训练数据中单词的排列模式
        - 语言模型的评价：困惑度(perplexity)、分叉度
    6. “马尔可夫性”或者“马尔 可夫模型”“马尔可夫链”：指未来的状态仅 依存于当前状态。
    7. RNN（循环神经网络）：被引入来解决前馈网络在处理时序数据上的局限性。
        - 传统RNN中存在的梯度消失和梯度爆炸问题
        - LSTM的结构与传统RNN的不同之处在于，它引入了记忆单元（c）。记忆单元在LSTM层之间传递，但不直接用作输出。LSTM的对外输出是隐藏状态向量（h）。
    8. seq2seq模型（也称为Encoder-Decoder模型）用于将一个时序数据转换为另一个时序数据    
        - 传统 seq2seq 模型 将编码器输出压缩为固定长度向量，导致长序列信息丢失
        - Attention 机制 允许模型在解码时关注输入序列的不同部分，类似人类注意力
    9. Transformer：基于 Attention 构成；基于 Attention 构成    


+ 《深度学习入门：强化学习》：https://github.com/Kingson4Wu/Reinforcement-Learning
    1. 机器学习（按学习方法划分）：监督学习(supervised learning)、无监督学习(unsupervised learning)、强化学习(reinforcement learning)
        1. 监督学习：给正确答案打标签；输入的数据由“老师”打标签
        2. 无监督学习：无“正确答案标签”；没有 “老师”的存在；主要目标是找到隐藏在数据中的结构和模式；分组(聚类)、特征提取、降维
        3. 强化学习：智能代理和环境相互作用；智能代理是行动的主体；强化学习接受"奖励”作为来自环境的反馈
    2. 强化学习行动的策略
        1.  “贪婪行动”(greedy )，也叫利用(exploitation)：根据以前的经验选择最佳行动（可能错过更好的选择）
        2.  “非贪婪行动”，也叫作探索(exploration)：对价值做出更准确的估计。
    3. 强化学习算法最终归结为如何在“利用”和 “探索”之间取得平衡
    4. ε-greedy 算法、马尔可夫决策过程(MDP)
    5. 在强化学习中，我们的目标是获得最优策略
    6. 深度强化学习(deep reinforcement learning)：强化学习和深度学习的结合
    7. 通用人工智能(artificial general intelligence, AGI)
        


# 其他

## 参数、输入向量和嵌入模型的真正关系

+ 在神经网络中，“参数”特指模型在训练过程中学习得到的变量，主要包括**权重（weights）和偏置（biases）**，它们决定了模型如何处理输入并生成输出，是模型内部真正“学到的知识”。输入特征的数量（即输入维度）常被初学者误认为是“参数”，但实际上，它属于输入数据的结构，而非模型的可训练部分。每个神经元在接受输入时，会对输入向量进行**加权求和并加上偏置**，将这个结果作为激活函数的输入，激活函数再施加非线性变换，产生神经元的输出。虽然权重确实与输入一一对应，但激活函数并不直接作用于原始输入，而是作用于这个线性组合的结果。

+ 在现代深度学习模型中，尤其是语言模型（如 GPT），输入向量的维度是固定的，这一维度通常由**嵌入模型（embedding layer）或输入预处理过程**所决定。例如，文本经过分词和嵌入后，每个 token 被映射为一个固定长度的向量（如 768 维），整个输入变为一个定长矩阵。为了确保矩阵运算的可行性与模型结构的稳定性，输入长度往往被限制在最大长度内，不足时通过填充处理，超长则截断。因此，嵌入层不仅是将离散信息转为向量表示的桥梁，也是模型输入维度的决定因素之一。

## 感知机、激活函数和神经网络三者关系
+ 感知机是神经网络的基本构建单元，它通过对输入加权求和后传入激活函数来决定输出。激活函数引入非线性，使神经网络能拟合复杂关系。神经网络由多个感知机层叠构成，层与层之间通过激活函数连接，从而具备强大的表达和学习能力。三者关系为：**激活函数是感知机的一部分，感知机是神经网络的基础单元。**

### 三者之间的关系总结：

| 组成层级 | 关系说明                          |
| ---- | ----------------------------- |
| 感知机  | 最基础的神经元模型，是神经网络的最小单元          |
| 激活函数 | 是感知机或神经元的核心部分，引入非线性           |
| 神经网络 | 是多个感知机/神经元的组合，通过激活函数形成强大的拟合能力 |

可以这样理解它们的层级结构：

```
神经网络 = 多个感知机（神经元）组成
感知机 = 线性加权求和 + 激活函数
激活函数 = 赋予神经网络非线性表达能力的关键
```

===

# 神经网络训练核心机制

## 一、训练流程概览

### 训练阶段（四步闭环）
1. **前向传播** → 计算预测结果
2. **计算损失** → 衡量预测与真实值的差距
3. **反向传播** → 计算各参数的梯度
4. **参数更新** → 优化算法统一更新参数

### 推理阶段
仅执行前向传播，不涉及梯度计算和参数更新。

---

## 二、前向传播：逐层计算预测值

### 定义
在参数固定下，输入数据按层级顺序逐层计算得到输出的过程。

### 每层的两步操作
1. **线性变换**: z = Wx + b
2. **激活函数**: a = f(z)

### 关键特点
- 只做计算，不更新参数
- 训练和推理都使用

---

## 三、激活函数：赋予网络非线性能力

### 核心作用
**引入非线性**，否则多层网络等价于单层线性模型。

### 为什么非线性必不可少（XOR 示例）
- 线性模型无法解决 XOR 等线性不可分问题
- 激活函数让网络能学习复杂模式

**结论**: 激活函数是神经网络成立的前提，而非优化技巧。

### 常用激活函数
- **隐藏层**: ReLU / GELU
- **输出层**:
  - 回归 → Linear
  - 二分类 → Sigmoid
  - 多分类 → Softmax
  - 多标签 → Sigmoid

---

## 四、反向传播：计算梯度的算法

### 定义
利用链式法则，从输出向输入逐层计算损失对每个参数的梯度。

### 解决的问题
在海量参数中，精确计算每个参数对误差的"责任"。

### 关键认知
- **只负责**: 计算梯度
- **不负责**: 更新参数

### 数据流向对比
- 前向传播: 数据流（输入 → 输出）
- 反向传播: 梯度流（输出 → 输入）

---

## 五、优化算法：统一更新参数

### 常见误区澄清
- ❌ 反向传播根据误差直接改参数
- ✅ 反向传播只算梯度，优化算法负责更新

### 为什么必须统一更新
- 所有梯度基于同一次前向传播
- 边算边更新会破坏数学一致性

### 基础更新公式（SGD）
θ ← θ - η·∇θL

### 主流优化器
1. **Momentum**: 利用历史梯度，减少震荡
2. **RMSProp/AdaGrad**: 自适应学习率
3. **Adam/AdamW**: 动量 + 自适应（工程首选）

### 使用建议
- 默认选择: Adam / AdamW
- 追求泛化: SGD + Momentum

---

## 六、核心概念速查表

| 组件 | 作用 | 关键点 |
|------|------|--------|
| **前向传播** | 计算预测 | 固定参数逐层计算 |
| **激活函数** | 引入非线性 | 网络表达能力的来源 |
| **反向传播** | 计算梯度 | 只算梯度不更新参数 |
| **优化算法** | 更新参数 | 基于梯度统一调整 |

---

## 七、本质总结

**神经网络训练 = 在非线性函数空间中的梯度驱动迭代优化**