---
title: 深度学习基础入门备忘
date: 2025-02-17 11:08:39
tags: [AI, 深度学习, Deep-Learning]
---

>> 简单粗略通读了解，先记录备忘
>> 后续考虑结合相关视频深入理解

+ 《深度学习人门：基于Python的理论与实现》：https://github.com/Kingson4Wu/Deep-Learning-from-Scratch
    1. 感知机是一种接收多个输入信号并输出一个信号的算法。它的工作原理基于权重和偏置这两个关键参数。
    2. 机器学习的任务是让计算机自动确定合适的权重和偏置参数值。
    3. 求解机器学习问题的步骤
        1. 训练（学习）
        2. 推理（神经网络的前向传播）
    4. 激活函数(activation function)：决定如何来激活输入信号的总和；激活函数是连接感知机和神经网络的桥梁。
    5. 神经网络的学习过程：通过损失函数 (loss function)和梯度法 (gradient method)来优化网络参数
        1. 学习的目标是通过梯度下降法(gradient descent method)找到使损失函数值最小的权重参数
        2. 学习率(learning rate)：决定参数更新的步长（超参数、人工设定）
    6. 随机梯度下降(stochastic gradient descent)(SGD)能在一定程度上帮助避免局部最优，通常将SGD与其他技术结合使用,以获得更好的优化效果
    7. 深度学习：加深了层的深度神经网络；通过叠加层，可以创建更深的网络结构

+ 《深度学习进阶：自然语言处理》：https://github.com/Kingson4Wu/Natural-Language-Processing
    1. 自然语言处理的目标就是让计算机理解人说的话，进而完成 对我们有帮助的事情
    2. 单词的分布式表示（分布式假设）（单词向量化）：“某个单词的含义由它周围的单词形成”；单词本身没有含义，单词含义由它 所在的上下文(语境)形成。
    3. 向量间的相似度：余弦相似度(cosine similarity)；直观地表示了“两个向量在多大程度上指向同一方向”
    4. 让计算机理解单词含义：基于推理的方法(word2vec)（基于神经网络）。
    5. 语言模型(language model)给出了单词序列发生的概率；使用概率来评估一个单词序列发生的可能性，即在多大程度上是自然的 单词序列。
        - 生成的新文本是训练数据中没有的新生成的文本。因为语言模型并不是背诵了训练数据，而是学习了训练数据中单词的排列模式
        - 语言模型的评价：困惑度(perplexity)、分叉度
    6. “马尔可夫性”或者“马尔 可夫模型”“马尔可夫链”：指未来的状态仅 依存于当前状态。
    7. RNN（循环神经网络）：被引入来解决前馈网络在处理时序数据上的局限性。
        - 传统RNN中存在的梯度消失和梯度爆炸问题
        - LSTM的结构与传统RNN的不同之处在于，它引入了记忆单元（c）。记忆单元在LSTM层之间传递，但不直接用作输出。LSTM的对外输出是隐藏状态向量（h）。
    8. seq2seq模型（也称为Encoder-Decoder模型）用于将一个时序数据转换为另一个时序数据    
        - 传统 seq2seq 模型 将编码器输出压缩为固定长度向量，导致长序列信息丢失
        - Attention 机制 允许模型在解码时关注输入序列的不同部分，类似人类注意力
    9. Transformer：基于 Attention 构成；基于 Attention 构成    


+ 《深度学习入门：强化学习》：https://github.com/Kingson4Wu/Reinforcement-Learning
    1. 机器学习（按学习方法划分）：监督学习(supervised learning)、无监督学习(unsupervised learning)、强化学习(reinforcement learning)
        1. 监督学习：给正确答案打标签；输入的数据由“老师”打标签
        2. 无监督学习：无“正确答案标签”；没有 “老师”的存在；主要目标是找到隐藏在数据中的结构和模式；分组(聚类)、特征提取、降维
        3. 强化学习：智能代理和环境相互作用；智能代理是行动的主体；强化学习接受"奖励”作为来自环境的反馈
    2. 强化学习行动的策略
        1.  “贪婪行动”(greedy )，也叫利用(exploitation)：根据以前的经验选择最佳行动（可能错过更好的选择）
        2.  “非贪婪行动”，也叫作探索(exploration)：对价值做出更准确的估计。
    3. 强化学习算法最终归结为如何在“利用”和 “探索”之间取得平衡
    4. ε-greedy 算法、马尔可夫决策过程(MDP)
    5. 在强化学习中，我们的目标是获得最优策略
    6. 深度强化学习(deep reinforcement learning)：强化学习和深度学习的结合
    7. 通用人工智能(artificial general intelligence, AGI)
        


