---
title: Softmax：从直觉到本质
date: 2026-01-28 16:08:31
tags: [LLM, Math, 数学, 激活函数, AI, 深度学习基础]
mathjax: true
---

>> 以下内容由AI辅助生成

Softmax 是多分类任务中最常见的输出层函数。它的任务表面上是"把 logits 变成概率"，但本质是：**将一组可加的分数转换为可比较、可优化的相对强度**，并与极大似然/交叉熵无缝对接，同时保证数值稳定和梯度友好。

---

## 1. 问题设定：从 logits 到概率

### 1.1 什么是 logits？

神经网络最后一层通常输出一组实数：

$$z = (z_1, z_2, \dots, z_K) \in \mathbb{R}^K$$

这组 $z_i$ 称为 **logits**（未归一化分数）。

### 1.2 我们需要什么？

我们需要一个函数，将任意实数向量映射到"概率单纯形"：

$$f: \mathbb{R}^K \to \Delta^{K-1}$$

其中概率单纯形定义为：

$$\Delta^{K-1} = \{p \mid p_i > 0, \sum_i p_i = 1\}$$

### 1.3 硬约束（缺一不可）

- **非负性**：输出必须 > 0
- **归一化**：总和为 1
- **全域定义**：对所有实数输入都有定义
- **保序性**：$z_i > z_j \Rightarrow p_i > p_j$
  
  **例子**：如果 logits 是 $[2.0, 1.5, 0.8]$，那么 Softmax 后 $p_1 > p_2 > p_3$ 的顺序不变
- **可微性**：平滑可导，梯度稳定（用于反向传播）

---

## 2. Softmax 的定义

### 2.1 标准形式

$$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \quad i = 1, \dots, K$$

其中：
- $K$：类别数
- $z_i$：第 $i$ 类的 logit
- 分母：对所有类别的指数求和，用于归一化

### 2.2 向量形式

$$\text{Softmax}(z) = \frac{\exp(z)}{\mathbf{1}^\top \exp(z)}$$

### 2.3 数值稳定版（工程实践）

$$\text{Softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_{j=1}^K e^{z_j - \max(z)}}$$

**为什么要减 $\max(z)$？**

当 $z_i$ 很大时（如 1000），$e^{1000}$ 会导致数值溢出（Overflow，变成 Inf）。减去 $\max(z)$ 后：
- 最大的 logit 变为 0：$e^0 = 1$
- 其他 logit 都是负数：$e^{负数}$ 最多下溢到 0，不会 NaN
- 数学上结果完全相同（分子分母同时除以 $e^{\max(z)}$）

**例子**：
- 原始：$z = [1000, 999, 998]$ → $e^{1000}$ 溢出！
- 稳定版：$z - 1000 = [0, -1, -2]$ → $e^0=1, e^{-1}≈0.37, e^{-2}≈0.14$ ✓

---

## 3. 为什么输出在 (0,1) 且和为 1？

这是纯代数结论，无需"概率直觉"。

令 $p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$

### 3.1 为什么 $p_i > 0$？

- 对任意实数 $z_i$，都有 $e^{z_i} > 0$
- 分母是正数之和，必然 > 0
- 因此 $p_i > 0$

### 3.2 为什么 $p_i < 1$？

分母包含分子本身：

$$\sum_j e^{z_j} \ge e^{z_i} \Rightarrow p_i = \frac{e^{z_i}}{\sum_j e^{z_j}} \le 1$$

（除非 $K=1$，否则严格 < 1）

### 3.3 为什么 $\sum_i p_i = 1$？

$$\sum_{i=1}^K p_i = \sum_i \frac{e^{z_i}}{\sum_j e^{z_j}} = \frac{\sum_i e^{z_i}}{\sum_j e^{z_j}} = 1$$

### 3.4 本质

**Softmax = 对一组正数做 L1 归一化**

指数的作用是将"任意实数"转为"严格正权重"。

---

## 4. 为什么必须用指数？

### 4.1 方案一：直接归一化 ❌

$$p_i = \frac{z_i}{\sum_j z_j}$$

**问题**：
- $z_i$ 可能为负 → "概率"为负
- 分母可能为 0
- 符号变化时语义被破坏

**结论**：不满足基本约束

### 4.2 方案二：ReLU 后归一化 ❌

$$p_i = \frac{\max(0, z_i)}{\sum_j \max(0, z_j)}$$

**问题**：
- 0 处不可导，训练困难
- 大量类可能变为 0，梯度长期为 0（神经元"死亡"）
- 相对差异被扭曲

### 4.3 方案三：平方归一化 ❌

$$p_i = \frac{z_i^2}{\sum_j z_j^2}$$

**问题**：
- $(-10)$ 与 $(+10)$ 得到相同权重
- logits 的"偏好方向"丢失
- 分类语义崩塌

---

## 5. 指数为什么"刚刚好"？

指数函数 $e^x$ 同时满足所有需求：

1. **严格正性**：$e^x > 0$（无零点、无负值、无断点）
2. **严格单调**：保持顺序
3. **差异放大**：线性差转为倍率差
   $$\frac{e^{z_i}}{e^{z_j}} = e^{z_i - z_j}$$
   
   **例子**：
   - 如果 $z_1 - z_2 = 2$，则 $p_1/p_2 = e^2 ≈ 7.4$ 倍
   - 如果 $z_1 - z_2 = 5$，则 $p_1/p_2 = e^5 ≈ 148$ 倍
   - logit 差距越大，概率比越悬殊
   
4. **加法→乘法同态**：
   $$e^{a+b} = e^a \cdot e^b$$

第 4 点是关键的"桥梁"性质，后文将深入阐述。

---

## 6. e 是什么？

### 6.1 连续复利的极限

**背景**：假设你存 1 元钱，年利率 100%。
- 一年计息 1 次：$(1 + 1)^1 = 2$ 元
- 半年计息 1 次：$(1 + 0.5)^2 = 2.25$ 元
- 每天计息：$(1 + \frac{1}{365})^{365} ≈ 2.7146$ 元
- 每秒计息：$(1 + \frac{1}{31536000})^{31536000} ≈ 2.71828$ 元

当计息频率趋于无限（**连续复利**），极限就是 $e$：

$$e = \lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^n \approx 2.718281828...$$

### 6.2 级数定义（实际计算常用）

$$e = \sum_{k=0}^{\infty} \frac{1}{k!} = 1 + 1 + \frac{1}{2} + \frac{1}{6} + \frac{1}{24} + \cdots$$

算到 $1/9!$ 已经非常接近 2.71828。

### 6.3 最关键性质：自导数

$$\frac{d}{dx} e^x = e^x$$

这是 Softmax + 交叉熵梯度简洁的根本原因。

---

## 7. 为什么用 e 而非其他底数？

### 7.1 其他底数可行吗？

假设用底数 $a > 0$：

$$p_i = \frac{a^{z_i}}{\sum_j a^{z_j}}$$

导数为：

$$\frac{d}{dz} a^z = a^z \ln a$$

### 7.2 梯度尺度污染

配合交叉熵时，梯度变为：

$$\frac{\partial L}{\partial z_i} = \ln(a) \cdot (p_i - y_i)$$

而使用 $e$ 时：

$$\ln(e) = 1 \Rightarrow \frac{\partial L}{\partial z_i} = p_i - y_i$$

### 7.3 为什么这很重要？

- 学习率本应直接控制步长
- 换底数会引入无意义的常数 $\ln a$
- 多层网络中尺度难以控制
- 表达能力没有提升，纯属干扰

**结论**：用 $e$ 等价于"剥离多余的常数尺度"，使系统最简洁。

---

## 8. Softmax + 交叉熵的"奇迹"

### 8.1 什么是交叉熵？

在分类任务中，我们用 **one-hot 编码** 表示真实标签：
- 如果样本属于第 2 类（共 3 类）：$y = [0, 1, 0]$
- 只有正确类别为 1，其他为 0

**交叉熵损失**衡量预测分布 $p$ 与真实分布 $y$ 的差异：

$$L = -\sum_i y_i \log p_i$$

因为 $y$ 是 one-hot，只有真实类别 $y_{true}$ 处为 1，所以简化为：

$$L = -\log p_{true}$$

意义：
- 如果 $p_{true} = 1$（预测完全正确）→ $L = 0$
- 如果 $p_{true} = 0.1$（预测很不确定）→ $L = 2.3$
- 如果 $p_{true} = 0.01$（预测错误）→ $L = 4.6$

**目标**：最小化交叉熵 = 最大化正确类别的预测概率。

### 8.2 交叉熵损失的完整形式

使用 one-hot 标签 $y$，交叉熵（也叫负对数似然）：

$$L = -\sum_i y_i \log p_i$$

### 8.3 代入 Softmax

$$L = -z_y + \log \sum_j e^{z_j}$$

### 8.4 梯度极简

$$\frac{\partial L}{\partial z_i} = p_i - y_i$$

这个"干净到难以置信"的形式，根源于：

$$\frac{d}{dx} e^x = e^x$$

若用其他正函数 $g(x)$ 替代指数，梯度会出现复杂的 $g'(x)/g(x)$ 项，优化困难且不稳定。

---

## 9. "加法 → 乘法"：唯一的自然桥梁

这不是比喻，而是结构必然性。

### 9.1 Logits 的加法世界

最后一层的典型形式：

$$z_i = w_i^\top x + b_i$$

语义是"证据累加"：
- 支持特征 → $z_i$ 增加
- 反对证据 → $z_i$ 减少
- 多条证据 → 分数相加

**例子**（图像分类）：
- 检测到"毛发" → 猫的分数 +2
- 检测到"尖耳朵" → 猫的分数 +1.5
- 检测到"圆脸" → 猫的分数 +1
- 最终：$z_{猫} = 2 + 1.5 + 1 = 4.5$

比较两类时，自然量是**差值**：

$$z_i - z_j$$

表示"$i$ 相对 $j$ 的净优势"，典型的**加法结构**。

### 9.2 概率的乘法世界

分类中真正关心的是"相对可能性"，即**比值**：

$$\frac{p_i}{p_j}$$

**例子**：
- 如果 $p_{猫} = 0.7, p_{狗} = 0.2$
- 比值：$p_{猫}/p_{狗} = 3.5$，表示"猫的可能性是狗的 3.5 倍"
- 这是倍率/赔率，本质是**乘法结构**（比例、连乘）

### 9.3 核心需求：差值控制比值

希望存在单调函数 $g$，使得：

$$\frac{p_i}{p_j} = g(z_i - z_j)$$

**左边**：概率的比例结构（乘法世界）  
**右边**：logit 的差分结构（加法世界）

**我们需要一座桥，将"差"转为"比"。**

### 9.4 一致性约束逼出指数

希望满足传递性：
- 若 $i$ 比 $j$ 强 $\Delta_1$，$j$ 比 $k$ 强 $\Delta_2$
- 则 $i$ 比 $k$ 应强 $\Delta_1 + \Delta_2$

即：

$$(z_i - z_j) + (z_j - z_k) = z_i - z_k$$

对应概率比值的链式相乘：

$$\frac{p_i}{p_k} = \frac{p_i}{p_j} \cdot \frac{p_j}{p_k}$$

结合两式，得函数方程：

$$g(\Delta_1 + \Delta_2) = g(\Delta_1) \cdot g(\Delta_2)$$

在"正、连续、单调"等合理条件下，唯一解是**指数族**：

$$g(\Delta) = e^{c\Delta}$$

因此：

$$\frac{p_i}{p_j} = e^{c(z_i - z_j)} \Rightarrow p_i \propto e^{cz_i}$$

归一化后：

$$p_i = \frac{e^{cz_i}}{\sum_j e^{cz_j}}$$

（$c$ 为温度参数，通常 $c = 1/T$）

**结论**："加法→乘法"不是比喻，而是结构要求的必然结果。

### 9.5 对训练的友好性

训练使用对数似然，对数将乘法拉回加法：
- **指数**：加法 → 乘法
- **对数**：乘法 → 加法

整个系统形成闭环：logits 的差是线性的，log 概率比也是线性的，梯度才简洁稳定。

---

## 10. 信息论视角：最大熵推导

### 10.1 问题设定

在约束条件下：

$$\sum_i p_i = 1, \quad \sum_i p_i z_i = c$$

最大化熵：

$$H = -\sum_i p_i \log p_i$$

### 10.2 拉格朗日方法

用拉格朗日乘子法求解，得到：

$$p_i \propto e^{z_i}$$

归一化后即为 Softmax。

### 10.3 深层意义

**Softmax 不是约定俗成，而是在"仅知期望分数"约束下，熵最大的唯一形式。**

这从信息论角度证明了 Softmax 的必然性。

---

## 11. 总结：Softmax 的必然性

Softmax 不是随便设计的，而是唯一同时满足以下要求的函数：

**基础要求**：
- 任意实数输入 → 正数输出 → 和为 1
- 大的 logit → 大的概率（保序）
- 处处可导，梯度稳定

**核心机制**：
- 用指数把"证据累加"（加法）转成"可能性倍率"（乘法）
- logit 差 2 → 概率比 $e^2 ≈ 7$ 倍；差 5 → 比 $e^5 ≈ 150$ 倍

**训练完美**：
- 配合交叉熵，梯度就是 $p_i - y_i$（干净到极致）
- 用 $e$ 而非其他底数，避免梯度尺度污染

**数值稳定**：
- 减 $\max(z)$ 防溢出，下溢到 0 也符合语义

**理论支撑**：
- 最大熵原理下的唯一解
- 指数族分布的自然形式

**一句话**：Softmax 是"把加法世界的分数转成乘法世界的概率"的唯一自然方式。

