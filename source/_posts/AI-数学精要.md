---
title: AI 数学精要速览
date: 2026-01-20 12:52:37
tags: [AI, Math, 数学]
mathjax: true
---


## 一、人工智能的本质：数学建模 + 优化，而非算力魔法

人工智能并非神秘技术，其本质是：

* **数学**：描述问题、刻画规律、定义目标
* **算法**：在数学模型上进行搜索与优化
* **计算机**：负责实现与加速计算

可以用一句话概括：

> 人工智能 = 数学模型 + 优化算法 + 工程实现

因此，真正决定 AI 能力上限的，不是算力本身，而是**建模方式与优化思想**。

---

## 二、机器学习的本质：函数估计与函数逼近

### 1. 从数据到映射关系

在机器学习中：

* 数据最终都会被表示为**数值向量**
* 模型的作用是学习一个映射关系：

$$
\text{输入向量} \rightarrow \text{输出向量}
$$

这本质上就是在学习一个函数：

$$
\hat{y} = f_\theta(x)
$$

其中 $\theta$ 是模型参数。

### 2. 函数逼近视角（核心）

真实世界的规律函数通常未知，只能通过有限样本观察。

机器学习做的事情是：

* 定义损失函数
* 选定模型形式（线性、神经网络等）
* 通过数据训练参数

最终得到：

> 一个对真实函数的近似

因此可以准确地说：

> 机器学习问题，本质是函数估计/函数逼近问题

---

## 三、损失函数与目标函数：从单点误差到整体准则

这是整个体系中最容易混淆、但也最关键的一层。

### 1. 损失函数（Loss Function）

损失函数衡量的是：

> 模型在单个样本上的预测误差

例如平方误差：

$$
\ell(y, \hat{y}) = (y - \hat{y})^2
$$

它回答的问题是：

> 这一次预测错得有多严重？

### 2. 目标函数（Objective Function）

训练时真正被优化的，是一个**全局函数**：

$$
J(\theta) = \frac{1}{N}\sum_{i=1}^N \ell_i + \lambda \Omega(\theta)
$$

其中：
- 第一项：平均损失（经验风险），用于拟合数据
- 第二项：正则项，用于限制模型复杂度

**结论（非常重要）：**

> 损失函数是局部误差，目标函数是训练时真正被最小化的整体准则

在最简单的情况下，目标函数可以等于平均损失；在真实问题中，目标函数**几乎总是平均损失加上正则约束**。

---

## 四、统一视角：人工智能 = 优化问题

无论是机器学习还是深度学习，核心任务都可以统一为：

> 在参数空间中，最小化（或最大化）一个目标函数

### 1. 全局最优 vs 局部最优

* **全局最小值**：整个空间中最小
* **局部极小值**：某个邻域内最小

在高维参数空间中：

* 全局搜索不可行
* 实用算法通常只能找到足够好的解

工程上接受的标准是：

> 目标函数足够小 + 泛化性能可接受

---

## 五、微积分：优化算法的数学基础

### 1. 导数与梯度

* 导数：函数变化率
* 梯度：多变量函数的一阶导数向量

梯度方向表示：

> 函数上升最快的方向

因此，**负梯度方向**就是下降最快的方向。

### 2. 核心优化算法

所有主流训练算法，底层都依赖导数和矩阵运算：

* 梯度下降（GD）
* 随机梯度下降（SGD）
* 牛顿法、拟牛顿法（BFGS/L-BFGS）

---

## 六、凸函数：为什么理论上好解，工程上难找

### 1. 凸性的决定性作用

如果目标函数是凸的：

* 任意局部最小值 = 全局最小值
* 优化是干净的

这在传统机器学习中非常常见：

| 模型 | 损失 | 目标函数 |
|------|------|----------|
| 线性回归 | 平方误差 | 凸 |
| 逻辑回归 | 对数损失 | 凸 |
| SVM | Hinge loss | 凸 |

### 2. 深度学习为什么是非凸的

在神经网络中：

* 模型是高度非线性的
* 参数强耦合
* 多层激活叠加

结果是：

> 即使损失函数形式是凸的，目标函数关于参数仍然是非凸的

---

## 七、为什么深度学习还能工作？

关键不在理论保证，而在工程现实：

1. **不追求全局最优**  
   只要性能足够好即可

2. **高维空间的性质**  
   坏的局部极小值很少，更多是鞍点

3. **SGD 的随机性**  
   噪声反而有助于跳出鞍点

4. **工程手段**  
   初始化、正则化、BatchNorm、残差结构等

一句话总结：

> 非凸优化在理论上困难，在工程上可控

---

## 八、线性代数：深度学习的骨架系统

现代深度学习几乎完全建立在线性代数之上：

* 向量表示与嵌入
* 神经网络前向传播
* CNN、Attention、Transformer

可以直接断言：

> 没有线性代数，就没有现代深度学习

---

## 九、感知机、激活函数与偏置项

### 1. 感知机模型

$$
y = f(w \cdot x + b)
$$

* $w$：权重，决定方向与敏感度
* $b$：偏置，决定阈值/平移

### 2. 偏置项的本质

偏置项的作用是：

> 让决策边界不被强制经过原点

几何上：

* 权重决定方向与斜率
* 偏置决定起始位置

没有偏置项，模型表达能力会严重受限。

### 3. 激活函数的意义

早期阶跃函数不可导，无法优化；现代网络使用可导函数（Sigmoid、ReLU、Tanh），以支持梯度下降。

---

## 十、训练机制：参数不是写出来的，而是学出来的

程序员负责：

* 模型结构
* 损失函数
* 数据准备

**参数的具体数值：**

> 完全由训练过程自动学习得到

即使模型结构相同，只要数据不同，学到的模型也会不同。

---

## 十一、过拟合：模型记住了题目，但没学会规律

典型特征：

* 训练集表现很好
* 测试集表现很差

本质原因：

> 模型复杂度 > 数据所能支撑的复杂度

解决思路：

* 正则化
* 数据增强
* 控制模型规模
* Dropout、早停等技术

---

## 十二、神经网络与深度学习

* 神经网络：多层可导感知机的组合
* 深度学习：更深的神经网络

加深的效果是：

> 在参数规模相近的情况下，逼近更复杂的函数

理论解释仍在研究中，但工程效果已被反复验证。

---

## 十三、强化学习：从监督学习到交互学习

强化学习的目标不是最小化预测误差，而是：

> 最大化长期累积回报

特点：

* 没有标准答案
* 通过试错学习
* 奖励信号驱动参数更新

---

## 总纲（高度压缩版）

> 人工智能是在用数学定义目标，用优化寻找参数，用数据逼近未知函数；理论上关心凸性与最优性，工程上关心效果、稳定性与泛化能力。


---

## Reference
+ [简单研究一下人工智能和数学](https://kingson4wu.github.io/2025/01/06/20250106-jian-dan-yan-jiu-yi-xia-ren-gong-zhi-neng-he-shu-xue/)